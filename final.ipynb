{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99ad9bfb",
   "metadata": {},
   "source": [
    "# Next-Year Financial Distress Early-Warning (Transition) & Surveillance (Compustat Annual Panel) \u2014 Reproducible ML Pipeline\n",
    "\n",
    "**Goal.** Predict the probability that a firm is in *financial distress* in fiscal year **t+1** using accounting (and permitted market) information available at fiscal year **t**.\n",
    "\n",
    "**Important scope note.** The outcome is an **engineered distress proxy** (high leverage / balance-sheet stress), not a realized legal default or bankruptcy. The notebook is therefore a **predictive measurement and decision-support pipeline**, not a causal identification design.\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook structure (Data Science Lifecycle \u2014 10 phases)\n",
    "\n",
    "1. Problem Definition & Setup  \n",
    "2. Data Collection & Panel Integrity  \n",
    "3. Data Cleaning & Missingness Handling (leakage-aware)  \n",
    "4. Exploratory Data Analysis (EDA)  \n",
    "5. Feature Engineering & Target Construction  \n",
    "6. Preprocessing for Modeling (train-only fitting)  \n",
    "7. Model Selection & Training (7A Logit; 7B Trees)  \n",
    "8. Model Evaluation & Diagnostic Monitoring  \n",
    "9. Operational Risk Management Layer (Events + PDs)\n",
    "10. Results Summary, Guardrails, and Replication Artifacts\n",
    "\n",
    "> This organization mirrors the course lifecycle guidance and the project's technical review action items (see provided PDF and technical report).\n",
    "\n",
    "## How to run (replication package convention)\n",
    "\n",
    "1. Place `data.csv` in the project root (or update `CONFIG[\"DATA_PATH\"]` in Section 1).\n",
    "2. Keep `Variables.xlsx` (variable dictionary) alongside the notebook for automatic documentation.\n",
    "3. Run **Kernel \u2192 Restart & Run All**.\n",
    "\n",
    "The notebook creates an `outputs/` folder containing:\n",
    "- a predictions export (`predictions.csv`),\n",
    "- configuration and threshold tables,\n",
    "- model summary tables suitable for an appendix,\n",
    "- figures saved as PNG for paper workflow.\n",
    "\n",
    "## 1. Problem Definition & Setup\n",
    "\n",
    "### 1.1 Prediction target, success metrics, and decision objective\n",
    "\n",
    "- **Target (supervised label):** `target_next_v1`, `target_next_v2`, or `target_next_v3` (separate distress proxies). Downstream modeling uses `target_next_v2` by default.  \n",
    "- **Primary performance metrics (out-of-sample):**\n",
    "  - ROC-AUC (ranking quality),\n",
    "  - PR-AUC (class imbalance),\n",
    "  - Brier score (probability accuracy / calibration).\n",
    "- **Decision objective (screening):** convert predicted PDs into a review policy using:\n",
    "  - **misclassification costs** (`COST_FN`, `COST_FP`) and\n",
    "  - **capacity constraints** (screen top `CAPACITY_PCT` percent of firms).\n",
    "\n",
    "This is a *risk scoring* workflow: calibrated probabilities and operational interpretability matter more than headline accuracy.\n",
    "\n",
    "### 1.2 Configuration, determinism, and library versions\n",
    "\n",
    "**Objective options:**\n",
    "- **Transition (early-warning):** predict *healthy at t \u2192 distressed at t+1*.\n",
    "- **State (surveillance):** predict *distress state at t+1* (includes persistence).\n",
    "\n",
    "The notebook preserves proxy selection (V1/V2/V3); set `PROXY_VERSION` and `OBJECTIVE` in Section 4 (Targets).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n1. Separate class-imbalance handling from decision-cost weighting and document the fix in modeling and diagnostics.\n2. Add 95% confidence intervals for AUC-PR/Brier via stratified firm bootstrap and fold-based summaries.\n3. Extend the logit section with firm/time fixed effects and Firth bias-corrected rare-event logit.\n4. Run winsorization sensitivity (1/99, 5/95, 10/90) and report AUC-PR.\n"
   ],
   "id": "2f349c4deb07949a"
  },
  {
   "cell_type": "code",
   "id": "86fca47fb377817c",
   "metadata": {},
   "source": [
    "# Core numerics\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import inspect\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ML / metrics\n",
    "from sklearn.model_selection import ParameterGrid, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "# Stats / inference\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.sandwich_covariance import cov_cluster, cov_cluster_2groups\n",
    "from scipy import stats\n",
    "\n",
    "# Trees / explainability\n",
    "import xgboost as xgb\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "\n",
    "# XGBoost API compatibility (feval -> custom_metric in v2+)\n",
    "XGB_TRAIN_METRIC_KW = (\n",
    "    \"custom_metric\" if \"custom_metric\" in inspect.signature(xgb.train).parameters else \"feval\"\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------------------------\n",
    "# Determinism\n",
    "# ----------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "USING_SYNTHETIC_DATA = False # Global flag for data mode\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration (edit here)\n",
    "# ----------------------------\n",
    "CONFIG = {\n",
    "    # Data inputs\n",
    "    \"DATA_PATH\": \"data.csv\",\n",
    "    \"VARIABLES_XLSX_PATH\": \"Variables.xlsx\",\n",
    "\n",
    "    # Temporal splitting via label_year = fyear + 1\n",
    "    \"TRAIN_CUTOFF_LABEL_YEAR\": 2022,   # label_year <= cutoff -> train/val pool; later -> test\n",
    "    \"VAL_YEARS\": 1,                    # number of last label years inside the train pool used as validation\n",
    "\n",
    "    # Missingness / imputation\n",
    "    \"KNN_K\": 5,\n",
    "    \"IMPUTE_LO_Q\": 0.01,\n",
    "    \"IMPUTE_HI_Q\": 0.99,\n",
    "\n",
    "    # Preprocessing\n",
    "    \"WINSOR_LO_Q\": 0.01,\n",
    "    \"WINSOR_HI_Q\": 0.99,\n",
    "\n",
    "    # Logit hyperparameter search\n",
    "    \"LOGIT_C_GRID\": [0.01, 0.1, 1.0, 10.0],\n",
    "\n",
    "    # Tree model (XGBoost) parameters \u2014 base settings; tuned below via policy-aligned search\n",
    "    \"XGB_PARAMS\": {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": [\"aucpr\", \"auc\"],\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"seed\": SEED,\n",
    "    },\n",
    "    \"XGB_PARAM_SPACE\": {\n",
    "        \"max_depth\": [2, 3, 4],\n",
    "        \"min_child_weight\": [10, 30, 60, 100],\n",
    "        \"gamma\": [0.5, 2.0, 5.0, 10.0],\n",
    "        \"eta\": [0.01, 0.03, 0.05],\n",
    "        \"reg_lambda\": [1.0, 10.0, 30.0, 50.0],\n",
    "        \"reg_alpha\": [0.0, 0.5, 1.0, 2.0],\n",
    "        \"subsample\": [0.5, 0.7, 0.9],\n",
    "        \"colsample_bytree\": [0.3, 0.5, 0.8],\n",
    "        \"max_delta_step\": [1, 3, 5],\n",
    "        \"booster\": [\"gbtree\", \"dart\"],\n",
    "    },\n",
    "    \"XGB_N_TRIALS\": 100,\n",
    "    \"XGB_NUM_BOOST_ROUND\": 30000,\n",
    "    \"XGB_EARLY_STOPPING\": 200,\n",
    "\n",
    "    # Decision policy parameters (costs + capacity)\n",
    "    \"COST_FN\": 25.0,\n",
    "    \"COST_FP\": 1.0,\n",
    "    \"CAPACITY_PCT\": 0.20,  # screen top 20% by PD as a capacity policy\n",
    "\n",
    "    # Outputs\n",
    "    \"OUTPUT_DIR\": \"outputs\",\n",
    "    \"FIG_DIR\": \"figures\",\n",
    "}\n",
    "\n",
    "Path(CONFIG[\"OUTPUT_DIR\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(CONFIG[\"FIG_DIR\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"CONFIG (key parameters):\")\n",
    "for k in [\"DATA_PATH\",\"TRAIN_CUTOFF_LABEL_YEAR\",\"VAL_YEARS\",\"KNN_K\",\"WINSOR_LO_Q\",\"WINSOR_HI_Q\",\"COST_FN\",\"COST_FP\",\"CAPACITY_PCT\"]:\n",
    "    print(f\"  {k}: {CONFIG[k]}\")\n",
    "print(\"\\nPython:\", sys.version.split()[0])\n",
    "print(\"pandas:\", pd.__version__)\n",
    "print(\"numpy:\", np.__version__)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e2c6f95dbcde3593",
   "metadata": {},
   "source": [
    "### 1.3 Helper utilities (robust ratios, transforms, and reporting)"
   ]
  },
  {
   "cell_type": "code",
   "id": "80a9f29d9bea6451",
   "metadata": {},
   "source": [
    "def signed_log1p(x: pd.Series) -> pd.Series:\n",
    "    \"\"\"Signed log1p transform: sign(x) * log1p(|x|). Preserves zero and sign, stabilizes tails.\"\"\"\n",
    "    x = pd.to_numeric(x, errors=\"coerce\")\n",
    "    return np.sign(x) * np.log1p(np.abs(x))\n",
    "\n",
    "def safe_divide(numer: pd.Series, denom: pd.Series, denom_floor: float = None) -> pd.Series:\n",
    "    \"\"\"Safe divide with optional denominator floor for stability. Returns float with NaN where undefined.\"\"\"\n",
    "    numer = pd.to_numeric(numer, errors=\"coerce\")\n",
    "    denom = pd.to_numeric(denom, errors=\"coerce\")\n",
    "    if denom_floor is not None:\n",
    "        denom = denom.where(denom.abs() >= denom_floor, other=np.sign(denom).replace(0, 1) * denom_floor)\n",
    "    out = numer / denom\n",
    "    out = out.replace([np.inf, -np.inf], np.nan)\n",
    "    return out\n",
    "\n",
    "def ensure_nullable_float(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Convert to pandas nullable Float64 to enable NA-aware comparisons (returns <NA> instead of False).\"\"\"\n",
    "    return pd.to_numeric(s, errors=\"coerce\").astype(\"Float64\")\n",
    "\n",
    "def winsorize_train_bounds(x: pd.Series, lo: float, hi: float) -> tuple[float, float]:\n",
    "    \"\"\"Return winsorization bounds computed on *training* observed values.\"\"\"\n",
    "    x = pd.to_numeric(x, errors=\"coerce\")\n",
    "    x_obs = x.dropna()\n",
    "    if len(x_obs) == 0:\n",
    "        return (np.nan, np.nan)\n",
    "    return (float(x_obs.quantile(lo)), float(x_obs.quantile(hi)))\n",
    "\n",
    "def apply_bounds(x: pd.Series, lo: float, hi: float) -> pd.Series:\n",
    "    x = pd.to_numeric(x, errors=\"coerce\")\n",
    "    if np.isnan(lo) or np.isnan(hi):\n",
    "        return x\n",
    "    return x.clip(lower=lo, upper=hi)\n",
    "\n",
    "def compute_smd(train: pd.Series, test: pd.Series) -> float:\n",
    "    \"\"\"Standardized mean difference (SMD): (mu_train - mu_test)/pooled_sd.\"\"\"\n",
    "    a = pd.to_numeric(train, errors=\"coerce\").dropna()\n",
    "    b = pd.to_numeric(test, errors=\"coerce\").dropna()\n",
    "    if len(a) < 2 or len(b) < 2:\n",
    "        return np.nan\n",
    "    mu_a, mu_b = a.mean(), b.mean()\n",
    "    sd_a, sd_b = a.std(ddof=1), b.std(ddof=1)\n",
    "    pooled = np.sqrt(0.5*(sd_a**2 + sd_b**2))\n",
    "    return float((mu_a - mu_b) / pooled) if pooled > 0 else np.nan\n",
    "\n",
    "def logit(p: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    p = np.clip(p, eps, 1-eps)\n",
    "    return np.log(p/(1-p))\n",
    "\n",
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def print_df(df: pd.DataFrame, n: int = 10, name: str = None):\n",
    "    if name:\n",
    "        print(f\"\\n{name} (top {n} rows):\")\n",
    "    display(df.head(n))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "79fe16ce69d0f3b",
   "metadata": {},
   "source": [
    "## 2. Data Collection & Panel Integrity\n",
    "\n",
    "### 2.1 Load variable dictionary (for documentation)\n",
    "\n",
    "We load the provided variable dictionary (`Variables.xlsx`) to:\n",
    "- validate required Compustat mnemonics exist in the data file,\n",
    "- generate appendix-ready variable tables.\n",
    "\n",
    "This step **does not** transform the modeling data."
   ]
  },
  {
   "cell_type": "code",
   "id": "94aa62c32a506339",
   "metadata": {},
   "source": [
    "vars_path = Path(CONFIG[\"VARIABLES_XLSX_PATH\"])\n",
    "if vars_path.exists():\n",
    "    var_dict = pd.read_excel(vars_path, sheet_name=0)\n",
    "    var_dict.columns = [c.strip() for c in var_dict.columns]\n",
    "    print(f\"Loaded variable dictionary with {len(var_dict)} rows from: {vars_path}\")\n",
    "    display(var_dict.head(90))\n",
    "else:\n",
    "    var_dict = pd.DataFrame(columns=[\"Variable\",\"Two-word Description\",\"Category\"])\n",
    "    print(f\"WARNING: variable dictionary not found at {vars_path}. Continuing without it.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bcacd51e712d2cf7",
   "metadata": {},
   "source": [
    "### 2.2 Load raw data (no imputation or transformations)"
   ]
  },
  {
   "cell_type": "code",
   "id": "1e92457af45ce6cc",
   "metadata": {},
   "source": [
    "data_path = Path(CONFIG[\"DATA_PATH\"])\n",
    "df_raw = pd.read_csv(data_path, low_memory=False)\n",
    "print(f\"Loaded data from {data_path} with shape {df_raw.shape}\")\n",
    "\n",
    "display(df_raw.head())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dbc58d6530cba72",
   "metadata": {},
   "source": [
    "### 2.3 Enforce panel identifiers, types, sorting, and deduplication"
   ]
  },
  {
   "cell_type": "code",
   "id": "e10f5a0807ac2d0a",
   "metadata": {},
   "source": [
    "df = df_raw.copy()\n",
    "\n",
    "# Stable firm identifier\n",
    "if \"gvkey\" not in df.columns:\n",
    "    raise ValueError(\"Required identifier column `gvkey` not found in the dataset.\")\n",
    "df[\"firm_id\"] = df[\"gvkey\"].astype(str)\n",
    "\n",
    "# Fiscal year\n",
    "if \"fyear\" not in df.columns:\n",
    "    raise ValueError(\"Required time column `fyear` not found in the dataset.\")\n",
    "df[\"fyear\"] = pd.to_numeric(df[\"fyear\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Optional datadate parsing (kept as metadata; not used for splitting)\n",
    "if \"datadate\" in df.columns:\n",
    "    df[\"datadate\"] = pd.to_datetime(df[\"datadate\"], errors=\"coerce\")\n",
    "\n",
    "# Remove firm-year duplicates (keep-last rule, audit count)\n",
    "pre_n = len(df)\n",
    "dup_mask = df.duplicated(subset=[\"firm_id\",\"fyear\"], keep=False)\n",
    "n_dups = int(dup_mask.sum())\n",
    "if n_dups > 0:\n",
    "    print(f\"Found {n_dups} duplicated firm-year rows. Applying keep-last rule.\")\n",
    "    df = df.sort_values([\"firm_id\",\"fyear\",\"datadate\"] if \"datadate\" in df.columns else [\"firm_id\",\"fyear\"])\n",
    "    df = df.drop_duplicates(subset=[\"firm_id\",\"fyear\"], keep=\"last\")\n",
    "post_n = len(df)\n",
    "\n",
    "# Enforce sort order for lag/lead safety\n",
    "df = df.sort_values([\"firm_id\",\"fyear\"]).reset_index(drop=True)\n",
    "\n",
    "# Integrity checks\n",
    "assert df[[\"firm_id\",\"fyear\"]].isna().sum().sum() == 0, \"Missing firm_id or fyear after typing.\"\n",
    "assert df.duplicated(subset=[\"firm_id\",\"fyear\"]).sum() == 0, \"Duplicate firm-year keys remain after dedup.\"\n",
    "\n",
    "print(f\"Rows: {pre_n:,} -> {post_n:,} after deduplication.\")\n",
    "print(\"Unique firms:\", df[\"firm_id\"].nunique())\n",
    "print(\"Year range:\", int(df[\"fyear\"].min()), \"to\", int(df[\"fyear\"].max()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5820c5a4b33b70f",
   "metadata": {},
   "source": [
    "### 2.4 Raw sample composition (no transformations)"
   ]
  },
  {
   "cell_type": "code",
   "id": "428f568e664e713e",
   "metadata": {},
   "source": [
    "# Minimal sample composition diagnostics (kept lightweight for large panels)\n",
    "\n",
    "by_year = df.groupby(\"fyear\").agg(\n",
    "    n_obs=(\"firm_id\",\"size\"),\n",
    "    n_firms=(\"firm_id\",\"nunique\"),\n",
    ").reset_index()\n",
    "\n",
    "display(by_year.tail(12))\n",
    "\n",
    "# Optional: industry composition if SIC exists\n",
    "if \"sic\" in df.columns:\n",
    "    df[\"sic2\"] = pd.to_numeric(df[\"sic\"], errors=\"coerce\").astype(\"Int64\") // 100\n",
    "    by_sic2 = df.groupby(\"sic2\").size().sort_values(ascending=False).head(15).rename(\"n_obs\").reset_index()\n",
    "    display(by_sic2)\n",
    "else:\n",
    "    print(\"Note: `sic` not present; skipping industry composition.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7974cbff71f71876",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning & Missingness Handling (leakage-aware)\n",
    "\n",
    "### 3.1 Non-imputable identifiers and label-year setup\n",
    "\n",
    "We drop observations missing non-imputable identifiers (firm, year).  \n",
    "We also define `label_year = fyear + 1` as the *outcome year* used for forecasting splits."
   ]
  },
  {
   "cell_type": "code",
   "id": "f7adbd880b32e81b",
   "metadata": {},
   "source": [
    "# Drop rows with missing key identifiers (already asserted, but keep explicit)\n",
    "df = df.dropna(subset=[\"firm_id\",\"fyear\"]).copy()\n",
    "\n",
    "# label_year defines the year of the t+1 distress label\n",
    "df[\"label_year\"] = (df[\"fyear\"] + 1).astype(\"Int64\")\n",
    "\n",
    "# Remove excluded variables from dataframe before split (global removal)\n",
    "EXCLUDED_VARS = [\n",
    "    \"aco_act\", \"aqc_at\", \"caps_at\", \"capx_at\", \"dp_at\",\n",
    "    \"invch_act\", \"lco_lct\", \"mibt_at\", \"recch_act\",\n",
    "    \"txditc_at\", \"txp_lct\", \"xint_lct\"\n",
    "]\n",
    "df = df.drop(columns=EXCLUDED_VARS, errors=\"ignore\")\n",
    "print(f\"Dropped excluded variables: {EXCLUDED_VARS}\")\n",
    "\n",
    "# Split masks (defined early; used for leakage-safe preprocessing throughout)\n",
    "train_pool_mask = df[\"label_year\"] <= CONFIG[\"TRAIN_CUTOFF_LABEL_YEAR\"]\n",
    "train_pool_years = sorted(df.loc[train_pool_mask, \"label_year\"].dropna().unique().tolist())\n",
    "if len(train_pool_years) < (CONFIG[\"VAL_YEARS\"] + 1):\n",
    "    raise ValueError(\"Not enough label years in train pool to allocate validation years. Adjust TRAIN_CUTOFF_LABEL_YEAR or VAL_YEARS.\")\n",
    "\n",
    "val_years = train_pool_years[-CONFIG[\"VAL_YEARS\"]:]\n",
    "val_mask = df[\"label_year\"].isin(val_years)\n",
    "train_mask = train_pool_mask & (~val_mask)\n",
    "test_mask = df[\"label_year\"] > CONFIG[\"TRAIN_CUTOFF_LABEL_YEAR\"]\n",
    "\n",
    "df[\"split\"] = np.where(test_mask, \"test\", np.where(val_mask, \"val\", \"train\"))\n",
    "\n",
    "print(\"Split counts:\")\n",
    "display(df[\"split\"].value_counts(dropna=False).to_frame(\"n_obs\"))\n",
    "print(\"Validation label_year(s):\", val_years)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f506672fa701c72e",
   "metadata": {},
   "source": [
    "### 3.2 Missingness audit before intervention"
   ]
  },
  {
   "cell_type": "code",
   "id": "4e9d8e9b71ab42be",
   "metadata": {},
   "source": [
    "# Identify numeric columns eligible for imputation (exclude identifiers)\n",
    "id_cols = {\"gvkey\",\"firm_id\",\"fyear\",\"label_year\",\"datadate\",\"split\"}\n",
    "numeric_cols = [c for c in df.columns if c not in id_cols and pd.api.types.is_numeric_dtype(df[c])]\n",
    "\n",
    "missing_tbl = (df[numeric_cols].isna().mean().sort_values(ascending=False) * 100).rename(\"missing_%\").to_frame()\n",
    "missing_tbl[\"n_missing\"] = df[numeric_cols].isna().sum().astype(int)\n",
    "missing_tbl[\"dtype\"] = [str(df[c].dtype) for c in missing_tbl.index]\n",
    "\n",
    "display(missing_tbl.head(25))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "30906c08470c1af8",
   "metadata": {},
   "source": [
    "### 3.3 Create missingness indicators (informative signals)"
   ]
  },
  {
   "cell_type": "code",
   "id": "dda86fada9b859f3",
   "metadata": {},
   "source": [
    "# Choose a focused set of inputs used for core ratios/events.\n",
    "REQUIRED_RAW = [\n",
    "    \"at\",\"dlc\",\"dltt\",\"seq\",\"mibt\",\"niadj\",\n",
    "    \"oibdp\",\"oancf\",\"xint\",\n",
    "    \"act\",\"lct\",\"che\",\"rect\",\"invt\",\n",
    "    # dividend-related (we will auto-detect among these later)\n",
    "    \"dv\",\"dvc\",\"dvt\",\"dvp\",\n",
    "]\n",
    "available_required = [c for c in REQUIRED_RAW if c in df.columns]\n",
    "\n",
    "# Hard requirement for the distress proxy; fail if absent (unless synthetic mode)\n",
    "HARD_REQUIRED = [\"at\",\"dlc\",\"dltt\",\"seq\",\"oibdp\",\"niadj\",\"oancf\"]\n",
    "missing_hard = [c for c in HARD_REQUIRED if c not in df.columns]\n",
    "if missing_hard and not USING_SYNTHETIC_DATA:\n",
    "    raise ValueError(f\"Missing required columns for distress proxy construction: {missing_hard}\")\n",
    "\n",
    "for c in available_required:\n",
    "    df[f\"fmiss_{c}\"] = df[c].isna().astype(\"Int8\")\n",
    "\n",
    "print(\"Created missingness flags for:\", available_required)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eb834853acac11f4",
   "metadata": {},
   "source": [
    "### 3.4 Training-derived size deciles (used for peer imputation groups)"
   ]
  },
  {
   "cell_type": "code",
   "id": "27dfe85deadce822",
   "metadata": {},
   "source": [
    "# Size is based on log(assets) from TRAIN only, to avoid leakage.\n",
    "at_train = pd.to_numeric(df.loc[train_mask, \"at\"], errors=\"coerce\")\n",
    "log_at_train = np.log(at_train.where(at_train > 0)).dropna()\n",
    "\n",
    "if len(log_at_train) < 50:\n",
    "    print(\"WARNING: too few non-missing training `at` values for stable size deciles. Using a single size bin.\")\n",
    "    df[\"size_decile\"] = 5  # arbitrary mid-bin\n",
    "    size_edges = None\n",
    "else:\n",
    "    # Use quantile cutpoints computed on training only\n",
    "    qs = np.linspace(0, 1, 11)\n",
    "    size_edges = log_at_train.quantile(qs).values\n",
    "    size_edges[0] = -np.inf\n",
    "    size_edges[-1] = np.inf\n",
    "\n",
    "    log_at_all = np.log(pd.to_numeric(df[\"at\"], errors=\"coerce\").where(lambda s: s > 0))\n",
    "    df[\"size_decile\"] = pd.cut(log_at_all, bins=size_edges, labels=False, include_lowest=True).astype(\"Float64\")\n",
    "\n",
    "# Fill NA size_decile with training median decile for downstream stability\n",
    "sd_med = float(pd.to_numeric(df.loc[train_mask, \"size_decile\"], errors=\"coerce\").median())\n",
    "df[\"size_decile\"] = pd.to_numeric(df[\"size_decile\"], errors=\"coerce\").fillna(sd_med).astype(int)\n",
    "\n",
    "print(\"Size decile distribution (train):\")\n",
    "display(df.loc[train_mask, \"size_decile\"].value_counts().sort_index().to_frame(\"n_obs\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a04b877231e2d42e",
   "metadata": {},
   "source": [
    "### 3.5 Imputation Pipeline\n",
    "\n",
    "I build the imputation pipeline in two stages:\n",
    "1. KNN on core structural statements (train-fit only).\n",
    "2. Peer-median imputation for sparse or secondary items (train-fit only).\n",
    "\n",
    "Targets are computed from the pre-imputation snapshot, so label construction never depends on imputed values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a384b0b905eeead1",
   "metadata": {},
   "source": [
    "# Snapshot before any imputation\n",
    "df_pre_impute_snapshot = df.copy(deep=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "448740b4c5e3073a",
   "metadata": {},
   "source": [
    "### 3.5.1 KNN imputation on core structural items (train-fit; signed-log transform)\n",
    "\n",
    "I use KNN on core balance sheet and income statement aggregates. These fields are tightly linked (e.g., assets vs. liabilities), so multivariate neighbors tend to preserve accounting structure.\n",
    "\n",
    "### 3.5.2 KNN parameter selection audit\n",
    "\n",
    "I audit reconstruction error across a grid of $K$ values using training-only complete rows with simulated missingness. The chosen KNN setting is the value configured in `CONFIG[\"KNN_K\"]`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "613726cbe6565efa",
   "metadata": {},
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "# pyampute (audit missingness generation)\n",
    "try:\n",
    "    from pyampute.ampute import MultivariateAmputation\n",
    "except Exception as e:\n",
    "    raise ImportError(\n",
    "        \"pyampute is required for the KNN audit. Install via: pip install pyampute\\n\"\n",
    "        f\"Import error: {e}\"\n",
    "    )\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Core structural variables for KNN (NO fyear / size_decile)\n",
    "# ---------------------------------------------------------------------\n",
    "knn_cols = [\n",
    "    \"at\", \"act\", \"lct\", \"che\", \"rect\", \"invt\", \"dlc\", \"dltt\",\n",
    "    \"seq\", \"ceq\", \"lt\", \"ppent\", \"intan\", \"oibdp\", \"niadj\",\n",
    "    \"oancf\", \"xint\", \"dp\", \"re\", \"capx\"\n",
    "]\n",
    "knn_cols = [c for c in knn_cols if c in df.columns]\n",
    "\n",
    "def signed_log1p(x):\n",
    "    x = pd.to_numeric(x, errors=\"coerce\")\n",
    "    return np.sign(x) * np.log1p(np.abs(x))\n",
    "\n",
    "def inverse_signed_log1p(z):\n",
    "    z = pd.to_numeric(z, errors=\"coerce\")\n",
    "    return np.sign(z) * (np.expm1(np.abs(z)))\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# pyampute-based audit: NRMSE on forced-missing cells (TRAIN only)\n",
    "# ---------------------------------------------------------------------\n",
    "def knn_audit_pyampute_train_only(\n",
    "    Z_train: pd.DataFrame,\n",
    "    knn_cols: list,\n",
    "    k_list=(5, 10, 25, 50, 100),\n",
    "    prop_rows_incomplete=0.50,\n",
    "    row_subsample=2000,\n",
    "    seed=42\n",
    "):\n",
    "    # complete TRAIN rows only (pyampute requirement)\n",
    "    Zc = Z_train.copy().apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
    "    if len(Zc) < 200:\n",
    "        print(f\"[KNN audit] Not enough complete TRAIN rows: n={len(Zc)}\")\n",
    "        return None\n",
    "\n",
    "    if len(Zc) > row_subsample:\n",
    "        Zc = Zc.sample(n=row_subsample, random_state=seed)\n",
    "\n",
    "    std = Zc[knn_cols].std(ddof=0).replace(0, np.nan)\n",
    "\n",
    "    # KEY FIX: one-variable patterns (so rows are not fully missing)\n",
    "    patterns = [\n",
    "        {\"incomplete_vars\": [c], \"mechanism\": \"MCAR\", \"freq\": 1.0/len(knn_cols)}\n",
    "        for c in knn_cols\n",
    "    ]\n",
    "\n",
    "    ma = MultivariateAmputation(\n",
    "        prop=float(prop_rows_incomplete),\n",
    "        patterns=patterns,\n",
    "        std=False,\n",
    "        seed=int(seed),\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    Za = ma.fit_transform(Zc)\n",
    "    Za = pd.DataFrame(Za, columns=Zc.columns, index=Zc.index)\n",
    "\n",
    "    introduced = Za[knn_cols].isna() & Zc[knn_cols].notna()\n",
    "    n_amputed = int(introduced.values.sum())\n",
    "    if n_amputed == 0:\n",
    "        print(\"[KNN audit] No cells amputated; increase prop_rows_incomplete.\")\n",
    "        return None\n",
    "\n",
    "    rows = []\n",
    "    for k in k_list:\n",
    "        imp = KNNImputer(n_neighbors=int(k), weights=\"distance\")\n",
    "        imp.fit(Zc)\n",
    "        Zimp = pd.DataFrame(imp.transform(Za), columns=Za.columns, index=Za.index)\n",
    "\n",
    "        per = {}\n",
    "        sqerrs = []\n",
    "        var_w = []\n",
    "        cnt = 0\n",
    "\n",
    "        for c in knn_cols:\n",
    "            m = introduced[c].values\n",
    "            if m.sum() == 0 or pd.isna(std[c]) or std[c] <= 0:\n",
    "                continue\n",
    "            y_true = Zc[c].values[m]\n",
    "            y_pred = Zimp[c].values[m]\n",
    "            rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "            nrmse = float(rmse / std[c])\n",
    "            per[f\"NRMSE_{c}\"] = nrmse\n",
    "\n",
    "            sqerrs.append((y_true - y_pred) ** 2)\n",
    "            var_w.append((std[c] ** 2) * m.sum())\n",
    "            cnt += int(m.sum())\n",
    "\n",
    "        pooled_nrmse = np.nan\n",
    "        if cnt > 0 and sqerrs:\n",
    "            pooled_mse = float(np.mean(np.concatenate(sqerrs)))\n",
    "            pooled_rmse = float(np.sqrt(pooled_mse))\n",
    "            pooled_std = float(np.sqrt(np.sum(var_w) / cnt))\n",
    "            pooled_nrmse = float(pooled_rmse / pooled_std) if pooled_std > 0 else np.nan\n",
    "\n",
    "        rows.append({\n",
    "            \"K\": int(k),\n",
    "            \"amputated_cells\": n_amputed,\n",
    "            \"pooled_NRMSE\": pooled_nrmse,\n",
    "            **per\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values(\"K\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Main: build Z (ONLY knn_cols), run audit (TRAIN only), then impute df\n",
    "# ---------------------------------------------------------------------\n",
    "if len(knn_cols) >= 3:\n",
    "    # Build Z (NO fyear / size_decile)\n",
    "    Z = df[knn_cols].copy()\n",
    "\n",
    "    # Transform magnitudes for distance stability\n",
    "    for c in knn_cols:\n",
    "        Z[c] = signed_log1p(Z[c])\n",
    "\n",
    "    # ---- pyampute audit (TRAIN only) ----\n",
    "    print(\"Auditing KNN imputation via pyampute (TRAIN only, forced-missing cells, NRMSE)...\")\n",
    "    k_options = [5, 10, 25, 50, 100]\n",
    "    audit_tbl = knn_audit_pyampute_train_only(\n",
    "        Z_train=Z.loc[train_mask, :],\n",
    "        knn_cols=knn_cols,\n",
    "        k_list=k_options,\n",
    "        row_subsample=2000,\n",
    "        seed=SEED if \"SEED\" in globals() else 42\n",
    "    )\n",
    "    if audit_tbl is not None:\n",
    "        display(audit_tbl)\n",
    "\n",
    "    # ---- Production imputation (train-fit) ----\n",
    "    imputer = KNNImputer(n_neighbors=CONFIG[\"KNN_K\"], weights=\"distance\")\n",
    "    imputer.fit(Z.loc[train_mask, :])\n",
    "\n",
    "    Z_imp = pd.DataFrame(imputer.transform(Z), columns=Z.columns, index=Z.index)\n",
    "\n",
    "    # Invert signed-log transform back for magnitudes and write into df\n",
    "    for c in knn_cols:\n",
    "        df[c] = inverse_signed_log1p(Z_imp[c])\n",
    "\n",
    "else:\n",
    "    print(\"Skipping KNN imputation: insufficient columns available.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9285a25c1152e642",
   "metadata": {},
   "source": [
    "### 3.6 Train-only peer-median imputation (fyear \u00d7 size_decile)\n",
    "\n",
    "I use year-by-size median imputation for sparse flows (dividends, buybacks) where KNN would overfit noise or fill in non-existent activity. The medians are computed on training data only, with size-decile and global fallbacks for unseen groups.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1829bef74e467c4a",
   "metadata": {},
   "source": [
    "# Secondary/incidental variables for Peer Median\n",
    "# Removed raw variables that create excluded ratios: aco, lco, recch, invch, txp, txditc, caps, mibt, aqc\n",
    "peer_impute_candidates = [\n",
    "    \"prstkc\",\n",
    "    \"dv\", \"dvc\", \"dvt\", \"dvp\"\n",
    "]\n",
    "peer_impute_cols = [c for c in peer_impute_candidates if c in df.columns]\n",
    "\n",
    "group_cols = [\"fyear\",\"size_decile\"]\n",
    "\n",
    "def peer_median_impute(df_in: pd.DataFrame, cols: list[str], train_mask: pd.Series, group_cols: list[str]) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Impute NaNs using TRAIN-only medians by group_cols, with TRAIN (size_decile) then global median fallback.\"\"\"\n",
    "    df_out = df_in.copy()\n",
    "    train = df_out.loc[train_mask, group_cols + cols].copy()\n",
    "    group_meds = train.groupby(group_cols)[cols].median()\n",
    "    global_meds = train[cols].median()\n",
    "\n",
    "    # Intermediate fallback for unseen (fyear, size_decile): use TRAIN size_decile medians\n",
    "    size_meds = train.groupby([\"size_decile\"])[cols].median()\n",
    "    tmp_size = df_out[[\"size_decile\"]].merge(size_meds.reset_index(), on=\"size_decile\", how=\"left\")\n",
    "\n",
    "    # Join group medians (wide) to all rows\n",
    "    tmp = df_out[group_cols].merge(group_meds.reset_index(), on=group_cols, how=\"left\", suffixes=(\"\", \"_peer\"))\n",
    "    # tmp currently contains the group median columns with original names\n",
    "    for c in cols:\n",
    "        peer_med = tmp[c]\n",
    "        df_out[c] = df_out[c].where(df_out[c].notna(), peer_med)\n",
    "        size_med = tmp_size[c]\n",
    "        df_out[c] = df_out[c].where(df_out[c].notna(), size_med)\n",
    "        df_out[c] = df_out[c].where(df_out[c].notna(), global_meds[c])\n",
    "    impact = pd.DataFrame({\n",
    "        \"col\": cols,\n",
    "        \"n_imputed\": [int(df_in[c].isna().sum() - df_out[c].isna().sum()) for c in cols],\n",
    "        \"train_global_median\": [float(global_meds[c]) if pd.notna(global_meds[c]) else np.nan for c in cols],\n",
    "    })\n",
    "    return df_out, impact\n",
    "\n",
    "df, peer_impact = peer_median_impute(df, peer_impute_cols, train_mask, group_cols)\n",
    "\n",
    "display(peer_impact.sort_values(\"n_imputed\", ascending=False).head(15))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "18b7f581e29e9170",
   "metadata": {},
   "source": [
    "### 3.7 Guardrail capping of imputed magnitudes (train quantile bands)\n",
    "\n",
    "After imputation, I clip imputed values to training-only quantile bands. This prevents imputation artifacts from injecting extreme tails into the modeling features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6377a5e929c6a27e",
   "metadata": {},
   "source": [
    "# Apply capping to all columns that underwent imputation (KNN and Peer Median)\n",
    "cap_cols = list(set(knn_cols + peer_impute_cols))\n",
    "bounds = {}\n",
    "\n",
    "for c in cap_cols:\n",
    "    lo, hi = winsorize_train_bounds(df_pre_impute_snapshot.loc[train_mask, c], CONFIG[\"IMPUTE_LO_Q\"], CONFIG[\"IMPUTE_HI_Q\"])\n",
    "    bounds[c] = {\"lo\": lo, \"hi\": hi}\n",
    "    df[c] = apply_bounds(df[c], lo, hi)\n",
    "\n",
    "bounds_df = pd.DataFrame({c: (v[\"lo\"], v[\"hi\"]) for c,v in bounds.items()}, index=[\"lo\",\"hi\"]).T\n",
    "bounds_df.index.name = \"col\"\n",
    "display(bounds_df.head(15))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9722603c795d906",
   "metadata": {},
   "source": [
    "### 3.8 Imputation impact audit (pre vs post)\n",
    "\n",
    "I compare distributional summaries before and after imputation so I can see if imputation is shifting levels or compressing tails.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "178edf05b6a9d757",
   "metadata": {},
   "source": [
    "audit_cols = [c for c in [\"at\",\"dlc\",\"dltt\",\"seq\",\"oibdp\",\"oancf\",\"act\",\"lct\"] if c in df.columns]\n",
    "\n",
    "def dist_summary(x: pd.Series) -> dict:\n",
    "    x = pd.to_numeric(x, errors=\"coerce\")\n",
    "    return {\n",
    "        \"n\": int(x.notna().sum()),\n",
    "        \"mean\": float(x.mean()) if x.notna().any() else np.nan,\n",
    "        \"p50\": float(x.median()) if x.notna().any() else np.nan,\n",
    "        \"p10\": float(x.quantile(0.10)) if x.notna().any() else np.nan,\n",
    "        \"p90\": float(x.quantile(0.90)) if x.notna().any() else np.nan,\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "for c in audit_cols:\n",
    "    pre = dist_summary(df_pre_impute_snapshot[c])\n",
    "    post = dist_summary(df[c])\n",
    "    rows.append({\n",
    "        \"col\": c,\n",
    "        \"n_pre\": pre[\"n\"],\n",
    "        \"n_post\": post[\"n\"],\n",
    "        \"mean_pre\": pre[\"mean\"],\n",
    "        \"mean_post\": post[\"mean\"],\n",
    "        \"p50_pre\": pre[\"p50\"],\n",
    "        \"p50_post\": post[\"p50\"],\n",
    "    })\n",
    "impact_tbl = pd.DataFrame(rows).sort_values(\"col\")\n",
    "display(impact_tbl)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d2b8062a2c6d01a1",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "This section is a quick audit of signal and data quality. It uses the imputed feature set (post-cleaning) and reports results by split.\n",
    "\n",
    "### 4.1 Summary statistics by split (key magnitudes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b7a010553b3bd5e3",
   "metadata": {},
   "source": [
    "eda_cols = [c for c in [\"at\",\"dlc\",\"dltt\",\"seq\",\"oibdp\",\"oancf\",\"xint\"] if c in df.columns]\n",
    "\n",
    "def split_describe(df_in: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    out = []\n",
    "    for sp in [\"train\",\"val\",\"test\"]:\n",
    "        d = df_in.loc[df_in[\"split\"]==sp, cols].describe(percentiles=[0.01,0.1,0.5,0.9,0.99]).T\n",
    "        d.insert(0, \"split\", sp)\n",
    "        d.insert(1, \"col\", d.index)\n",
    "        out.append(d.reset_index(drop=True))\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "desc_tbl = split_describe(df, eda_cols)\n",
    "display(desc_tbl.head(20))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c218c28e3576de1e",
   "metadata": {},
   "source": [
    "### 4.2 Missingness rates after imputation (key inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "745a6b6e06db22e8",
   "metadata": {},
   "source": [
    "# Recompute missingness after imputation (exclude prior missingness flags)\n",
    "id_cols = {\"gvkey\",\"firm_id\",\"fyear\",\"label_year\",\"datadate\",\"split\"}\n",
    "numeric_cols = [\n",
    "    c for c in df.columns\n",
    "    if c not in id_cols\n",
    "    and not c.startswith(\"fmiss_\")\n",
    "    and pd.api.types.is_numeric_dtype(df[c])\n",
    "]\n",
    "\n",
    "missing_tbl = (\n",
    "    df[numeric_cols].isna().mean().sort_values(ascending=False) * 100\n",
    ").rename(\"missing_%\").to_frame()\n",
    "missing_tbl[\"n_missing\"] = df[numeric_cols].isna().sum().astype(int)\n",
    "missing_tbl[\"dtype\"] = [str(df[c].dtype) for c in missing_tbl.index]\n",
    "\n",
    "display(missing_tbl.head(25))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f331fe19a6617b21",
   "metadata": {},
   "source": [
    "### 4.3 Visual sanity-check plots (train vs test distributions)"
   ]
  },
  {
   "cell_type": "code",
   "id": "8c1e5a3ee4b031b4",
   "metadata": {},
   "source": [
    "# Lightweight plots to spot gross drift / outliers.\n",
    "plot_cols = [c for c in [\"at\",\"dltt\",\"dlc\",\"oibdp\",\"oancf\"] if c in df.columns]\n",
    "\n",
    "for c in plot_cols[:3]:\n",
    "    a = pd.to_numeric(df.loc[df[\"split\"]==\"train\", c], errors=\"coerce\")\n",
    "    b = pd.to_numeric(df.loc[df[\"split\"]==\"test\", c], errors=\"coerce\")\n",
    "    plt.figure()\n",
    "    plt.hist(np.log1p(a.dropna()), bins=60, alpha=0.5, label=\"train\")\n",
    "    plt.hist(np.log1p(b.dropna()), bins=60, alpha=0.5, label=\"test\")\n",
    "    plt.title(f\"log1p({c}) distribution: train vs test\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(CONFIG[\"FIG_DIR\"]) / f\"eda_log1p_{c}_train_vs_test.png\", dpi=140)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "264bc0cd8892e5f5",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering & Target Construction\n",
    "\n",
    "I derive all ratio features and events from the cleaned inputs. The distress targets are still computed from the raw (pre-imputation) snapshot to avoid label leakage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "4110b3d5e41f0c87",
   "metadata": {},
   "source": [
    "# Variables to exclude from modeling (removed per requirements)\n",
    "EXCLUDED_VARS = [\n",
    "    \"aco_act\", \"aqc_at\", \"caps_at\", \"capx_at\", \"dp_at\", \n",
    "    \"invch_act\", \"lco_lct\", \"mibt_at\", \"recch_act\", \n",
    "    \"txditc_at\", \"txp_lct\", \"xint_lct\"\n",
    "]\n",
    "\n",
    "FEATURES_V1 = [\n",
    "    \"ln_at\", \"cash_at\", \"current_ratio\", \"nwc_at\", \n",
    "    \"rect_act\", \"invt_act\", \n",
    "    \"lt_at\", \"dlc_at\", \"dltt_at\", \n",
    "    \"debt_at\", \"st_debt_share\", \"ebitda_at\", \n",
    "    \"xint_at\", \"interest_coverage\", \"debt_to_ebitda\", \"ebit_to_capital\"\n",
    "]\n",
    "\n",
    "FEATURES_V2 = [\n",
    "    \"ln_at\", \"cash_at\", \"current_ratio\", \"nwc_at\", \n",
    "    \"rect_act\", \"invt_act\", \"ppent_at\", \"intan_at\", \n",
    "    \"lt_at\", \"debt_at\", \"st_debt_share\", \n",
    "    \"ebitda_at\", \"xint_at\", \"interest_coverage\", \"debt_to_ebitda\", \n",
    "    \"ebit_to_capital\", \"ocf_to_debt\", \"fcf_to_debt\",\n",
    "]\n",
    "\n",
    "FEATURES_V3 = [\n",
    "    \"ln_at\", \"cash_at\", \"current_ratio\", \"nwc_at\", \n",
    "    \"rect_act\", \"invt_act\", \n",
    "    \"lt_at\", \"ceq_at\", \"re_at\", \n",
    "    \"niadj_at\", \"loss_indicator\", \n",
    "    \"xint_at\", \"prstkc_at\"\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "403eb178f3368c87",
   "metadata": {},
   "source": [
    "### 5.2 Debt, capital, and operating aggregates"
   ]
  },
  {
   "cell_type": "code",
   "id": "bbe02b8c26fe3d73",
   "metadata": {},
   "source": [
    "# Ensure all required raw items are numeric for safe arithmetic\n",
    "raw_items = [\n",
    "    \"at\", \"che\", \"act\", \"lct\", \"aco\", \"lco\", \"rect\", \"invt\", \"recch\", \"invch\",\n",
    "    \"txp\", \"txditc\", \"lt\", \"dlc\", \"dltt\", \"oibdp\", \"dp\", \"xint\", \"ceq\", \"capx\",\n",
    "    \"ppent\", \"intan\", \"oancf\", \"re\", \"caps\", \"mibt\", \"niadj\", \"aqc\", \"prstkc\", \"seq\"\n",
    "]\n",
    "for c in raw_items:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Debt aggregate\n",
    "df[\"total_debt\"] = df[[\"dlc\",\"dltt\"]].sum(axis=1, min_count=1)\n",
    "\n",
    "# Equity plus minority interest (if available)\n",
    "if \"mibt\" in df.columns:\n",
    "    df[\"equity_plus_mi\"] = df[[\"seq\",\"mibt\"]].sum(axis=1, min_count=1)\n",
    "else:\n",
    "    df[\"equity_plus_mi\"] = df[\"seq\"]\n",
    "\n",
    "# Total capital and a non-positive capital flag\n",
    "df[\"total_capital\"] = df[[\"total_debt\",\"equity_plus_mi\"]].sum(axis=1, min_count=1)\n",
    "df[\"cap_nonpos_flag\"] = (df[\"total_capital\"] <= 0).astype(\"Int8\")\n",
    "\n",
    "# EBITDA proxy\n",
    "df[\"ebitda_proxy\"] = df[\"oibdp\"]\n",
    "df[\"ebitda_nonpos_flag\"] = (df[\"ebitda_proxy\"] <= 0).astype(\"Int8\")\n",
    "\n",
    "# Log transforms\n",
    "df[\"ln_at\"] = np.log(df[\"at\"].where(lambda s: s > 0))\n",
    "# Legacy name if needed elsewhere\n",
    "df[\"log_at\"] = df[\"ln_at\"]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ba92599df400e45f",
   "metadata": {},
   "source": [
    "### 5.3 Leverage, coverage, and cash-flow ratios (V1, V2, V3 features)"
   ]
  },
  {
   "cell_type": "code",
   "id": "49ff396c591089c2",
   "metadata": {},
   "source": [
    "# --- V1/V2/V3 Shared & Specific Features ---\n",
    "# (Using safe_divide which handles division by zero and returns NaN for extreme states)\n",
    "\n",
    "# Basic Ratios\n",
    "df[\"cash_at\"] = safe_divide(df[\"che\"], df[\"at\"])\n",
    "df[\"current_ratio\"] = safe_divide(df[\"act\"], df[\"lct\"])\n",
    "df[\"nwc_at\"] = safe_divide(df[\"act\"] - df[\"lct\"], df[\"at\"])\n",
    "# Removed: aco_act, lco_lct (excluded variables)\n",
    "df[\"rect_act\"] = safe_divide(df[\"rect\"], df[\"act\"])\n",
    "df[\"invt_act\"] = safe_divide(df[\"invt\"], df[\"act\"])\n",
    "# Removed: recch_act, invch_act, txp_lct, txditc_at (excluded variables)\n",
    "df[\"lt_at\"] = safe_divide(df[\"lt\"], df[\"at\"])\n",
    "df[\"dlc_at\"] = safe_divide(df[\"dlc\"], df[\"at\"])\n",
    "df[\"dltt_at\"] = safe_divide(df[\"dltt\"], df[\"at\"])\n",
    "df[\"debt_at\"] = safe_divide(df[\"total_debt\"], df[\"at\"])\n",
    "df[\"st_debt_share\"] = safe_divide(df[\"dlc\"], df[\"total_debt\"])\n",
    "df[\"ebitda_at\"] = safe_divide(df[\"oibdp\"], df[\"at\"])\n",
    "# Removed: dp_at (excluded variable)\n",
    "df[\"xint_at\"] = safe_divide(df[\"xint\"], df[\"at\"])\n",
    "df[\"interest_coverage\"] = safe_divide(df[\"oibdp\"], df[\"xint\"])\n",
    "df[\"debt_to_ebitda\"] = safe_divide(df[\"total_debt\"], df[\"oibdp\"])\n",
    "df[\"ebit_to_capital\"] = safe_divide(df[\"oibdp\"] - df[\"dp\"], df[\"total_debt\"] + df[\"ceq\"])\n",
    "# Removed: capx_at (excluded variable)\n",
    "\n",
    "# V2 extras\n",
    "df[\"ppent_at\"] = safe_divide(df[\"ppent\"], df[\"at\"])\n",
    "df[\"intan_at\"] = safe_divide(df[\"intan\"], df[\"at\"])\n",
    "df[\"ocf_to_debt\"] = safe_divide(df[\"oancf\"], df[\"total_debt\"])\n",
    "df[\"fcf_to_debt\"] = safe_divide(df[\"oancf\"] - df[\"capx\"], df[\"total_debt\"])\n",
    "\n",
    "# V3 extras\n",
    "df[\"ceq_at\"] = safe_divide(df[\"ceq\"], df[\"at\"])\n",
    "# Removed: caps_at, mibt_at (excluded variables)\n",
    "df[\"niadj_at\"] = safe_divide(df[\"niadj\"], df[\"at\"])\n",
    "df[\"loss_indicator\"] = (df[\"niadj\"] < 0).astype(float)\n",
    "# Removed: xint_lct, aqc_at (excluded variables)\n",
    "df[\"prstkc_at\"] = safe_divide(df[\"prstkc\"], df[\"at\"])\n",
    "\n",
    "# --- Legacy mappings for distress proxy definitions (Section 5.4) ---\n",
    "# (Keeping sp_ prefix for variables used in distress proxy definition rules)\n",
    "ffo_proxy = df[\"oancf\"] + df[\"xint\"]\n",
    "if \"txp\" in df.columns:\n",
    "    ffo_proxy = ffo_proxy - df[\"txp\"]\n",
    "df[\"sp_ffo_to_debt\"] = safe_divide(ffo_proxy, df[\"total_debt\"])\n",
    "df[\"sp_debt_to_capital\"] = safe_divide(df[\"total_debt\"], df[\"total_capital\"])\n",
    "df[\"sp_debt_to_ebitda\"] = df[\"debt_to_ebitda\"]\n",
    "df[\"sp_interest_coverage\"] = df[\"interest_coverage\"].clip(lower=-50, upper=50)\n",
    "\n",
    "# Identify remaining +/-inf (though safe_divide already handles most)\n",
    "ratio_cols = [\"sp_debt_to_capital\",\"sp_debt_to_ebitda\",\"sp_ffo_to_debt\",\"sp_interest_coverage\"]\n",
    "for c in ratio_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].replace([np.inf, -np.inf], np.nan)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e38d0a9c18bddffd",
   "metadata": {},
   "source": [
    "### 5.4 Multiple Distress Proxies (fiscal year t) and next-year supervised labels (t+1)\n",
    "choose the state/transition for OBJECTIVE and choose the version 1,2,3 as a proxy of distress"
   ]
  },
  {
   "cell_type": "code",
   "id": "bdc21f0c06f258c",
   "metadata": {},
   "source": [
    "# Distress proxy thresholds (frozen and documented)\n",
    "DISTRESS_RULE = {\n",
    "    \"FFO_TO_DEBT_LT\": 0.15,\n",
    "    \"DEBT_TO_CAPITAL_GT\": 0.55,\n",
    "    \"DEBT_TO_EBITDA_GT\": 4.5,\n",
    "    \"NEG_EQUITY_SEQ_LE\": 0.0,\n",
    "}\n",
    "\n",
    "# --- Target construction from raw (non-imputed) data ---\n",
    "# We compute distress proxies from the raw snapshot (df_pre_impute_snapshot) \n",
    "# to ensure that target labels are not contaminated by the imputation process.\n",
    "# Imputation is strictly reserved for predictive features.\n",
    "\n",
    "raw_niadj = ensure_nullable_float(df_pre_impute_snapshot[\"niadj\"])\n",
    "raw_oancf = ensure_nullable_float(df_pre_impute_snapshot[\"oancf\"])\n",
    "raw_seq = ensure_nullable_float(df_pre_impute_snapshot[\"seq\"])\n",
    "\n",
    "# S&P components from raw items (propagate missingness - Issue 3)\n",
    "raw_dlc = ensure_nullable_float(df_pre_impute_snapshot[\"dlc\"])\n",
    "raw_dltt = ensure_nullable_float(df_pre_impute_snapshot[\"dltt\"])\n",
    "raw_total_debt = raw_dlc + raw_dltt\n",
    "\n",
    "raw_oibdp = ensure_nullable_float(df_pre_impute_snapshot[\"oibdp\"])\n",
    "raw_xint = ensure_nullable_float(df_pre_impute_snapshot[\"xint\"])\n",
    "raw_txp = ensure_nullable_float(df_pre_impute_snapshot[\"txp\"]) if \"txp\" in df_pre_impute_snapshot.columns else 0\n",
    "\n",
    "raw_ffo = raw_oancf + raw_xint - raw_txp\n",
    "raw_ffo_to_debt = safe_divide(raw_ffo, raw_total_debt)\n",
    "\n",
    "raw_mibt = ensure_nullable_float(df_pre_impute_snapshot[\"mibt\"]) if \"mibt\" in df_pre_impute_snapshot.columns else pd.Series(np.nan, index=df_pre_impute_snapshot.index)\n",
    "raw_equity = ensure_nullable_float(df_pre_impute_snapshot[[\"seq\",\"mibt\"]].sum(axis=1, min_count=1)) if \"mibt\" in df_pre_impute_snapshot.columns else raw_seq\n",
    "raw_total_capital = raw_total_debt + raw_equity\n",
    "\n",
    "raw_debt_to_cap = safe_divide(raw_total_debt, raw_total_capital)\n",
    "raw_debt_to_ebitda = safe_divide(raw_total_debt, raw_oibdp)\n",
    "\n",
    "# V1: Loss + NegCFO (Accounting-based)\n",
    "# Beaver (1966), Ohlson (1980) logic: niadj < 0 and oancf < 0\n",
    "df[\"distress_v1_t\"] = (raw_niadj < 0) & (raw_oancf < 0)\n",
    "# Fix: explicitly set to missing if inputs are missing\n",
    "df.loc[raw_niadj.isna() | raw_oancf.isna(), \"distress_v1_t\"] = pd.NA\n",
    "\n",
    "# V2: Negative Equity\n",
    "df[\"distress_v2_t\"] = raw_seq <= DISTRESS_RULE[\"NEG_EQUITY_SEQ_LE\"]\n",
    "# Fix: explicitly set to missing if inputs are missing\n",
    "df.loc[raw_seq.isna(), \"distress_v2_t\"] = pd.NA\n",
    "\n",
    "# V3: S&P High Leverage Solely (without conditioning on negative equity)\n",
    "cond_ffo = raw_ffo_to_debt < DISTRESS_RULE[\"FFO_TO_DEBT_LT\"]\n",
    "cond_cap = raw_debt_to_cap > DISTRESS_RULE[\"DEBT_TO_CAPITAL_GT\"]\n",
    "cond_ebitda = raw_debt_to_ebitda > DISTRESS_RULE[\"DEBT_TO_EBITDA_GT\"]\n",
    "df[\"distress_v3_t\"] = cond_ffo & cond_cap & cond_ebitda\n",
    "# Fix: explicitly set to missing if inputs are missing\n",
    "df.loc[raw_ffo_to_debt.isna() | raw_debt_to_cap.isna() | raw_debt_to_ebitda.isna(), \"distress_v3_t\"] = pd.NA\n",
    "\n",
    "# Next-year targets: lead of proxies within firm\n",
    "# Fix: Robust adjacency check (exactly fyear + 1) to avoid mislabeling gaps (Issue 1)\n",
    "next_fyear = df.groupby(\"firm_id\")[\"fyear\"].shift(-1)\n",
    "is_adjacent = (next_fyear == (df[\"fyear\"] + 1))\n",
    "\n",
    "df[\"target_next_v1\"] = df.groupby(\"firm_id\")[\"distress_v1_t\"].shift(-1)\n",
    "df.loc[~is_adjacent, \"target_next_v1\"] = pd.NA\n",
    "df[\"target_next_v1\"] = df[\"target_next_v1\"].astype(\"Int8\")\n",
    "\n",
    "df[\"target_next_v2\"] = df.groupby(\"firm_id\")[\"distress_v2_t\"].shift(-1)\n",
    "df.loc[~is_adjacent, \"target_next_v2\"] = pd.NA\n",
    "df[\"target_next_v2\"] = df[\"target_next_v2\"].astype(\"Int8\")\n",
    "\n",
    "df[\"target_next_v3\"] = df.groupby(\"firm_id\")[\"distress_v3_t\"].shift(-1)\n",
    "df.loc[~is_adjacent, \"target_next_v3\"] = pd.NA\n",
    "df[\"target_next_v3\"] = df[\"target_next_v3\"].astype(\"Int8\")\n",
    "\n",
    "# Transition targets (early-warning): 1[distress_t==0 AND distress_{t+1}==1]\n",
    "# - Defined only for observations that are *healthy at t* (distress_t==0) and have an adjacent t+1.\n",
    "# - Rows with distress_t==1 are set to NA (they are not part of the early-warning risk set).\n",
    "for _v in [\"v1\", \"v2\", \"v3\"]:\n",
    "    _dcol = f\"distress_{_v}_t\"\n",
    "    _ncol = f\"target_next_{_v}\"\n",
    "    _tcol = f\"target_transition_{_v}\"\n",
    "\n",
    "    _dcur = pd.to_numeric(df[_dcol], errors=\"coerce\")  # {0,1} with NaNs\n",
    "    _ncur = pd.to_numeric(df[_ncol], errors=\"coerce\")\n",
    "\n",
    "    df[_tcol] = pd.NA\n",
    "    _ok = is_adjacent & _dcur.notna() & _ncur.notna()\n",
    "\n",
    "    _healthy = _ok & (_dcur == 0)\n",
    "    df.loc[_healthy, _tcol] = (_ncur.loc[_healthy] == 1).astype(\"Int8\")\n",
    "\n",
    "    df[_tcol] = df[_tcol].astype(\"Int8\")\n",
    "\n",
    "# -------------------------\n",
    "# Modeling objective + proxy selection\n",
    "# -------------------------\n",
    "# PROXY_VERSION: choose among {\"v1\",\"v2\",\"v3\"}.\n",
    "# OBJECTIVE:\n",
    "#   - \"transition\": early-warning (healthy at t -> distressed at t+1)   [recommended for claims of \"early warning\"]\n",
    "#   - \"state\":       surveillance of the t+1 distress state (includes persistence)\n",
    "PROXY_VERSION = \"v2\"\n",
    "OBJECTIVE = \"transition\"\n",
    "\n",
    "PROXY_NAME = f\"distress_{PROXY_VERSION}_t\"\n",
    "STATE_TARGET_NAME = f\"target_next_{PROXY_VERSION}\"\n",
    "TRANS_TARGET_NAME = f\"target_transition_{PROXY_VERSION}\"\n",
    "TARGET_NAME = TRANS_TARGET_NAME if OBJECTIVE.lower().startswith(\"trans\") else STATE_TARGET_NAME\n",
    "\n",
    "# Label availability / attrition (fixed to check adjacency)\n",
    "df[\"has_next_year_obs\"] = is_adjacent.fillna(False).astype(\"Int8\")\n",
    "\n",
    "target_cols = [\"target_next_v1\", \"target_next_v2\", \"target_next_v3\"]\n",
    "print(\"Distress prevalence (by split) \u2014 multiple targets:\")\n",
    "display(df.groupby(\"split\")[target_cols].mean())\n",
    "\n",
    "print(\"Share of observations with next-year observation (attrition diagnostic):\")\n",
    "display(df.groupby(\"split\")[\"has_next_year_obs\"].mean().rename(\"has_next_rate\").to_frame())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b3de777f601f7d9",
   "metadata": {},
   "source": [
    "### 5.5 Target prevalence and attrition diagnostics (by year and size)"
   ]
  },
  {
   "cell_type": "code",
   "id": "c03670e02ce52bc4",
   "metadata": {},
   "source": [
    "# Target prevalence by label year\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if \"label_year\" not in df.columns and \"fyear\" in df.columns:\n",
    "    df[\"label_year\"] = (df[\"fyear\"] + 1).astype(\"Int64\")\n",
    "\n",
    "if \"split\" not in df.columns:\n",
    "    df[\"split\"] = \"train\"\n",
    "\n",
    "if \"size_decile\" not in df.columns:\n",
    "    if \"train_mask\" not in globals():\n",
    "        train_mask = df[\"split\"].eq(\"train\") if \"split\" in df.columns else pd.Series(True, index=df.index)\n",
    "    if \"at\" in df.columns:\n",
    "        at_train = pd.to_numeric(df.loc[train_mask, \"at\"], errors=\"coerce\")\n",
    "        log_at_train = np.log(at_train.where(at_train > 0)).dropna()\n",
    "        if len(log_at_train) < 50:\n",
    "            df[\"size_decile\"] = 5\n",
    "        else:\n",
    "            qs = np.linspace(0, 1, 11)\n",
    "            size_edges = log_at_train.quantile(qs).values\n",
    "            size_edges[0] = -np.inf\n",
    "            size_edges[-1] = np.inf\n",
    "            log_at_all = np.log(pd.to_numeric(df[\"at\"], errors=\"coerce\").where(lambda s: s > 0))\n",
    "            df[\"size_decile\"] = pd.cut(log_at_all, bins=size_edges, labels=False, include_lowest=True).astype(\"Float64\")\n",
    "        sd_med = float(pd.to_numeric(df.loc[train_mask, \"size_decile\"], errors=\"coerce\").median())\n",
    "        df[\"size_decile\"] = pd.to_numeric(df[\"size_decile\"], errors=\"coerce\").fillna(sd_med).astype(int)\n",
    "    else:\n",
    "        df[\"size_decile\"] = 5\n",
    "\n",
    "# Target prevalence by label year\n",
    "if \"has_next_year_obs\" not in df.columns:\n",
    "    df[\"has_next_year_obs\"] = pd.NA\n",
    "\n",
    "target_cols = [\"target_next_v1\", \"target_next_v2\", \"target_next_v3\"]\n",
    "agg_dict = {\n",
    "    \"n_obs\": (\"firm_id\", \"size\"),\n",
    "    \"has_next_rate\": (\"has_next_year_obs\", \"mean\"),\n",
    "}\n",
    "for c in target_cols:\n",
    "    agg_dict[f\"{c}_rate\"] = (c, \"mean\")\n",
    "\n",
    "by_label_year = df.groupby([\"label_year\", \"split\"]).agg(**agg_dict).reset_index()\n",
    "\n",
    "display(by_label_year.tail(15))\n",
    "\n",
    "# By size decile (train pool), to assess composition effects\n",
    "agg_dict_size = {\"n_obs\": (\"firm_id\", \"size\")}\n",
    "for c in target_cols:\n",
    "    agg_dict_size[f\"{c}_rate\"] = (c, \"mean\")\n",
    "\n",
    "by_size = df.groupby([\"size_decile\", \"split\"]).agg(**agg_dict_size).reset_index()\n",
    "\n",
    "display(by_size.sort_values([\"split\", \"size_decile\"]).head(30))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "52d37d65875957c3",
   "metadata": {},
   "source": [
    "### 5.6 Event indicators (evt_*) for decision support\n",
    "\n",
    "Events are discrete, interpretable signals designed for operational triage.  \n",
    "They are calibrated **using training data only** (when thresholds are estimated), and we explicitly **exclude** events mechanically tied to the distress-definition ratios (leverage/coverage) from the predictive feature set.\n",
    "\n",
    "Events implemented here (subject to data availability):\n",
    "- Dividend cut / suspension / initiation\n",
    "- Liquidity squeeze (current ratio < 1.0) and quick-ratio squeeze (< 0.8)\n",
    "- EBITDA drop (vs. t-1) and CFO drop (vs. t-1)"
   ]
  },
  {
   "cell_type": "code",
   "id": "8dd8b2b2804342d7",
   "metadata": {},
   "source": [
    "# Ensure sorting already enforced\n",
    "assert df.index.is_monotonic_increasing\n",
    "\n",
    "# Lag helpers\n",
    "def lag(df_in: pd.DataFrame, col: str, n: int = 1) -> pd.Series:\n",
    "    \"\"\"Robust firm-level lag that enforces year adjacency (Issue 1).\"\"\"\n",
    "    val = df_in.groupby(\"firm_id\")[col].shift(n)\n",
    "    year_lag = df_in.groupby(\"firm_id\")[\"fyear\"].shift(n)\n",
    "    is_adjacent = (year_lag == (df_in[\"fyear\"] - n))\n",
    "    return val.where(is_adjacent.fillna(False), np.nan)\n",
    "\n",
    "# Identify dividend column (prefer dvc if present; else dv / dvt / dvp)\n",
    "dividend_candidates = [\"dvc\",\"dv\",\"dvt\",\"dvp\"]\n",
    "div_col = next((c for c in dividend_candidates if c in df.columns), None)\n",
    "\n",
    "if div_col is None:\n",
    "    print(\"Dividend column not found (looked for dvc/dv/dvt/dvp). Dividend events will be NaN.\")\n",
    "    df[\"evt_divcut\"] = np.nan\n",
    "    df[\"evt_divsusp\"] = np.nan\n",
    "    df[\"evt_divinit\"] = np.nan\n",
    "else:\n",
    "    # Use absolute value (guard against sign conventions)\n",
    "    df[\"dv_obs\"] = pd.to_numeric(df[div_col], errors=\"coerce\").abs()\n",
    "    df[\"dv_obs_l1\"] = lag(df, \"dv_obs\", 1)\n",
    "\n",
    "# Liquidity ratios\n",
    "if \"act\" in df.columns and \"lct\" in df.columns:\n",
    "    df[\"current_ratio\"] = safe_divide(df[\"act\"], df[\"lct\"], denom_floor=1e-6)\n",
    "else:\n",
    "    df[\"current_ratio\"] = np.nan\n",
    "\n",
    "if \"act\" in df.columns and \"lct\" in df.columns:\n",
    "    if \"invt\" in df.columns:\n",
    "        df[\"quick_ratio\"] = safe_divide(pd.to_numeric(df[\"act\"], errors=\"coerce\") - pd.to_numeric(df[\"invt\"], errors=\"coerce\"),\n",
    "                                        df[\"lct\"], denom_floor=1e-6)\n",
    "    elif \"che\" in df.columns and \"rect\" in df.columns:\n",
    "        df[\"quick_ratio\"] = safe_divide(pd.to_numeric(df[\"che\"], errors=\"coerce\") + pd.to_numeric(df[\"rect\"], errors=\"coerce\"),\n",
    "                                        df[\"lct\"], denom_floor=1e-6)\n",
    "    else:\n",
    "        df[\"quick_ratio\"] = df[\"current_ratio\"]\n",
    "else:\n",
    "    df[\"quick_ratio\"] = np.nan\n",
    "\n",
    "# EBITDA and CFO lags for deterioration events\n",
    "if \"ebitda_proxy\" in df.columns:\n",
    "    df[\"ebitda_l1\"] = lag(df, \"ebitda_proxy\", 1)\n",
    "if \"oancf\" in df.columns:\n",
    "    df[\"cfo_l1\"] = lag(df, \"oancf\", 1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8975416a6a7e910f",
   "metadata": {},
   "source": [
    "#### 5.5.1 Dividend policy events (training-calibrated cut threshold)"
   ]
  },
  {
   "cell_type": "code",
   "id": "b0cb5410815abe6",
   "metadata": {},
   "source": [
    "event_params = {}\n",
    "\n",
    "if div_col is None:\n",
    "    pass\n",
    "else:\n",
    "    # YoY % change among observed payers with meaningful baseline\n",
    "    dv_l1 = pd.to_numeric(df[\"dv_obs_l1\"], errors=\"coerce\")\n",
    "    dv = pd.to_numeric(df[\"dv_obs\"], errors=\"coerce\")\n",
    "    df[\"div_pct_change\"] = np.where(dv_l1 > 1e-2, (dv - dv_l1) / dv_l1, np.nan)\n",
    "\n",
    "    payer_train = train_mask & (dv_l1 > 0) & pd.notna(df[\"div_pct_change\"])\n",
    "    if payer_train.sum() >= 50:\n",
    "        cut_thr = float(np.nanpercentile(df.loc[payer_train, \"div_pct_change\"], 10))\n",
    "    else:\n",
    "        cut_thr = -0.25\n",
    "\n",
    "    # Bound cut threshold to avoid pathological values\n",
    "    cut_thr = float(np.clip(cut_thr, -0.50, -0.10))\n",
    "    event_params[\"DIV_CUT_THR_P10_BOUNDED\"] = cut_thr\n",
    "\n",
    "    # Dividend cut: large negative YoY change among payers\n",
    "    df[\"evt_divcut\"] = (df[\"div_pct_change\"] <= cut_thr).astype(\"Int8\")\n",
    "    df.loc[df[\"div_pct_change\"].isna(), \"evt_divcut\"] = pd.NA\n",
    "\n",
    "    # Suspension: payer last year, ~zero dividend now\n",
    "    df[\"evt_divsusp\"] = ((dv_l1 > 0) & (dv.fillna(0) <= 1e-4)).astype(\"Int8\")\n",
    "    df.loc[dv_l1.isna() | dv.isna(), \"evt_divsusp\"] = pd.NA\n",
    "\n",
    "    # Initiation: ~zero last year, dividend now positive\n",
    "    df[\"evt_divinit\"] = ((dv_l1.fillna(0) <= 1e-4) & (dv > 1e-4)).astype(\"Int8\")\n",
    "    df.loc[dv_l1.isna() | dv.isna(), \"evt_divinit\"] = pd.NA\n",
    "\n",
    "    print(f\"Dividend cut threshold (train P10 bounded): {cut_thr:.3f}\")\n",
    "    display(df[[\"dv_obs\",\"dv_obs_l1\",\"div_pct_change\",\"evt_divcut\",\"evt_divsusp\",\"evt_divinit\"]].head(8))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "36d008de561e66fb",
   "metadata": {},
   "source": [
    "#### 5.5.2 Liquidity squeeze events"
   ]
  },
  {
   "cell_type": "code",
   "id": "b017ec8b6756a6fd",
   "metadata": {},
   "source": [
    "cr = pd.to_numeric(df[\"current_ratio\"], errors=\"coerce\")\n",
    "df[\"evt_liq_squeeze\"] = (cr < 1.0).astype(\"Int8\")\n",
    "df.loc[cr.isna(), \"evt_liq_squeeze\"] = pd.NA\n",
    "\n",
    "qr = pd.to_numeric(df[\"quick_ratio\"], errors=\"coerce\")\n",
    "df[\"evt_quick_squeeze\"] = (qr < 0.8).astype(\"Int8\")\n",
    "df.loc[qr.isna(), \"evt_quick_squeeze\"] = pd.NA\n",
    "\n",
    "display(df[[\"current_ratio\",\"quick_ratio\",\"evt_liq_squeeze\",\"evt_quick_squeeze\"]].head(8))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b000cc9cfa538672",
   "metadata": {},
   "source": [
    "#### 5.5.3 Operating deterioration events (vs. t-1)"
   ]
  },
  {
   "cell_type": "code",
   "id": "68b990d0f8712e6f",
   "metadata": {},
   "source": [
    "# EBITDA drop: requires lagged EBITDA observed and positive\n",
    "if \"ebitda_proxy\" in df.columns:\n",
    "    e = pd.to_numeric(df[\"ebitda_proxy\"], errors=\"coerce\")\n",
    "    e_l1 = pd.to_numeric(df[\"ebitda_l1\"], errors=\"coerce\")\n",
    "    ratio = e / e_l1\n",
    "    evt = ((e_l1 > 0) & ((ratio < 0.5) | (e <= 0))).astype(\"Int8\")\n",
    "    evt = evt.where(pd.notna(e_l1) & pd.notna(e), other=pd.NA).astype(\"Int8\")\n",
    "    df[\"evt_ebitdadrop\"] = evt\n",
    "else:\n",
    "    df[\"evt_ebitdadrop\"] = pd.NA\n",
    "\n",
    "# CFO drop: requires lagged CFO observed and positive\n",
    "if \"oancf\" in df.columns:\n",
    "    c = pd.to_numeric(df[\"oancf\"], errors=\"coerce\")\n",
    "    c_l1 = pd.to_numeric(df[\"cfo_l1\"], errors=\"coerce\")\n",
    "    ratio = c / c_l1\n",
    "    evt = ((c_l1 > 0) & ((ratio < 0.5) | (c <= 0))).astype(\"Int8\")\n",
    "    evt = evt.where(pd.notna(c_l1) & pd.notna(c), other=pd.NA).astype(\"Int8\")\n",
    "    df[\"evt_cfdrop\"] = evt\n",
    "else:\n",
    "    df[\"evt_cfdrop\"] = pd.NA\n",
    "\n",
    "display(df[[\"ebitda_proxy\",\"ebitda_l1\",\"evt_ebitdadrop\",\"oancf\",\"cfo_l1\",\"evt_cfdrop\"]].head(10))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "21df27fd3dc455da",
   "metadata": {},
   "source": [
    "#### 5.5.4 Event dictionary (appendix-ready)"
   ]
  },
  {
   "cell_type": "code",
   "id": "3a53f31e9c416c3c",
   "metadata": {},
   "source": [
    "event_dict_rows = [\n",
    "    {\"event\":\"evt_divcut\", \"definition\":\"Dividend YoY % change <= training P10 threshold (bounded [-0.50,-0.10])\", \"inputs\":div_col or \"N/A\", \"calibration\":\"train-only\"},\n",
    "    {\"event\":\"evt_divsusp\", \"definition\":\"Dividend >0 at t-1 and ~0 at t\", \"inputs\":div_col or \"N/A\", \"calibration\":\"none\"},\n",
    "    {\"event\":\"evt_divinit\", \"definition\":\"Dividend ~0 at t-1 and >0 at t\", \"inputs\":div_col or \"N/A\", \"calibration\":\"none\"},\n",
    "    {\"event\":\"evt_liq_squeeze\", \"definition\":\"Current ratio < 1.0\", \"inputs\":\"act,lct\", \"calibration\":\"fixed threshold\"},\n",
    "    {\"event\":\"evt_quick_squeeze\", \"definition\":\"Quick ratio < 0.8\", \"inputs\":\"act,lct,invt (or che+rect)\", \"calibration\":\"fixed threshold\"},\n",
    "    {\"event\":\"evt_ebitdadrop\", \"definition\":\"EBITDA <=0 OR EBITDA/EBITDA_{t-1}<0.5 (requires EBITDA_{t-1}>0)\", \"inputs\":\"oibdp\", \"calibration\":\"fixed threshold\"},\n",
    "    {\"event\":\"evt_cfdrop\", \"definition\":\"CFO <=0 OR CFO/CFO_{t-1}<0.5 (requires CFO_{t-1}>0)\", \"inputs\":\"oancf\", \"calibration\":\"fixed threshold\"},\n",
    "]\n",
    "event_dict = pd.DataFrame(event_dict_rows)\n",
    "event_dict[\"parameter\"] = event_dict[\"event\"].map(lambda e: json.dumps({k:v for k,v in event_params.items()}) if e==\"evt_divcut\" else \"\")\n",
    "display(event_dict)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cf7ed714325bb4d",
   "metadata": {},
   "source": [
    "## 6. Preprocessing for Modeling (train-only fitting)\n",
    "\n",
    "I keep preprocessing leakage-safe:\n",
    "- fit medians, winsor bounds, and scalers on **train** only,\n",
    "- apply those transforms to all splits,\n",
    "- preserve an unprocessed snapshot for walk-forward validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b89af8984c6dbf95",
   "metadata": {},
   "source": [
    "# Features that participate in the distress proxy definition (must be excluded from predictors)\n",
    "# We key off PROXY_VERSION (not TARGET_NAME) so the same leakage rules apply to both objectives:\n",
    "#   - OBJECTIVE=\"state\"       -> TARGET_NAME = target_next_vX\n",
    "#   - OBJECTIVE=\"transition\"  -> TARGET_NAME = target_transition_vX\n",
    "\n",
    "try:\n",
    "    _pv = PROXY_VERSION\n",
    "except NameError:\n",
    "    # Backward compatibility if PROXY_VERSION is not defined in the targets cell\n",
    "    if TARGET_NAME.endswith(\"_v1\"):\n",
    "        _pv = \"v1\"\n",
    "    elif TARGET_NAME.endswith(\"_v2\"):\n",
    "        _pv = \"v2\"\n",
    "    elif TARGET_NAME.endswith(\"_v3\"):\n",
    "        _pv = \"v3\"\n",
    "    else:\n",
    "        _pv = \"v2\"\n",
    "\n",
    "if _pv == \"v1\":\n",
    "    # v1 uses niadj and oancf\n",
    "    DISTRESS_DEFINITION_VARS = {\"niadj\", \"oancf\", \"niadj_at\", \"loss_indicator\", \"ocf_to_debt\", \"fcf_to_debt\"}\n",
    "    continuous_feats_raw = [c for c in FEATURES_V1]\n",
    "    event_feats = []\n",
    "elif _pv == \"v2\":\n",
    "    # v2 uses seq\n",
    "    DISTRESS_DEFINITION_VARS = {\"seq\"}\n",
    "    continuous_feats_raw = [c for c in FEATURES_V2]\n",
    "    event_feats = []\n",
    "elif _pv == \"v3\":\n",
    "    # v3 uses ffo (oancf, xint, txp), debt (dlc, dltt), and equity (seq, mibt)\n",
    "    DISTRESS_DEFINITION_VARS = {\n",
    "        \"sp_ffo_to_debt\", \"sp_debt_to_capital\", \"sp_debt_to_ebitda\",\n",
    "        \"oancf\", \"xint\", \"txp\", \"dlc\", \"dltt\", \"seq\", \"mibt\", \"oibdp\",\n",
    "        \"ocf_to_debt\", \"fcf_to_debt\", \"debt_to_ebitda\", \"interest_coverage\"\n",
    "    }\n",
    "    # loss_indicator is binary, treat as event feature to avoid z-scoring\n",
    "    continuous_feats_raw = [c for c in FEATURES_V3 if c != \"loss_indicator\"]\n",
    "    event_feats = [\"loss_indicator\"]\n",
    "else:\n",
    "    DISTRESS_DEFINITION_VARS = set()\n",
    "    continuous_feats_raw = [c for c in FEATURES_V2]\n",
    "    event_feats = []\n",
    "continuous_feats_raw = [c for c in continuous_feats_raw if c in df.columns]\n",
    "event_feats = [c for c in event_feats if c in df.columns]\n",
    "\n",
    "# Final model feature list (events in levels; continuous will be z-scored with z_ prefix)\n",
    "MODEL_FEATS = [f\"z_{c}\" for c in continuous_feats_raw] + event_feats\n",
    "\n",
    "# Leakage audit: ensure no distress-definition variables are included (raw or scaled variants)\n",
    "leakage_hits = []\n",
    "for v in DISTRESS_DEFINITION_VARS:\n",
    "    if v in continuous_feats_raw or v in event_feats or f\"z_{v}\" in MODEL_FEATS:\n",
    "        leakage_hits.append(v)\n",
    "\n",
    "if leakage_hits:\n",
    "    raise ValueError(f\"Leakage audit failed: distress-definition variables present in feature set: {leakage_hits}\")\n",
    "\n",
    "print(\"Continuous (to be scaled):\", continuous_feats_raw)\n",
    "print(\"Events (kept in levels):\", event_feats)\n",
    "print(\"MODEL_FEATS (post-scaling names):\", MODEL_FEATS)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5fbdc4c04d58f7fa",
   "metadata": {},
   "source": [
    "### 6.2 Modeling sample and target availability"
   ]
  },
  {
   "cell_type": "code",
   "id": "4f85fb4599bda421",
   "metadata": {},
   "source": [
    "# Modeling requires a defined next-year label\n",
    "model_mask = df[TARGET_NAME].notna()\n",
    "df_model = df.loc[model_mask].copy()\n",
    "\n",
    "print(\"Modeling sample size:\", df_model.shape[0])\n",
    "display(df_model[\"split\"].value_counts().to_frame(\"n_obs\"))\n",
    "# Snapshot for leakage-free walk-forward validation\n",
    "df_model_raw = df_model.copy(deep=True)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5e7ac5decb66c6de",
   "metadata": {},
   "source": [
    "### 6.3 Replace infinities and set up train-only median imputation for remaining NaNs"
   ]
  },
  {
   "cell_type": "code",
   "id": "f63788ceca39c972",
   "metadata": {},
   "source": [
    "# Replace inf with NaN for preprocessing\n",
    "for c in continuous_feats_raw:\n",
    "    df_model[c] = pd.to_numeric(df_model[c], errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Train-only medians for remaining NaNs (after earlier imputation steps)\n",
    "train_medians = df_model.loc[df_model[\"split\"]==\"train\", continuous_feats_raw].median()\n",
    "\n",
    "for c in continuous_feats_raw:\n",
    "    df_model[c] = df_model[c].fillna(train_medians[c])\n",
    "\n",
    "# Event features: coerce to Int8 with missing -> 0 (conservative) but preserve missingness flags separately if desired\n",
    "for c in event_feats:\n",
    "    df_model[c] = pd.to_numeric(df_model[c], errors=\"coerce\").fillna(0).astype(\"Int8\")\n",
    "\n",
    "assert df_model[continuous_feats_raw].isna().sum().sum() == 0, \"NaNs remain in continuous features after train-median fill.\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9d9d917be0aea31a",
   "metadata": {},
   "source": [
    "### 6.4 Winsorize continuous features (train quantile bounds)"
   ]
  },
  {
   "cell_type": "code",
   "id": "7ce56607c8789160",
   "metadata": {},
   "source": [
    "winsor_bounds = {}\n",
    "for c in continuous_feats_raw:\n",
    "    lo, hi = winsorize_train_bounds(df_model.loc[df_model[\"split\"]==\"train\", c], CONFIG[\"WINSOR_LO_Q\"], CONFIG[\"WINSOR_HI_Q\"])\n",
    "    winsor_bounds[c] = (lo, hi)\n",
    "    df_model[c] = apply_bounds(df_model[c], lo, hi)\n",
    "\n",
    "winsor_tbl = pd.DataFrame(\n",
    "    [{\"feature\": c, \"lo\": winsor_bounds[c][0], \"hi\": winsor_bounds[c][1]} for c in continuous_feats_raw]\n",
    ")\n",
    "display(winsor_tbl)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4b Winsorization sensitivity analysis (1/99 vs 5/95 vs 10/90)\nTo ensure conclusions are not driven by aggressive tail clipping, I re-run a lightweight training/evaluation loop under three winsor cutoffs and report test PR-AUC for each.\n"
   ],
   "id": "421bc4de3e4a19bb"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "def _safe_pr_auc(y, p):\n",
    "    y = np.asarray(y).astype(int)\n",
    "    p = np.asarray(p).astype(float)\n",
    "    if np.unique(y).size < 2:\n",
    "        return np.nan\n",
    "    return float(average_precision_score(y, p))\n",
    "\n",
    "def _prep_winsor_dataset(df_in: pd.DataFrame, lo_q: float, hi_q: float):\n",
    "    df_work = df_in.copy()\n",
    "    cont = [c for c in continuous_feats_raw if c in df_work.columns]\n",
    "    train_mask = df_work[\"split\"] == \"train\"\n",
    "\n",
    "    Xtr = df_work.loc[train_mask, cont].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    med = Xtr.median()\n",
    "    Xall = df_work[cont].apply(pd.to_numeric, errors=\"coerce\").fillna(med)\n",
    "\n",
    "    bounds = {c: winsorize_train_bounds(Xtr[c], lo_q, hi_q) for c in cont}\n",
    "    for c, (lo, hi) in bounds.items():\n",
    "        Xall[c] = apply_bounds(Xall[c], lo, hi)\n",
    "\n",
    "    df_work[cont] = Xall\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df_work.loc[train_mask, cont])\n",
    "    Z = pd.DataFrame(\n",
    "        scaler.transform(df_work[cont]),\n",
    "        columns=[f\"z_{c}\" for c in cont],\n",
    "        index=df_work.index,\n",
    "    )\n",
    "\n",
    "    ev = [c for c in event_feats if c in df_work.columns]\n",
    "    if ev:\n",
    "        Z = pd.concat([Z, df_work[ev].fillna(0.0)], axis=1)\n",
    "\n",
    "    return df_work, Z\n",
    "\n",
    "winsor_grid = [(0.01, 0.99), (0.05, 0.95), (0.10, 0.90)]\n",
    "rows = []\n",
    "\n",
    "for lo_q, hi_q in winsor_grid:\n",
    "    df_w, Z = _prep_winsor_dataset(df_model_raw, lo_q, hi_q)\n",
    "    y_w = df_w[TARGET_NAME].astype(int)\n",
    "\n",
    "    train_mask = df_w[\"split\"] == \"train\"\n",
    "    val_mask = df_w[\"split\"] == \"val\"\n",
    "    test_mask = df_w[\"split\"] == \"test\"\n",
    "\n",
    "    # --- Logit (train on train only; evaluate on test) ---\n",
    "    C_use = float(globals().get(\"best_C\", 1.0))\n",
    "    log_mdl = LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        C=C_use,\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=3000,\n",
    "        random_state=SEED,\n",
    "    )\n",
    "    log_mdl.fit(Z.loc[train_mask], y_w.loc[train_mask])\n",
    "    p_te_log = log_mdl.predict_proba(Z.loc[test_mask])[:, 1]\n",
    "\n",
    "    rows.append({\n",
    "        \"winsor_lo\": lo_q,\n",
    "        \"winsor_hi\": hi_q,\n",
    "        \"model\": \"logit\",\n",
    "        \"test_pr_auc\": _safe_pr_auc(y_w.loc[test_mask].values, p_te_log),\n",
    "    })\n",
    "\n",
    "    # --- XGBoost (train on train, early-stop on val, evaluate on test) ---\n",
    "    if y_w.loc[train_mask].nunique() > 1 and y_w.loc[val_mask].nunique() > 1:\n",
    "        n_pos = int(y_w.loc[train_mask].sum())\n",
    "        n_neg = int((y_w.loc[train_mask] == 0).sum())\n",
    "        imbalance = n_neg / max(n_pos, 1)\n",
    "\n",
    "        w_pos = CONFIG[\"COST_FN\"]\n",
    "        w_neg = CONFIG[\"COST_FP\"]\n",
    "\n",
    "        w_tr = np.where(y_w.loc[train_mask].values == 1, w_pos, w_neg).astype(float)\n",
    "        w_val = np.where(y_w.loc[val_mask].values == 1, w_pos, w_neg).astype(float)\n",
    "\n",
    "        dtrain = xgb.DMatrix(Z.loc[train_mask], label=y_w.loc[train_mask], weight=w_tr)\n",
    "        dval = xgb.DMatrix(Z.loc[val_mask], label=y_w.loc[val_mask], weight=w_val)\n",
    "        dtest = xgb.DMatrix(Z.loc[test_mask], label=y_w.loc[test_mask])\n",
    "\n",
    "        xgb_params = CONFIG.get(\"XGB_BEST_PARAMS\", CONFIG[\"XGB_PARAMS\"]).copy()\n",
    "        xgb_params[\"scale_pos_weight\"] = float(imbalance)\n",
    "\n",
    "        xgb_model = xgb.train(\n",
    "            params=xgb_params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=CONFIG[\"XGB_NUM_BOOST_ROUND\"],\n",
    "            evals=[(dtrain, \"train\"), (dval, \"val\")],\n",
    "            early_stopping_rounds=CONFIG[\"XGB_EARLY_STOPPING\"],\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "\n",
    "        best_iter = int(getattr(xgb_model, \"best_iteration\", 0)) + 1\n",
    "        p_te_xgb = xgb_model.predict(dtest, iteration_range=(0, best_iter))\n",
    "        rows.append({\n",
    "            \"winsor_lo\": lo_q,\n",
    "            \"winsor_hi\": hi_q,\n",
    "            \"model\": \"tree_raw\",\n",
    "            \"test_pr_auc\": _safe_pr_auc(y_w.loc[test_mask].values, p_te_xgb),\n",
    "        })\n",
    "    else:\n",
    "        rows.append({\n",
    "            \"winsor_lo\": lo_q,\n",
    "            \"winsor_hi\": hi_q,\n",
    "            \"model\": \"tree_raw\",\n",
    "            \"test_pr_auc\": np.nan,\n",
    "            \"note\": \"Skipped: single-class train/val\"\n",
    "        })\n",
    "\n",
    "winsor_sensitivity_tbl = pd.DataFrame(rows)\n",
    "display(winsor_sensitivity_tbl.sort_values([\"model\", \"winsor_lo\"]))\n"
   ],
   "id": "75512be185269a9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ddad00b48676c27b",
   "metadata": {},
   "source": [
    "### 6.5 Standardize continuous features (train-fit scaler; z_ prefix)"
   ]
  },
  {
   "cell_type": "code",
   "id": "37b2f03bd6afcb83",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize continuous features (fit on TRAIN only)\n",
    "scaler = StandardScaler()\n",
    "df_model[continuous_feats_raw] = df_model[continuous_feats_raw].apply(lambda s: pd.to_numeric(s, errors=\"coerce\"))\n",
    "\n",
    "train_cont = df_model.loc[df_model[\"split\"] == \"train\", continuous_feats_raw].astype(float)\n",
    "scaler.fit(train_cont)\n",
    "\n",
    "Z_all = scaler.transform(df_model[continuous_feats_raw].astype(float))\n",
    "for j, c in enumerate(continuous_feats_raw):\n",
    "    df_model[f\"z_{c}\"] = Z_all[:, j].astype(float)\n",
    "\n",
    "# Final modeling matrix (events forced to clean 0/1 ints)\n",
    "z_cols = [f\"z_{c}\" for c in continuous_feats_raw]\n",
    "X = df_model[z_cols + event_feats].copy()\n",
    "\n",
    "# Guardrail check: verify excluded variables are not present in final feature matrix\n",
    "EXCLUDED_VARS = [\n",
    "    \"aco_act\", \"aqc_at\", \"caps_at\", \"capx_at\", \"dp_at\",\n",
    "    \"invch_act\", \"lco_lct\", \"mibt_at\", \"recch_act\",\n",
    "    \"txditc_at\", \"txp_lct\", \"xint_lct\"\n",
    "]\n",
    "EXCLUDED_Z_VARS = [f\"z_{v}\" for v in EXCLUDED_VARS]\n",
    "offending_cols = [c for c in X.columns if c in EXCLUDED_VARS or c in EXCLUDED_Z_VARS]\n",
    "if offending_cols:\n",
    "    raise ValueError(f\"Guardrail check failed: Excluded variables found in final feature matrix X: {offending_cols}\")\n",
    "\n",
    "for c in event_feats:\n",
    "    X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "    X[c] = X[c].fillna(0).astype(\"int8\")\n",
    "    assert set(X[c].unique()).issubset({0, 1}), f\"{c} not binary after coercion: {sorted(X[c].unique())}\"\n",
    "\n",
    "y = df_model[TARGET_NAME].astype(int)\n",
    "\n",
    "# Split views\n",
    "splits = {}\n",
    "for sp in [\"train\", \"val\", \"test\"]:\n",
    "    mask = df_model[\"split\"] == sp\n",
    "    splits[sp] = {\"X\": X.loc[mask, :], \"y\": y.loc[mask], \"df\": df_model.loc[mask, :]}\n",
    "\n",
    "print({sp: (v[\"X\"].shape[0], v[\"X\"].shape[1]) for sp, v in splits.items()})\n",
    "\n",
    "# Numeric-safe finiteness check\n",
    "assert np.isfinite(X.astype(\"float64\").to_numpy()).all(), \"Non-finite values in modeling matrix.\"\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "56207f2a3c9462b5",
   "metadata": {},
   "source": [
    "## 7. Model Selection & Training\n",
    "\n",
    "### 7A. Logit model (primary baseline: calibrated PD with interpretable coefficients)\n",
    "\n",
    "#### 7A.1 Hyperparameter tuning on out-of-time validation year"
   ]
  },
  {
   "cell_type": "code",
   "id": "f9f0a679e38683f8",
   "metadata": {},
   "source": [
    "train_X, train_y = splits[\"train\"][\"X\"], splits[\"train\"][\"y\"]\n",
    "val_X, val_y = splits[\"val\"][\"X\"], splits[\"val\"][\"y\"]\n",
    "\n",
    "results = []\n",
    "for C in CONFIG[\"LOGIT_C_GRID\"]:\n",
    "    mdl = LogisticRegression(C=C, solver=\"lbfgs\", max_iter=2000, random_state=SEED)\n",
    "    mdl.fit(train_X, train_y)\n",
    "    val_proba = mdl.predict_proba(val_X)[:, 1]\n",
    "    results.append({\n",
    "        \"C\": C,\n",
    "        \"val_roc_auc\": roc_auc_score(val_y, val_proba),\n",
    "        \"val_pr_auc\": average_precision_score(val_y, val_proba),\n",
    "        \"val_brier\": brier_score_loss(val_y, val_proba),\n",
    "    })\n",
    "\n",
    "tune_tbl = pd.DataFrame(results).sort_values(\"val_roc_auc\", ascending=False)\n",
    "display(tune_tbl)\n",
    "\n",
    "best_C = float(tune_tbl.iloc[0][\"C\"])\n",
    "print(\"Selected C:\", best_C)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "34da3f26ec4574c8",
   "metadata": {},
   "source": [
    "#### 7A.2 Fit Logit models and generate PDs"
   ]
  },
  {
   "cell_type": "code",
   "id": "5265c285777d20a4",
   "metadata": {},
   "source": [
    "trainval_mask = df_model[\"split\"].isin([\"train\",\"val\"])\n",
    "X_trainval = X.loc[trainval_mask, :]\n",
    "y_trainval = y.loc[trainval_mask]\n",
    "\n",
    "# To ensure 'val' metrics are honest out-of-sample estimates, we use the model \n",
    "# trained on 'train' only for the validation split. \n",
    "# For the final 'test' performance, we use the model trained on 'train+val'.\n",
    "\n",
    "# Model trained on 'train' ONLY (for honest 'val' evaluation)\n",
    "logit_train_only = LogisticRegression(C=best_C, solver=\"lbfgs\", max_iter=3000, random_state=SEED)\n",
    "logit_train_only.fit(train_X, train_y)\n",
    "\n",
    "# Model trained on 'train+val' (for final 'test' evaluation)\n",
    "logit_trainval = LogisticRegression(C=best_C, solver=\"lbfgs\", max_iter=3000, random_state=SEED)\n",
    "logit_trainval.fit(X_trainval, y_trainval)\n",
    "\n",
    "# Assign predictions\n",
    "df_model[\"pd_logit\"] = np.nan\n",
    "# val gets predictions from train-only model (honest out-of-sample)\n",
    "df_model.loc[df_model[\"split\"]==\"val\", \"pd_logit\"] = logit_train_only.predict_proba(val_X)[:, 1]\n",
    "# test gets predictions from train+val model\n",
    "df_model.loc[df_model[\"split\"]==\"test\", \"pd_logit\"] = logit_trainval.predict_proba(splits[\"test\"][\"X\"])[:, 1]\n",
    "# train gets predictions from train+val model (in-sample)\n",
    "df_model.loc[df_model[\"split\"]==\"train\", \"pd_logit\"] = logit_trainval.predict_proba(train_X)[:, 1]\n",
    "\n",
    "# For legacy compatibility in reporting\n",
    "df_model[\"pd_logit_val\"] = np.where(df_model[\"split\"]==\"val\", df_model[\"pd_logit\"], np.nan)\n",
    "df_model[\"pd_logit_test\"] = np.where(df_model[\"split\"]==\"test\", df_model[\"pd_logit\"], np.nan)\n",
    "\n",
    "# Keep logit_clf as the final model for downstream use\n",
    "logit_clf = logit_trainval\n",
    "\n",
    "print(\"Example PDs (Logit):\")\n",
    "display(df_model[[\"firm_id\",\"fyear\",\"label_year\",\"split\",TARGET_NAME,\"pd_logit\"]].head(10))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "25b530bf52c9f3e0",
   "metadata": {},
   "source": [
    "#### 7A.3 Inference audit (statsmodels Logit; clustered standard errors)"
   ]
  },
  {
   "cell_type": "code",
   "id": "15b6aff45deb92ce",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from statsmodels.stats.sandwich_covariance import cov_cluster, cov_cluster_2groups\n",
    "# Statsmodels requires numpy arrays; keep column names for tables.\n",
    "X_sm = sm.add_constant(X_trainval, has_constant=\"add\")\n",
    "y_sm = y_trainval.astype(float)\n",
    "\n",
    "logit_sm = sm.Logit(y_sm, X_sm)\n",
    "res_sm = logit_sm.fit(disp=False, maxiter=200)\n",
    "\n",
    "# --- Firm cluster (numeric codes to avoid dtype issues) ---\n",
    "firm_raw = df_model.loc[trainval_mask, \"firm_id\"]\n",
    "firm_codes = pd.factorize(firm_raw, sort=True)[0].astype(np.int64)\n",
    "\n",
    "cov_firm = cov_cluster(res_sm, firm_codes)\n",
    "se_firm = np.sqrt(np.diag(cov_firm))\n",
    "\n",
    "# --- Two-way cluster (firm + year), with feasibility + shape guards ---\n",
    "year_raw = df_model.loc[trainval_mask, \"label_year\"]\n",
    "firm_raw = df_model.loc[trainval_mask, \"firm_id\"]\n",
    "\n",
    "firm_codes = pd.factorize(firm_raw, sort=True)[0].astype(np.int64)\n",
    "year_codes = pd.factorize(year_raw, sort=True)[0].astype(np.int64)\n",
    "\n",
    "if (np.unique(firm_codes).size < 2) or (np.unique(year_codes).size < 2):\n",
    "    # Not enough clusters in one dimension -> two-way clustering not identified\n",
    "    se_2 = se_firm.copy()\n",
    "else:\n",
    "    cov_2 = cov_cluster_2groups(res_sm, firm_codes, year_codes)\n",
    "    cov_2 = np.asarray(cov_2)\n",
    "\n",
    "    k = len(res_sm.params)\n",
    "    if cov_2.ndim == 2 and cov_2.shape == (k, k):\n",
    "        se_2 = np.sqrt(np.diag(cov_2))\n",
    "    elif cov_2.ndim == 1 and cov_2.size == k:\n",
    "        # Some statsmodels versions may return only the diagonal variances\n",
    "        se_2 = np.sqrt(cov_2)\n",
    "    else:\n",
    "        # Unexpected shape -> fall back (safer than crashing)\n",
    "        se_2 = se_firm.copy()\n",
    "\n",
    "coef = res_sm.params\n",
    "p_firm = 2 * (1 - stats.norm.cdf(np.abs(coef / se_firm)))\n",
    "p_2 = 2 * (1 - stats.norm.cdf(np.abs(coef / se_2)))\n",
    "\n",
    "infer_tbl = pd.DataFrame({\n",
    "    \"coef_logodds\": coef,\n",
    "    \"se_firm\": se_firm,\n",
    "    \"p_firm\": p_firm,\n",
    "    \"se_firm_year\": se_2,\n",
    "    \"p_firm_year\": p_2,\n",
    "    \"odds_ratio\": np.exp(coef),\n",
    "})\n",
    "infer_tbl.index.name = \"feature\"\n",
    "display(infer_tbl)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5a5c8a5d46699d34",
   "metadata": {},
   "source": [
    "#### 7A.4 Economic magnitude: MEM marginal effects and IQR-scaled effects"
   ]
  },
  {
   "cell_type": "code",
   "id": "e57f12ed1f1dc46b",
   "metadata": {},
   "source": [
    "# MEM marginal effects using statsmodels (on train+val)\n",
    "try:\n",
    "    me = res_sm.get_margeff(at=\"mean\")\n",
    "    me_tbl = me.summary_frame()\n",
    "    # Align to inference table index (margeff typically excludes const)\n",
    "    me_tbl = me_tbl.reindex(infer_tbl.index)\n",
    "    display(me_tbl)\n",
    "except Exception as e:\n",
    "    print(\"Marginal effects computation failed:\", e)\n",
    "    me_tbl = None\n",
    "\n",
    "# IQR-scaled effects for continuous features (using TRAIN distribution, mapped into z-space)\n",
    "train_df = df_model.loc[df_model[\"split\"]==\"train\", :].copy()\n",
    "\n",
    "iqr_rows = []\n",
    "for j, raw_c in enumerate(continuous_feats_raw):\n",
    "    q25 = float(train_df[raw_c].quantile(0.25))\n",
    "    q75 = float(train_df[raw_c].quantile(0.75))\n",
    "    iqr = q75 - q25\n",
    "    sd = float(scaler.scale_[j]) if scaler.scale_[j] > 0 else np.nan\n",
    "    delta_z = iqr / sd if sd and not np.isnan(sd) else np.nan\n",
    "    beta = float(infer_tbl.loc[f\"z_{raw_c}\", \"coef_logodds\"]) if f\"z_{raw_c}\" in infer_tbl.index else np.nan\n",
    "    logodds_delta = beta * delta_z if not np.isnan(beta) and not np.isnan(delta_z) else np.nan\n",
    "    iqr_rows.append({\n",
    "        \"raw_feature\": raw_c,\n",
    "        \"IQR_raw\": iqr,\n",
    "        \"delta_z_equiv\": delta_z,\n",
    "        \"logodds_change_IQR\": logodds_delta,\n",
    "        \"odds_ratio_IQR\": float(np.exp(logodds_delta)) if not np.isnan(logodds_delta) else np.nan,\n",
    "    })\n",
    "\n",
    "iqr_tbl = pd.DataFrame(iqr_rows)\n",
    "display(iqr_tbl)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b67d27ce8963fda3",
   "metadata": {},
   "source": [
    "#### 7A.5 Average Partial Effects (APEs) in probability units with cluster-robust uncertainty\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a562e1432b2abe6c",
   "metadata": {},
   "source": [
    "# APEs (Average Partial Effects) for logit model, using delta-method SEs with cluster-robust covariances\n",
    "# Notes:\n",
    "# - For logit, dP/dx_j = p*(1-p)*beta_j. The APE is the sample average of this derivative.\n",
    "# - We compute APEs on the TRAIN+VAL estimation sample used in statsmodels (X_sm, res_sm).\n",
    "# - SEs use the same cluster-robust covariance matrices already computed above (cov_firm and cov_2).\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from statsmodels.stats.sandwich_covariance import cov_cluster, cov_cluster_2groups\n",
    "\n",
    "# ---- Ensure prerequisites are available ----\n",
    "if \"trainval_mask\" not in globals():\n",
    "    if \"df_model\" in globals() and \"split\" in df_model.columns:\n",
    "        trainval_mask = df_model[\"split\"].isin([\"train\", \"val\"])\n",
    "    else:\n",
    "        raise ValueError(\"trainval_mask not found; run the split/model setup cells first.\")\n",
    "\n",
    "if \"X_trainval\" not in globals():\n",
    "    if \"X\" in globals():\n",
    "        X_trainval = X.loc[trainval_mask, :]\n",
    "    else:\n",
    "        raise ValueError(\"X_trainval not found; run the feature matrix setup cell first.\")\n",
    "\n",
    "if \"y_trainval\" not in globals():\n",
    "    if \"y\" in globals():\n",
    "        y_trainval = y.loc[trainval_mask]\n",
    "    else:\n",
    "        raise ValueError(\"y_trainval not found; run the target setup cell first.\")\n",
    "\n",
    "if \"X_sm\" not in globals() or \"res_sm\" not in globals():\n",
    "    X_sm = sm.add_constant(X_trainval, has_constant=\"add\")\n",
    "    y_sm = y_trainval.astype(float)\n",
    "    logit_sm = sm.Logit(y_sm, X_sm)\n",
    "    res_sm = logit_sm.fit(disp=False, maxiter=200)\n",
    "\n",
    "if \"cov_firm\" not in globals() or \"infer_tbl\" not in globals():\n",
    "    if \"df_model\" not in globals():\n",
    "        raise ValueError(\"df_model not found; run the modeling data prep cell first.\")\n",
    "\n",
    "    firm_raw = df_model.loc[trainval_mask, \"firm_id\"]\n",
    "    firm_codes = pd.factorize(firm_raw, sort=True)[0].astype(np.int64)\n",
    "    cov_firm = cov_cluster(res_sm, firm_codes)\n",
    "    se_firm = np.sqrt(np.diag(cov_firm))\n",
    "\n",
    "    year_raw = df_model.loc[trainval_mask, \"label_year\"]\n",
    "    year_codes = pd.factorize(year_raw, sort=True)[0].astype(np.int64)\n",
    "\n",
    "    if (np.unique(firm_codes).size < 2) or (np.unique(year_codes).size < 2):\n",
    "        se_2 = se_firm.copy()\n",
    "    else:\n",
    "        cov_2 = cov_cluster_2groups(res_sm, firm_codes, year_codes)\n",
    "        cov_2 = np.asarray(cov_2)\n",
    "        k = len(res_sm.params)\n",
    "        if cov_2.ndim == 2 and cov_2.shape == (k, k):\n",
    "            se_2 = np.sqrt(np.diag(cov_2))\n",
    "        elif cov_2.ndim == 1 and cov_2.size == k:\n",
    "            se_2 = np.sqrt(cov_2)\n",
    "        else:\n",
    "            se_2 = se_firm.copy()\n",
    "\n",
    "    coef = res_sm.params\n",
    "    p_firm = 2 * (1 - stats.norm.cdf(np.abs(coef / se_firm)))\n",
    "    p_2 = 2 * (1 - stats.norm.cdf(np.abs(coef / se_2)))\n",
    "\n",
    "    infer_tbl = pd.DataFrame({\n",
    "        \"coef_logodds\": coef,\n",
    "        \"se_firm\": se_firm,\n",
    "        \"p_firm\": p_firm,\n",
    "        \"se_firm_year\": se_2,\n",
    "        \"p_firm_year\": p_2,\n",
    "        \"odds_ratio\": np.exp(coef),\n",
    "    })\n",
    "    infer_tbl.index.name = \"feature\"\n",
    "\n",
    "\n",
    "def _coerce_cov(V, names):\n",
    "    # Return numeric (k x k) covariance aligned to names. Fallback logic handles common statsmodels outputs.\n",
    "    k = len(names)\n",
    "\n",
    "    if isinstance(V, pd.DataFrame):\n",
    "        V = V.reindex(index=names, columns=names).to_numpy(dtype=float)\n",
    "        return V\n",
    "\n",
    "    V = np.asarray(V)\n",
    "    V = np.squeeze(V)\n",
    "\n",
    "    # Handle 3D objects (e.g., (3,k,k) or (k,k,3)): take first covariance slice\n",
    "    if V.ndim == 3:\n",
    "        if V.shape[1:] == (k, k):\n",
    "            V = V[0]\n",
    "        elif V.shape[:2] == (k, k):\n",
    "            V = V[:, :, 0]\n",
    "        else:\n",
    "            V = V.reshape(-1, k, k)[0]\n",
    "\n",
    "    # Handle diagonal-only variances\n",
    "    if V.ndim == 1:\n",
    "        if V.size != k:\n",
    "            raise ValueError(f\"Unexpected 1D covariance length: {V.size} (expected {k})\")\n",
    "        V = np.diag(V.astype(float))\n",
    "\n",
    "    if V.ndim != 2 or V.shape != (k, k):\n",
    "        raise ValueError(f\"Unexpected covariance shape: {V.shape} (expected {(k, k)})\")\n",
    "\n",
    "    V = V.astype(float)\n",
    "    V[~np.isfinite(V)] = 0.0\n",
    "    return V\n",
    "\n",
    "# ---- Align design matrix to params order ----\n",
    "X_df = X_sm if isinstance(X_sm, pd.DataFrame) else pd.DataFrame(X_sm)\n",
    "b_ser = res_sm.params\n",
    "\n",
    "names = list(b_ser.index)\n",
    "X_df = X_df.loc[:, names]                 # enforce same column order\n",
    "X_audit_np = X_df.to_numpy(dtype=float)\n",
    "\n",
    "b = b_ser.to_numpy(dtype=float)\n",
    "k = len(names)\n",
    "\n",
    "# Predicted probabilities on estimation sample\n",
    "eta = X_audit_np @ b\n",
    "p = 1.0 / (1.0 + np.exp(-eta))\n",
    "w = p * (1.0 - p)\n",
    "mw = float(np.mean(w))\n",
    "\n",
    "# APE_j = beta_j * mean(w)\n",
    "ape = b * mw\n",
    "if \"const\" in names:\n",
    "    ape[names.index(\"const\")] = np.nan\n",
    "\n",
    "# Delta-method gradient\n",
    "t = (w * (1.0 - 2.0 * p))[:, None] * X_audit_np\n",
    "dmw_db = np.mean(t, axis=0)\n",
    "\n",
    "G = np.full((k, k), np.nan, dtype=float)\n",
    "for j in range(k):\n",
    "    if names[j] == \"const\":\n",
    "        continue\n",
    "    g = dmw_db * b[j]\n",
    "    g[j] += mw\n",
    "    G[j, :] = g\n",
    "\n",
    "# Covariances (coerce 2-way; fallback to firm)\n",
    "V_firm = _coerce_cov(cov_firm, names)\n",
    "if \"cov_2\" in globals():\n",
    "    try:\n",
    "        V_2 = _coerce_cov(cov_2, names)\n",
    "    except Exception:\n",
    "        V_2 = V_firm\n",
    "else:\n",
    "    V_2 = V_firm\n",
    "\n",
    "\n",
    "def _se_from_V(V):\n",
    "    se = np.full(k, np.nan, dtype=float)\n",
    "    for j in range(k):\n",
    "        if not np.all(np.isfinite(G[j, :])):\n",
    "            continue\n",
    "        g = G[j, :].astype(float)\n",
    "        v = (g @ V @ g).item()  # scalar quadratic form\n",
    "        se[j] = np.sqrt(v) if v >= 0 else np.nan\n",
    "    return se\n",
    "\n",
    "\n",
    "se_ape_firm = _se_from_V(V_firm)\n",
    "se_ape_2 = _se_from_V(V_2)\n",
    "\n",
    "# p-values (normal approximation)\n",
    "z_firm = ape / se_ape_firm\n",
    "p_ape_firm = 2 * (1 - stats.norm.cdf(np.abs(z_firm)))\n",
    "\n",
    "z_2 = ape / se_ape_2\n",
    "p_ape_2 = 2 * (1 - stats.norm.cdf(np.abs(z_2)))\n",
    "\n",
    "ape_tbl = pd.DataFrame({\n",
    "    \"APE\": ape,\n",
    "    \"se_APE_firm\": se_ape_firm,\n",
    "    \"p_APE_firm\": p_ape_firm,\n",
    "    \"se_APE_firm_year\": se_ape_2,\n",
    "    \"p_APE_firm_year\": p_ape_2,\n",
    "}, index=pd.Index(names, name=\"feature\"))\n",
    "\n",
    "display(ape_tbl)\n",
    "\n",
    "infer_tbl_with_ape = infer_tbl.join(ape_tbl, how=\"left\")\n",
    "display(infer_tbl_with_ape)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7A.6 Fixed effects logit + Firth bias correction (rare-event safeguard)\nTo address unobserved heterogeneity and rare-event bias, I fit (i) time fixed-effects logit, (ii) firm+time FE logit when feasible, and (iii) a Firth-penalized logit (Jeffreys prior).\n"
   ],
   "id": "5593d93340638081"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, brier_score_loss\n",
    "\n",
    "def _safe_metric(y, p, metric):\n",
    "    y = np.asarray(y).astype(int)\n",
    "    p = np.asarray(p).astype(float)\n",
    "    if metric == \"roc_auc\":\n",
    "        return np.nan if np.unique(y).size < 2 else float(roc_auc_score(y, p))\n",
    "    if metric == \"pr_auc\":\n",
    "        return np.nan if np.unique(y).size < 2 else float(average_precision_score(y, p))\n",
    "    if metric == \"brier\":\n",
    "        return float(brier_score_loss(y, p))\n",
    "    raise ValueError(metric)\n",
    "\n",
    "def _build_fe_design(base_X: pd.DataFrame, df_meta: pd.DataFrame, add_firm_fe: bool):\n",
    "    year_fe = pd.get_dummies(df_meta[\"label_year\"].astype(int), prefix=\"year\", drop_first=True)\n",
    "    X_fe = pd.concat([base_X, year_fe], axis=1)\n",
    "\n",
    "    if add_firm_fe:\n",
    "        firm_fe = pd.get_dummies(df_meta[\"firm_id\"].astype(str), prefix=\"firm\", drop_first=True)\n",
    "        X_fe = pd.concat([X_fe, firm_fe], axis=1)\n",
    "\n",
    "    return X_fe\n",
    "\n",
    "def _as_float_np(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Crucial for Firth + expit: avoid object arrays from pandas nullable dtypes.\n",
    "    \"\"\"\n",
    "    arr = df.to_numpy(dtype=np.float64, copy=False)\n",
    "    # If any inf slipped in, clip to finite (optional but robust)\n",
    "    arr = np.nan_to_num(arr, nan=np.nan, posinf=np.nan, neginf=np.nan)\n",
    "    return arr\n",
    "\n",
    "def firth_logit(X: np.ndarray, y: np.ndarray, max_iter: int = 100, tol: float = 1e-6):\n",
    "    X = np.asarray(X, dtype=np.float64)\n",
    "    y = np.asarray(y, dtype=np.float64)\n",
    "\n",
    "    n, p = X.shape\n",
    "    beta = np.zeros(p, dtype=np.float64)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        eta = X @ beta\n",
    "        mu = expit(eta)\n",
    "\n",
    "        W = np.clip(mu * (1.0 - mu), 1e-8, None)  # avoid zeros\n",
    "        sqrtW = np.sqrt(W)\n",
    "\n",
    "        WX = X * sqrtW[:, None]\n",
    "        XtWX = WX.T @ WX\n",
    "        try:\n",
    "            XtWX_inv = np.linalg.inv(XtWX)\n",
    "        except np.linalg.LinAlgError:\n",
    "            XtWX_inv = np.linalg.pinv(XtWX)\n",
    "\n",
    "        h = np.sum((WX @ XtWX_inv) * WX, axis=1)\n",
    "        a = (0.5 - mu) * h\n",
    "        z = eta + (y - mu + a) / W\n",
    "\n",
    "        beta_new = XtWX_inv @ (WX.T @ (sqrtW * z))\n",
    "\n",
    "        if np.max(np.abs(beta_new - beta)) < tol:\n",
    "            beta = beta_new\n",
    "            break\n",
    "        beta = beta_new\n",
    "\n",
    "    return beta\n",
    "\n",
    "if \"X\" not in globals():\n",
    "    raise ValueError(\"Base design matrix X not found; run preprocessing first.\")\n",
    "\n",
    "df_meta = df_model.loc[X.index, [\"firm_id\", \"label_year\", \"split\", TARGET_NAME]].copy()\n",
    "base_X = X.copy()\n",
    "\n",
    "FE_MAX_FIRMS = 500\n",
    "use_firm_fe = df_meta[\"firm_id\"].nunique() <= FE_MAX_FIRMS\n",
    "\n",
    "X_fe_year = _build_fe_design(base_X, df_meta, add_firm_fe=False)\n",
    "X_fe_firm_year = _build_fe_design(base_X, df_meta, add_firm_fe=True) if use_firm_fe else None\n",
    "\n",
    "trainval_mask = df_meta[\"split\"].isin([\"train\", \"val\"])\n",
    "test_mask     = df_meta[\"split\"].eq(\"test\")\n",
    "\n",
    "idx_trainval = df_meta.index[trainval_mask]\n",
    "idx_test     = df_meta.index[test_mask]\n",
    "\n",
    "# --- Time FE logit ---\n",
    "log_fe = LogisticRegression(\n",
    "    penalty=\"l2\",\n",
    "    C=float(globals().get(\"best_C\", 1.0)),\n",
    "    solver=\"lbfgs\",\n",
    "    max_iter=3000\n",
    ")\n",
    "log_fe.fit(X_fe_year.loc[idx_trainval], df_meta.loc[idx_trainval, TARGET_NAME].astype(int))\n",
    "p_te_fe_year = log_fe.predict_proba(X_fe_year.loc[idx_test])[:, 1]\n",
    "df_model.loc[idx_test, \"pd_logit_fe_year\"] = p_te_fe_year\n",
    "\n",
    "print(\"Time FE logit test metrics:\", {\n",
    "    \"roc_auc\": _safe_metric(df_meta.loc[idx_test, TARGET_NAME], p_te_fe_year, \"roc_auc\"),\n",
    "    \"pr_auc\":  _safe_metric(df_meta.loc[idx_test, TARGET_NAME], p_te_fe_year, \"pr_auc\"),\n",
    "    \"brier\":   _safe_metric(df_meta.loc[idx_test, TARGET_NAME], p_te_fe_year, \"brier\"),\n",
    "})\n",
    "\n",
    "# --- Firm + time FE logit (if feasible) ---\n",
    "if use_firm_fe and X_fe_firm_year is not None:\n",
    "    log_fy = LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        C=float(globals().get(\"best_C\", 1.0)),\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=3000\n",
    "    )\n",
    "    log_fy.fit(X_fe_firm_year.loc[idx_trainval], df_meta.loc[idx_trainval, TARGET_NAME].astype(int))\n",
    "    p_te_fe_fy = log_fy.predict_proba(X_fe_firm_year.loc[idx_test])[:, 1]\n",
    "    df_model.loc[idx_test, \"pd_logit_fe_firm_year\"] = p_te_fe_fy\n",
    "\n",
    "    print(\"Firm+Time FE logit test metrics:\", {\n",
    "        \"roc_auc\": _safe_metric(df_meta.loc[idx_test, TARGET_NAME], p_te_fe_fy, \"roc_auc\"),\n",
    "        \"pr_auc\":  _safe_metric(df_meta.loc[idx_test, TARGET_NAME], p_te_fe_fy, \"pr_auc\"),\n",
    "        \"brier\":   _safe_metric(df_meta.loc[idx_test, TARGET_NAME], p_te_fe_fy, \"brier\"),\n",
    "    })\n",
    "else:\n",
    "    print(f\"Skipping firm FE logit: too many firms (n={df_meta['firm_id'].nunique()}).\")\n",
    "\n",
    "# --- Firth bias-corrected logit (time FE only) ---\n",
    "X_firth = sm.add_constant(X_fe_year, has_constant=\"add\")\n",
    "y_firth = df_meta[TARGET_NAME].astype(int).to_numpy()\n",
    "\n",
    "X_train = _as_float_np(X_firth.loc[idx_trainval])\n",
    "y_train = y_firth[trainval_mask.to_numpy()]  # boolean mask to numpy\n",
    "\n",
    "beta_firth = firth_logit(X_train, y_train)\n",
    "\n",
    "X_test = _as_float_np(X_firth.loc[idx_test])\n",
    "linpred = X_test @ beta_firth\n",
    "p_te_firth = expit(linpred)  # now guaranteed float64 input\n",
    "\n",
    "df_model.loc[idx_test, \"pd_logit_firth\"] = p_te_firth\n",
    "\n",
    "print(\"Firth logit (time FE) test metrics:\", {\n",
    "    \"roc_auc\": _safe_metric(df_meta.loc[idx_test, TARGET_NAME], p_te_firth, \"roc_auc\"),\n",
    "    \"pr_auc\":  _safe_metric(df_meta.loc[idx_test, TARGET_NAME], p_te_firth, \"pr_auc\"),\n",
    "    \"brier\":   _safe_metric(df_meta.loc[idx_test, TARGET_NAME], p_te_firth, \"brier\"),\n",
    "})"
   ],
   "id": "1aa3a50a2efb4665",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c4d87dc56ea0f37a",
   "metadata": {},
   "source": [
    "### 7B. Tree-based model (XGBoost; nonlinear)\n",
    "\n",
    "I train a regularized XGBoost model with **decoupled weighting**: class imbalance is handled via `scale_pos_weight`, while decision-theoretic costs are handled via sample weights (`COST_FN` / `COST_FP`) only. I then calibrate probabilities on the validation split via isotonic regression.\n",
    "\n",
    "Hyperparameters are tuned via Bayesian Optimization to focus sampling on promising regions under the policy-aligned validation cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a50952",
   "metadata": {},
   "source": [
    "#### 7B.1 Policy-aligned hyperparameter search and training\n",
    "\n",
    "This search uses early stopping on a held-out validation split and prints progress so long runs have visible feedback.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "25b421e0b0451d9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T18:02:27.287674Z",
     "start_time": "2026-01-17T17:24:58.594204Z"
    }
   },
   "source": [
    "# Build DMatrix objects\n",
    "X_tr = splits[\"train\"][\"X\"]\n",
    "TRAIN_FEATURE_LIST = X_tr.columns.tolist()\n",
    "y_tr = splits[\"train\"][\"y\"].astype(int)\n",
    "X_va = splits[\"val\"][\"X\"]\n",
    "y_va = splits[\"val\"][\"y\"].astype(int)\n",
    "X_te = splits[\"test\"][\"X\"]\n",
    "y_te = splits[\"test\"][\"y\"].astype(int)\n",
    "\n",
    "# Split validation into early-stopping vs calibration to avoid double-use\n",
    "X_va_es, X_va_cal, y_va_es, y_va_cal = train_test_split(\n",
    "    X_va, y_va, test_size=0.5, random_state=SEED, stratify=y_va\n",
    ")\n",
    "\n",
    "n_pos = int(y_tr.sum())\n",
    "n_neg = int((y_tr==0).sum())\n",
    "imbalance = (n_neg / max(n_pos, 1))\n",
    "\n",
    "# Cost weights only; imbalance handled separately via scale_pos_weight\n",
    "w_pos = CONFIG[\"COST_FN\"]\n",
    "w_neg = CONFIG[\"COST_FP\"]\n",
    "\n",
    "w_tr = np.where(y_tr.values==1, w_pos, w_neg).astype(float)\n",
    "w_va_es = np.where(y_va_es.values==1, w_pos, w_neg).astype(float)\n",
    "w_va_cal = np.where(y_va_cal.values==1, w_pos, w_neg).astype(float)\n",
    "w_va = np.where(y_va.values==1, w_pos, w_neg).astype(float)\n",
    "w_te = np.where(y_te.values==1, w_pos, w_neg).astype(float)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_tr, label=y_tr, weight=w_tr, feature_names=X_tr.columns.tolist())\n",
    "dval_es = xgb.DMatrix(X_va_es, label=y_va_es, weight=w_va_es, feature_names=X_tr.columns.tolist())\n",
    "dval_cal = xgb.DMatrix(X_va_cal, label=y_va_cal, weight=w_va_cal, feature_names=X_tr.columns.tolist())\n",
    "dval_full = xgb.DMatrix(X_va, label=y_va, weight=w_va, feature_names=X_tr.columns.tolist())\n",
    "dtest  = xgb.DMatrix(X_te, label=y_te, weight=w_te, feature_names=X_tr.columns.tolist())\n",
    "dall   = xgb.DMatrix(X, label=y.astype(int), feature_names=X_tr.columns.tolist())\n",
    "\n",
    "\n",
    "def policy_cost_eval(preds, dtrain):\n",
    "    # Expected cost under capacity policy; lower is better.\n",
    "    y_true = dtrain.get_label().astype(int)\n",
    "    n = len(preds)\n",
    "    k = int(math.ceil(CONFIG[\"CAPACITY_PCT\"] * n))\n",
    "    if k <= 0:\n",
    "        return \"cap_cost\", 0.0\n",
    "    top_idx = np.argsort(preds)[-k:]\n",
    "    y_pred = np.zeros_like(y_true)\n",
    "    y_pred[top_idx] = 1\n",
    "    fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    fn = np.sum((y_pred == 0) & (y_true == 1))\n",
    "    cost = CONFIG[\"COST_FN\"] * fn + CONFIG[\"COST_FP\"] * fp\n",
    "    return \"cap_cost\", float(cost) / float(n)\n",
    "\n",
    "\n",
    "base_params = CONFIG[\"XGB_PARAMS\"].copy()\n",
    "base_params[\"scale_pos_weight\"] = float(imbalance)\n",
    "\n",
    "# Bayesian Optimization over the specified parameter ranges (policy-aligned)\n",
    "param_space = [\n",
    "    Integer(2, 4, name=\"max_depth\"),\n",
    "    Integer(10, 100, name=\"min_child_weight\"),\n",
    "    Real(0.5, 10.0, name=\"gamma\"),\n",
    "    Real(0.01, 0.05, prior=\"log-uniform\", name=\"eta\"),\n",
    "    Real(1.0, 50.0, prior=\"log-uniform\", name=\"reg_lambda\"),\n",
    "    Real(0.0, 2.0, name=\"reg_alpha\"),\n",
    "    Real(0.5, 0.9, name=\"subsample\"),\n",
    "    Real(0.3, 0.8, name=\"colsample_bytree\"),\n",
    "    Integer(1, 5, name=\"max_delta_step\"),\n",
    "    Categorical([\"gbtree\", \"dart\"], name=\"booster\"),\n",
    "    Real(0.05, 0.2, name=\"rate_drop\"),\n",
    "    Real(0.0, 0.1, name=\"skip_drop\"),\n",
    "]\n",
    "\n",
    "# Use the configured trial count, but allow a global override if it exists in-session.\n",
    "# This prevents stale values from earlier runs from silently capping the search.\n",
    "config_trials = CONFIG.get(\"XGB_N_TRIALS\", globals().get(\"XGB_N_TRIALS\", 25))\n",
    "num_trials = int(config_trials)\n",
    "CONFIG[\"XGB_N_TRIALS\"] = num_trials\n",
    "print(f\"Running {num_trials} XGBoost trials via Bayesian Optimization...\")\n",
    "\n",
    "trial_records = []\n",
    "\n",
    "\n",
    "def bo_objective(params):\n",
    "    (max_depth, min_child_weight, gamma, eta, reg_lambda, reg_alpha,\n",
    "     subsample, colsample_bytree, max_delta_step, booster, rate_drop, skip_drop) = params\n",
    "\n",
    "    xgb_params = base_params.copy()\n",
    "    xgb_params.update({\n",
    "        \"max_depth\": int(max_depth),\n",
    "        \"min_child_weight\": float(min_child_weight),\n",
    "        \"gamma\": float(gamma),\n",
    "        \"eta\": float(eta),\n",
    "        \"reg_lambda\": float(reg_lambda),\n",
    "        \"reg_alpha\": float(reg_alpha),\n",
    "        \"subsample\": float(subsample),\n",
    "        \"colsample_bytree\": float(colsample_bytree),\n",
    "        \"max_delta_step\": int(max_delta_step),\n",
    "        \"booster\": booster,\n",
    "    })\n",
    "\n",
    "    if booster == \"dart\":\n",
    "        xgb_params.update({\n",
    "            \"rate_drop\": float(rate_drop),\n",
    "            \"skip_drop\": float(skip_drop),\n",
    "            \"sample_type\": \"uniform\",\n",
    "            \"normalize_type\": \"tree\",\n",
    "        })\n",
    "\n",
    "    evals = [(dtrain, \"train\"), (dval_es, \"val_es\")]\n",
    "    evals_result = {}\n",
    "\n",
    "    model = xgb.train(\n",
    "        params=xgb_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=CONFIG[\"XGB_NUM_BOOST_ROUND\"],\n",
    "        evals=evals,\n",
    "        evals_result=evals_result,\n",
    "        **{XGB_TRAIN_METRIC_KW: policy_cost_eval},\n",
    "        early_stopping_rounds=CONFIG[\"XGB_EARLY_STOPPING\"],\n",
    "        maximize=False,\n",
    "        verbose_eval=200,\n",
    "    )\n",
    "\n",
    "    val_scores = np.array(evals_result.get(\"val_es\", {}).get(\"cap_cost\", []))\n",
    "    if len(val_scores) > 0:\n",
    "        trial_best_score = float(np.min(val_scores))\n",
    "        trial_best_iter = int(np.argmin(val_scores)) + 1\n",
    "    else:\n",
    "        trial_best_score = float(\"inf\")\n",
    "        trial_best_iter = int(getattr(model, \"best_iteration\", 0)) + 1\n",
    "\n",
    "    trial_records.append({\n",
    "        \"score\": trial_best_score,\n",
    "        \"iteration\": trial_best_iter,\n",
    "        \"params\": xgb_params,\n",
    "        \"model\": model,\n",
    "    })\n",
    "\n",
    "    print(\n",
    "        f\"Trial {len(trial_records)}/{num_trials}: \"\n",
    "        f\"best cap_cost={trial_best_score:.4f} \"\n",
    "        f\"at iter={trial_best_iter}\"\n",
    "    )\n",
    "\n",
    "    return trial_best_score\n",
    "\n",
    "\n",
    "result = gp_minimize(\n",
    "    bo_objective,\n",
    "    param_space,\n",
    "    n_calls=num_trials,\n",
    "    random_state=SEED,\n",
    "    n_initial_points=min(10, num_trials),\n",
    "    acq_func=\"EI\",\n",
    ")\n",
    "\n",
    "best_record = min(trial_records, key=lambda r: r[\"score\"])\n",
    "\n",
    "xgb_model = best_record[\"model\"]\n",
    "best_score = best_record[\"score\"]\n",
    "best_iteration = best_record[\"iteration\"]\n",
    "best_params = best_record[\"params\"]\n",
    "\n",
    "print(\"Best iteration:\", best_iteration)\n",
    "print(\"Best policy cost (val_es):\", best_score)\n",
    "print(\"Best params:\", best_params)\n",
    "\n",
    "CONFIG[\"XGB_BEST_PARAMS\"] = best_params.copy()\n",
    "print(\"Stored best XGB params for downstream validation.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-aucpr:0.69807\ttrain-auc:0.75819\ttrain-cap_cost:0.57902\tval_es-aucpr:0.74889\tval_es-auc:0.77272\tval_es-cap_cost:0.62991\n",
      "[200]\ttrain-aucpr:0.85665\ttrain-auc:0.89041\ttrain-cap_cost:0.35078\tval_es-aucpr:0.87363\tval_es-auc:0.88877\tval_es-cap_cost:0.38574\n",
      "[275]\ttrain-aucpr:0.85648\ttrain-auc:0.89034\ttrain-cap_cost:0.35145\tval_es-aucpr:0.87286\tval_es-auc:0.88877\tval_es-cap_cost:0.38574\n",
      "Trial 1/100: best cap_cost=0.3767 at iter=76\n",
      "[0]\ttrain-aucpr:0.73869\ttrain-auc:0.81726\ttrain-cap_cost:0.46690\tval_es-aucpr:0.78312\tval_es-auc:0.82227\tval_es-cap_cost:0.49426\n",
      "[200]\ttrain-aucpr:0.85029\ttrain-auc:0.88949\ttrain-cap_cost:0.34144\tval_es-aucpr:0.85780\tval_es-auc:0.88285\tval_es-cap_cost:0.38574\n",
      "[219]\ttrain-aucpr:0.85056\ttrain-auc:0.88974\ttrain-cap_cost:0.33943\tval_es-aucpr:0.85767\tval_es-auc:0.88304\tval_es-cap_cost:0.38574\n",
      "Trial 2/100: best cap_cost=0.3767 at iter=20\n",
      "[0]\ttrain-aucpr:0.72355\ttrain-auc:0.79852\ttrain-cap_cost:0.48492\tval_es-aucpr:0.77918\tval_es-auc:0.82080\tval_es-cap_cost:0.53043\n",
      "[200]\ttrain-aucpr:0.82456\ttrain-auc:0.86359\ttrain-cap_cost:0.40283\tval_es-aucpr:0.85318\tval_es-auc:0.87268\tval_es-cap_cost:0.36765\n",
      "[228]\ttrain-aucpr:0.82460\ttrain-auc:0.86373\ttrain-cap_cost:0.40150\tval_es-aucpr:0.85253\tval_es-auc:0.87273\tval_es-cap_cost:0.36765\n",
      "Trial 3/100: best cap_cost=0.3677 at iter=29\n",
      "[0]\ttrain-aucpr:0.74507\ttrain-auc:0.81621\ttrain-cap_cost:0.47358\tval_es-aucpr:0.78153\tval_es-auc:0.81295\tval_es-cap_cost:0.56661\n",
      "[200]\ttrain-aucpr:0.89985\ttrain-auc:0.93151\ttrain-cap_cost:0.25201\tval_es-aucpr:0.89348\tval_es-auc:0.90902\tval_es-cap_cost:0.34052\n",
      "[400]\ttrain-aucpr:0.91557\ttrain-auc:0.94672\ttrain-cap_cost:0.21063\tval_es-aucpr:0.89824\tval_es-auc:0.91142\tval_es-cap_cost:0.30435\n",
      "[586]\ttrain-aucpr:0.92430\ttrain-auc:0.95460\ttrain-cap_cost:0.18060\tval_es-aucpr:0.89734\tval_es-auc:0.91058\tval_es-cap_cost:0.31339\n",
      "Trial 4/100: best cap_cost=0.2953 at iter=388\n",
      "[0]\ttrain-aucpr:0.67377\ttrain-auc:0.77620\ttrain-cap_cost:0.59103\tval_es-aucpr:0.71844\tval_es-auc:0.76920\tval_es-cap_cost:0.70226\n",
      "[200]\ttrain-aucpr:0.88345\ttrain-auc:0.91883\ttrain-cap_cost:0.29138\tval_es-aucpr:0.88042\tval_es-auc:0.90518\tval_es-cap_cost:0.33148\n",
      "[400]\ttrain-aucpr:0.89892\ttrain-auc:0.93434\ttrain-cap_cost:0.24200\tval_es-aucpr:0.88894\tval_es-auc:0.90875\tval_es-cap_cost:0.32244\n",
      "[402]\ttrain-aucpr:0.89880\ttrain-auc:0.93436\ttrain-cap_cost:0.24200\tval_es-aucpr:0.88865\tval_es-auc:0.90853\tval_es-cap_cost:0.33148\n",
      "Trial 5/100: best cap_cost=0.3224 at iter=204\n",
      "[0]\ttrain-aucpr:0.76191\ttrain-auc:0.81330\ttrain-cap_cost:0.48092\tval_es-aucpr:0.79728\tval_es-auc:0.81984\tval_es-cap_cost:0.54852\n",
      "[200]\ttrain-aucpr:0.88828\ttrain-auc:0.92150\ttrain-cap_cost:0.28404\tval_es-aucpr:0.87772\tval_es-auc:0.89904\tval_es-cap_cost:0.37670\n",
      "[210]\ttrain-aucpr:0.88873\ttrain-auc:0.92181\ttrain-cap_cost:0.28337\tval_es-aucpr:0.87808\tval_es-auc:0.89926\tval_es-cap_cost:0.37670\n",
      "Trial 6/100: best cap_cost=0.3677 at iter=11\n",
      "[0]\ttrain-aucpr:0.65522\ttrain-auc:0.75343\ttrain-cap_cost:0.61639\tval_es-aucpr:0.69786\tval_es-auc:0.75312\tval_es-cap_cost:0.74748\n",
      "[200]\ttrain-aucpr:0.83623\ttrain-auc:0.87672\ttrain-cap_cost:0.37814\tval_es-aucpr:0.86708\tval_es-auc:0.88524\tval_es-cap_cost:0.38574\n",
      "[400]\ttrain-aucpr:0.84703\ttrain-auc:0.88587\ttrain-cap_cost:0.35545\tval_es-aucpr:0.87229\tval_es-auc:0.89104\tval_es-cap_cost:0.37670\n",
      "[600]\ttrain-aucpr:0.85618\ttrain-auc:0.89438\ttrain-cap_cost:0.33676\tval_es-aucpr:0.88166\tval_es-auc:0.89749\tval_es-cap_cost:0.36765\n",
      "[800]\ttrain-aucpr:0.86273\ttrain-auc:0.89931\ttrain-cap_cost:0.32809\tval_es-aucpr:0.88293\tval_es-auc:0.90009\tval_es-cap_cost:0.36765\n",
      "[845]\ttrain-aucpr:0.86330\ttrain-auc:0.89998\ttrain-cap_cost:0.32876\tval_es-aucpr:0.88351\tval_es-auc:0.90024\tval_es-cap_cost:0.36765\n",
      "Trial 7/100: best cap_cost=0.3586 at iter=646\n",
      "[0]\ttrain-aucpr:0.72629\ttrain-auc:0.79976\ttrain-cap_cost:0.48225\tval_es-aucpr:0.78315\tval_es-auc:0.82299\tval_es-cap_cost:0.52139\n",
      "[200]\ttrain-aucpr:0.87693\ttrain-auc:0.91113\ttrain-cap_cost:0.31541\tval_es-aucpr:0.88590\tval_es-auc:0.90170\tval_es-cap_cost:0.36765\n",
      "[400]\ttrain-aucpr:0.89180\ttrain-auc:0.92709\ttrain-cap_cost:0.26469\tval_es-aucpr:0.88931\tval_es-auc:0.90533\tval_es-cap_cost:0.34957\n",
      "[522]\ttrain-aucpr:0.89881\ttrain-auc:0.93376\ttrain-cap_cost:0.24667\tval_es-aucpr:0.89232\tval_es-auc:0.90707\tval_es-cap_cost:0.34052\n",
      "Trial 8/100: best cap_cost=0.3315 at iter=323\n",
      "[0]\ttrain-aucpr:0.76232\ttrain-auc:0.83491\ttrain-cap_cost:0.46023\tval_es-aucpr:0.79207\tval_es-auc:0.82318\tval_es-cap_cost:0.51235\n",
      "[200]\ttrain-aucpr:0.85622\ttrain-auc:0.89530\ttrain-cap_cost:0.32408\tval_es-aucpr:0.86313\tval_es-auc:0.88292\tval_es-cap_cost:0.39478\n",
      "[229]\ttrain-aucpr:0.85703\ttrain-auc:0.89591\ttrain-cap_cost:0.32342\tval_es-aucpr:0.86325\tval_es-auc:0.88350\tval_es-cap_cost:0.39478\n",
      "Trial 9/100: best cap_cost=0.3767 at iter=31\n",
      "[0]\ttrain-aucpr:0.64836\ttrain-auc:0.74326\ttrain-cap_cost:0.62306\tval_es-aucpr:0.69943\tval_es-auc:0.75634\tval_es-cap_cost:0.73843\n",
      "[200]\ttrain-aucpr:0.77429\ttrain-auc:0.81762\ttrain-cap_cost:0.50761\tval_es-aucpr:0.80966\tval_es-auc:0.82745\tval_es-cap_cost:0.56661\n",
      "[202]\ttrain-aucpr:0.77429\ttrain-auc:0.81761\ttrain-cap_cost:0.50828\tval_es-aucpr:0.80968\tval_es-auc:0.82749\tval_es-cap_cost:0.56661\n",
      "Trial 10/100: best cap_cost=0.5304 at iter=4\n",
      "[0]\ttrain-aucpr:0.65450\ttrain-auc:0.75201\ttrain-cap_cost:0.61773\tval_es-aucpr:0.69913\tval_es-auc:0.75311\tval_es-cap_cost:0.74748\n",
      "[200]\ttrain-aucpr:0.83607\ttrain-auc:0.87511\ttrain-cap_cost:0.38215\tval_es-aucpr:0.86113\tval_es-auc:0.88199\tval_es-cap_cost:0.39478\n",
      "[337]\ttrain-aucpr:0.84384\ttrain-auc:0.88185\ttrain-cap_cost:0.36746\tval_es-aucpr:0.86551\tval_es-auc:0.88641\tval_es-cap_cost:0.39478\n",
      "Trial 11/100: best cap_cost=0.3857 at iter=139\n",
      "[0]\ttrain-aucpr:0.70854\ttrain-auc:0.76688\ttrain-cap_cost:0.56701\tval_es-aucpr:0.74558\tval_es-auc:0.77442\tval_es-cap_cost:0.62991\n",
      "[200]\ttrain-aucpr:0.87011\ttrain-auc:0.90378\ttrain-cap_cost:0.32208\tval_es-aucpr:0.87945\tval_es-auc:0.89483\tval_es-cap_cost:0.37670\n",
      "[400]\ttrain-aucpr:0.88620\ttrain-auc:0.91907\ttrain-cap_cost:0.28805\tval_es-aucpr:0.88782\tval_es-auc:0.90569\tval_es-cap_cost:0.35861\n",
      "[431]\ttrain-aucpr:0.88782\ttrain-auc:0.92069\ttrain-cap_cost:0.28538\tval_es-aucpr:0.88851\tval_es-auc:0.90673\tval_es-cap_cost:0.35861\n",
      "Trial 12/100: best cap_cost=0.3586 at iter=233\n",
      "[0]\ttrain-aucpr:0.71836\ttrain-auc:0.79645\ttrain-cap_cost:0.49093\tval_es-aucpr:0.77353\tval_es-auc:0.81413\tval_es-cap_cost:0.52139\n",
      "[200]\ttrain-aucpr:0.81454\ttrain-auc:0.85589\ttrain-cap_cost:0.42419\tval_es-aucpr:0.84983\tval_es-auc:0.86848\tval_es-cap_cost:0.39478\n",
      "[400]\ttrain-aucpr:0.81536\ttrain-auc:0.85671\ttrain-cap_cost:0.41952\tval_es-aucpr:0.85023\tval_es-auc:0.86882\tval_es-cap_cost:0.38574\n",
      "[413]\ttrain-aucpr:0.81539\ttrain-auc:0.85675\ttrain-cap_cost:0.42018\tval_es-aucpr:0.85025\tval_es-auc:0.86890\tval_es-cap_cost:0.38574\n",
      "Trial 13/100: best cap_cost=0.3857 at iter=214\n",
      "[0]\ttrain-aucpr:0.73149\ttrain-auc:0.80414\ttrain-cap_cost:0.51095\tval_es-aucpr:0.77911\tval_es-auc:0.81080\tval_es-cap_cost:0.55756\n",
      "[200]\ttrain-aucpr:0.88842\ttrain-auc:0.92345\ttrain-cap_cost:0.27870\tval_es-aucpr:0.88962\tval_es-auc:0.90723\tval_es-cap_cost:0.34957\n",
      "[379]\ttrain-aucpr:0.90326\ttrain-auc:0.93803\ttrain-cap_cost:0.23399\tval_es-aucpr:0.89518\tval_es-auc:0.91059\tval_es-cap_cost:0.34052\n",
      "Trial 14/100: best cap_cost=0.3315 at iter=180\n",
      "[0]\ttrain-aucpr:0.73214\ttrain-auc:0.80707\ttrain-cap_cost:0.48759\tval_es-aucpr:0.77971\tval_es-auc:0.81484\tval_es-cap_cost:0.54852\n",
      "[200]\ttrain-aucpr:0.89520\ttrain-auc:0.92779\ttrain-cap_cost:0.26736\tval_es-aucpr:0.88953\tval_es-auc:0.90696\tval_es-cap_cost:0.34957\n",
      "[400]\ttrain-aucpr:0.91324\ttrain-auc:0.94495\ttrain-cap_cost:0.21464\tval_es-aucpr:0.88883\tval_es-auc:0.90801\tval_es-cap_cost:0.30435\n",
      "[548]\ttrain-aucpr:0.92268\ttrain-auc:0.95296\ttrain-cap_cost:0.18794\tval_es-aucpr:0.89273\tval_es-auc:0.90929\tval_es-cap_cost:0.31339\n",
      "Trial 15/100: best cap_cost=0.3043 at iter=350\n",
      "[0]\ttrain-aucpr:0.72355\ttrain-auc:0.79852\ttrain-cap_cost:0.48492\tval_es-aucpr:0.77918\tval_es-auc:0.82080\tval_es-cap_cost:0.53043\n",
      "[200]\ttrain-aucpr:0.84076\ttrain-auc:0.87756\ttrain-cap_cost:0.38482\tval_es-aucpr:0.86270\tval_es-auc:0.88341\tval_es-cap_cost:0.38574\n",
      "[228]\ttrain-aucpr:0.84344\ttrain-auc:0.88042\ttrain-cap_cost:0.37347\tval_es-aucpr:0.86419\tval_es-auc:0.88574\tval_es-cap_cost:0.37670\n",
      "Trial 16/100: best cap_cost=0.3767 at iter=29\n",
      "[0]\ttrain-aucpr:0.70148\ttrain-auc:0.76244\ttrain-cap_cost:0.57568\tval_es-aucpr:0.75173\tval_es-auc:0.77846\tval_es-cap_cost:0.63896\n",
      "[200]\ttrain-aucpr:0.86896\ttrain-auc:0.90302\ttrain-cap_cost:0.32542\tval_es-aucpr:0.88055\tval_es-auc:0.89732\tval_es-cap_cost:0.35861\n",
      "[400]\ttrain-aucpr:0.88395\ttrain-auc:0.91720\ttrain-cap_cost:0.29339\tval_es-aucpr:0.88732\tval_es-auc:0.90538\tval_es-cap_cost:0.35861\n",
      "[514]\ttrain-aucpr:0.89024\ttrain-auc:0.92322\ttrain-cap_cost:0.28070\tval_es-aucpr:0.89096\tval_es-auc:0.90797\tval_es-cap_cost:0.34957\n",
      "Trial 17/100: best cap_cost=0.3496 at iter=316\n",
      "[0]\ttrain-aucpr:0.72000\ttrain-auc:0.78370\ttrain-cap_cost:0.51829\tval_es-aucpr:0.76916\tval_es-auc:0.79607\tval_es-cap_cost:0.57565\n",
      "[200]\ttrain-aucpr:0.78192\ttrain-auc:0.82025\ttrain-cap_cost:0.48158\tval_es-aucpr:0.81292\tval_es-auc:0.82855\tval_es-cap_cost:0.58470\n",
      "[203]\ttrain-aucpr:0.78195\ttrain-auc:0.82034\ttrain-cap_cost:0.48158\tval_es-aucpr:0.81289\tval_es-auc:0.82858\tval_es-cap_cost:0.58470\n",
      "Trial 18/100: best cap_cost=0.4762 at iter=5\n",
      "[0]\ttrain-aucpr:0.63029\ttrain-auc:0.72990\ttrain-cap_cost:0.66444\tval_es-aucpr:0.69538\tval_es-auc:0.75609\tval_es-cap_cost:0.72939\n",
      "[200]\ttrain-aucpr:0.78785\ttrain-auc:0.82786\ttrain-cap_cost:0.51228\tval_es-aucpr:0.82515\tval_es-auc:0.84007\tval_es-cap_cost:0.53948\n",
      "[209]\ttrain-aucpr:0.78799\ttrain-auc:0.82784\ttrain-cap_cost:0.51095\tval_es-aucpr:0.82549\tval_es-auc:0.84016\tval_es-cap_cost:0.54852\n",
      "Trial 19/100: best cap_cost=0.4671 at iter=11\n",
      "[0]\ttrain-aucpr:0.62809\ttrain-auc:0.72934\ttrain-cap_cost:0.68046\tval_es-aucpr:0.68612\tval_es-auc:0.74798\tval_es-cap_cost:0.72939\n",
      "[200]\ttrain-aucpr:0.78775\ttrain-auc:0.82785\ttrain-cap_cost:0.50561\tval_es-aucpr:0.82268\tval_es-auc:0.83815\tval_es-cap_cost:0.53948\n",
      "[212]\ttrain-aucpr:0.78766\ttrain-auc:0.82798\ttrain-cap_cost:0.50361\tval_es-aucpr:0.82232\tval_es-auc:0.83816\tval_es-cap_cost:0.53948\n",
      "Trial 20/100: best cap_cost=0.5304 at iter=13\n",
      "[0]\ttrain-aucpr:0.70238\ttrain-auc:0.75835\ttrain-cap_cost:0.58836\tval_es-aucpr:0.73046\tval_es-auc:0.75680\tval_es-cap_cost:0.70226\n",
      "[200]\ttrain-aucpr:0.85713\ttrain-auc:0.89053\ttrain-cap_cost:0.35211\tval_es-aucpr:0.87354\tval_es-auc:0.88830\tval_es-cap_cost:0.39478\n",
      "[247]\ttrain-aucpr:0.85700\ttrain-auc:0.89045\ttrain-cap_cost:0.35412\tval_es-aucpr:0.87234\tval_es-auc:0.88844\tval_es-cap_cost:0.39478\n",
      "Trial 21/100: best cap_cost=0.3586 at iter=48\n",
      "[0]\ttrain-aucpr:0.69498\ttrain-auc:0.75184\ttrain-cap_cost:0.58236\tval_es-aucpr:0.75183\tval_es-auc:0.77849\tval_es-cap_cost:0.63896\n",
      "[200]\ttrain-aucpr:0.82463\ttrain-auc:0.86309\ttrain-cap_cost:0.40417\tval_es-aucpr:0.85097\tval_es-auc:0.86980\tval_es-cap_cost:0.43096\n",
      "[230]\ttrain-aucpr:0.82445\ttrain-auc:0.86284\ttrain-cap_cost:0.40350\tval_es-aucpr:0.85119\tval_es-auc:0.86970\tval_es-cap_cost:0.43096\n",
      "Trial 22/100: best cap_cost=0.4129 at iter=32\n",
      "[0]\ttrain-aucpr:0.73541\ttrain-auc:0.80838\ttrain-cap_cost:0.48892\tval_es-aucpr:0.77029\tval_es-auc:0.80051\tval_es-cap_cost:0.55756\n",
      "[200]\ttrain-aucpr:0.81956\ttrain-auc:0.86263\ttrain-cap_cost:0.40484\tval_es-aucpr:0.84813\tval_es-auc:0.86704\tval_es-cap_cost:0.40383\n",
      "[400]\ttrain-aucpr:0.82609\ttrain-auc:0.86537\ttrain-cap_cost:0.39816\tval_es-aucpr:0.85154\tval_es-auc:0.87077\tval_es-cap_cost:0.40383\n",
      "[482]\ttrain-aucpr:0.82708\ttrain-auc:0.86605\ttrain-cap_cost:0.39749\tval_es-aucpr:0.85310\tval_es-auc:0.87136\tval_es-cap_cost:0.39478\n",
      "Trial 23/100: best cap_cost=0.3948 at iter=284\n",
      "[0]\ttrain-aucpr:0.70536\ttrain-auc:0.78547\ttrain-cap_cost:0.49960\tval_es-aucpr:0.73822\tval_es-auc:0.77920\tval_es-cap_cost:0.57565\n",
      "[200]\ttrain-aucpr:0.82624\ttrain-auc:0.86472\ttrain-cap_cost:0.40350\tval_es-aucpr:0.85063\tval_es-auc:0.87314\tval_es-cap_cost:0.41287\n",
      "[352]\ttrain-aucpr:0.82659\ttrain-auc:0.86566\ttrain-cap_cost:0.40217\tval_es-aucpr:0.85226\tval_es-auc:0.87619\tval_es-cap_cost:0.40383\n",
      "Trial 24/100: best cap_cost=0.4038 at iter=153\n",
      "[0]\ttrain-aucpr:0.73627\ttrain-auc:0.79693\ttrain-cap_cost:0.48559\tval_es-aucpr:0.76605\tval_es-auc:0.79335\tval_es-cap_cost:0.55756\n",
      "[200]\ttrain-aucpr:0.86780\ttrain-auc:0.90290\ttrain-cap_cost:0.32208\tval_es-aucpr:0.88131\tval_es-auc:0.90087\tval_es-cap_cost:0.36765\n",
      "[400]\ttrain-aucpr:0.87486\ttrain-auc:0.90981\ttrain-cap_cost:0.31140\tval_es-aucpr:0.88640\tval_es-auc:0.90438\tval_es-cap_cost:0.34052\n",
      "[548]\ttrain-aucpr:0.88028\ttrain-auc:0.91465\ttrain-cap_cost:0.29872\tval_es-aucpr:0.89028\tval_es-auc:0.90577\tval_es-cap_cost:0.34052\n",
      "Trial 25/100: best cap_cost=0.3405 at iter=349\n",
      "[0]\ttrain-aucpr:0.69423\ttrain-auc:0.75108\ttrain-cap_cost:0.57969\tval_es-aucpr:0.75065\tval_es-auc:0.77595\tval_es-cap_cost:0.62991\n",
      "[200]\ttrain-aucpr:0.85926\ttrain-auc:0.89348\ttrain-cap_cost:0.34544\tval_es-aucpr:0.87956\tval_es-auc:0.89460\tval_es-cap_cost:0.34957\n",
      "[394]\ttrain-aucpr:0.86376\ttrain-auc:0.89812\ttrain-cap_cost:0.33476\tval_es-aucpr:0.88033\tval_es-auc:0.89651\tval_es-cap_cost:0.35861\n",
      "Trial 26/100: best cap_cost=0.3496 at iter=195\n",
      "[0]\ttrain-aucpr:0.73689\ttrain-auc:0.82883\ttrain-cap_cost:0.45355\tval_es-aucpr:0.77919\tval_es-auc:0.81441\tval_es-cap_cost:0.46713\n",
      "[200]\ttrain-aucpr:0.89614\ttrain-auc:0.92884\ttrain-cap_cost:0.26335\tval_es-aucpr:0.88808\tval_es-auc:0.90660\tval_es-cap_cost:0.34957\n",
      "[400]\ttrain-aucpr:0.91178\ttrain-auc:0.94409\ttrain-cap_cost:0.21664\tval_es-aucpr:0.89321\tval_es-auc:0.90935\tval_es-cap_cost:0.34052\n",
      "[412]\ttrain-aucpr:0.91243\ttrain-auc:0.94471\ttrain-cap_cost:0.21397\tval_es-aucpr:0.89286\tval_es-auc:0.90933\tval_es-cap_cost:0.34052\n",
      "Trial 27/100: best cap_cost=0.3315 at iter=213\n",
      "[0]\ttrain-aucpr:0.69803\ttrain-auc:0.75806\ttrain-cap_cost:0.57902\tval_es-aucpr:0.74823\tval_es-auc:0.77059\tval_es-cap_cost:0.62991\n",
      "[200]\ttrain-aucpr:0.86700\ttrain-auc:0.90124\ttrain-cap_cost:0.33276\tval_es-aucpr:0.87805\tval_es-auc:0.89306\tval_es-cap_cost:0.36765\n",
      "[380]\ttrain-aucpr:0.88191\ttrain-auc:0.91509\ttrain-cap_cost:0.29405\tval_es-aucpr:0.88552\tval_es-auc:0.90153\tval_es-cap_cost:0.36765\n",
      "Trial 28/100: best cap_cost=0.3586 at iter=181\n",
      "[0]\ttrain-aucpr:0.70840\ttrain-auc:0.79086\ttrain-cap_cost:0.50160\tval_es-aucpr:0.74855\tval_es-auc:0.79755\tval_es-cap_cost:0.55756\n",
      "[200]\ttrain-aucpr:0.88373\ttrain-auc:0.91705\ttrain-cap_cost:0.29272\tval_es-aucpr:0.88909\tval_es-auc:0.90745\tval_es-cap_cost:0.35861\n",
      "[400]\ttrain-aucpr:0.88964\ttrain-auc:0.92294\ttrain-cap_cost:0.28271\tval_es-aucpr:0.89131\tval_es-auc:0.90855\tval_es-cap_cost:0.34052\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     81\u001b[39m evals = [(dtrain, \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m), (dval_es, \u001b[33m\"\u001b[39m\u001b[33mval_es\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m     82\u001b[39m evals_result = {}\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m model = xgb.train(\n\u001b[32m     85\u001b[39m     params=xgb_params,\n\u001b[32m     86\u001b[39m     dtrain=dtrain,\n\u001b[32m     87\u001b[39m     num_boost_round=CONFIG[\u001b[33m\"\u001b[39m\u001b[33mXGB_NUM_BOOST_ROUND\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     88\u001b[39m     evals=evals,\n\u001b[32m     89\u001b[39m     evals_result=evals_result,\n\u001b[32m     90\u001b[39m     **{XGB_TRAIN_METRIC_KW: policy_cost_eval},\n\u001b[32m     91\u001b[39m     early_stopping_rounds=CONFIG[\u001b[33m\"\u001b[39m\u001b[33mXGB_EARLY_STOPPING\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     92\u001b[39m     maximize=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     93\u001b[39m     verbose_eval=\u001b[32m200\u001b[39m,\n\u001b[32m     94\u001b[39m )\n\u001b[32m     96\u001b[39m val_scores = np.array(evals_result.get(\u001b[33m\"\u001b[39m\u001b[33mval_es\u001b[39m\u001b[33m\"\u001b[39m, {}).get(\u001b[33m\"\u001b[39m\u001b[33mcap_cost\u001b[39m\u001b[33m\"\u001b[39m, []))\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(val_scores) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/py313/lib/python3.13/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/py313/lib/python3.13/site-packages/xgboost/training.py:184\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    182\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    183\u001b[39m     bst.update(dtrain, iteration=i, fobj=obj)\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    185\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    187\u001b[39m bst = cb_container.after_training(bst)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/py313/lib/python3.13/site-packages/xgboost/callback.py:264\u001b[39m, in \u001b[36mCallbackContainer.after_iteration\u001b[39m\u001b[34m(self, model, epoch, dtrain, evals)\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, name \u001b[38;5;129;01min\u001b[39;00m evals:\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m name.find(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m) == -\u001b[32m1\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDataset name should not contain `-`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m score: \u001b[38;5;28mstr\u001b[39m = model.eval_set(evals, epoch, \u001b[38;5;28mself\u001b[39m.metric, \u001b[38;5;28mself\u001b[39m._output_margin)\n\u001b[32m    265\u001b[39m metric_score = _parse_eval_str(score)\n\u001b[32m    266\u001b[39m \u001b[38;5;28mself\u001b[39m._update_history(metric_score, epoch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/py313/lib/python3.13/site-packages/xgboost/core.py:2367\u001b[39m, in \u001b[36mBooster.eval_set\u001b[39m\u001b[34m(self, evals, iteration, feval, output_margin)\u001b[39m\n\u001b[32m   2364\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m feval \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2365\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m dmat, evname \u001b[38;5;129;01min\u001b[39;00m evals:\n\u001b[32m   2366\u001b[39m         feval_ret = feval(\n\u001b[32m-> \u001b[39m\u001b[32m2367\u001b[39m             \u001b[38;5;28mself\u001b[39m.predict(dmat, training=\u001b[38;5;28;01mFalse\u001b[39;00m, output_margin=output_margin),\n\u001b[32m   2368\u001b[39m             dmat,\n\u001b[32m   2369\u001b[39m         )\n\u001b[32m   2370\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feval_ret, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m   2371\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m feval_ret:\n\u001b[32m   2372\u001b[39m                 \u001b[38;5;66;03m# pylint: disable=consider-using-f-string\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/py313/lib/python3.13/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/py313/lib/python3.13/site-packages/xgboost/core.py:2528\u001b[39m, in \u001b[36mBooster.predict\u001b[39m\u001b[34m(self, data, output_margin, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features, training, iteration_range, strict_shape)\u001b[39m\n\u001b[32m   2525\u001b[39m shape = ctypes.POINTER(c_bst_ulong)()\n\u001b[32m   2526\u001b[39m dims = c_bst_ulong()\n\u001b[32m   2527\u001b[39m _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2528\u001b[39m     _LIB.XGBoosterPredictFromDMatrix(\n\u001b[32m   2529\u001b[39m         \u001b[38;5;28mself\u001b[39m.handle,\n\u001b[32m   2530\u001b[39m         data.handle,\n\u001b[32m   2531\u001b[39m         from_pystr_to_cstr(json.dumps(args)),\n\u001b[32m   2532\u001b[39m         ctypes.byref(shape),\n\u001b[32m   2533\u001b[39m         ctypes.byref(dims),\n\u001b[32m   2534\u001b[39m         ctypes.byref(preds),\n\u001b[32m   2535\u001b[39m     )\n\u001b[32m   2536\u001b[39m )\n\u001b[32m   2537\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _prediction_output(shape, dims, preds, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "id": "bf880e4e28c454df",
   "metadata": {},
   "source": [
    "#### 7B.2 Isotonic calibration on validation set (primary)"
   ]
  },
  {
   "cell_type": "code",
   "id": "853de5f65a5d02ca",
   "metadata": {},
   "source": [
    "# Raw probabilities (uncalibrated) using the best iteration\n",
    "best_iter = int(getattr(xgb_model, \"best_iteration\", 0)) + 1\n",
    "p_val_cal_raw = xgb_model.predict(dval_cal, iteration_range=(0, best_iter))\n",
    "p_val_raw = xgb_model.predict(dval_full, iteration_range=(0, best_iter))\n",
    "p_te_raw  = xgb_model.predict(dtest, iteration_range=(0, best_iter))\n",
    "\n",
    "# Fit isotonic on validation calibration split only (primary calibration)\n",
    "iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "iso.fit(p_val_cal_raw, y_va_cal.values.astype(int))\n",
    "\n",
    "# Store raw + calibrated predictions\n",
    "for split_name, preds in [(\"val\", p_val_raw), (\"test\", p_te_raw), (\"train\", xgb_model.predict(dtrain, iteration_range=(0, best_iter)))]:\n",
    "    df_model.loc[df_model[\"split\"]==split_name, \"pd_tree_raw\"] = preds\n",
    "\n",
    "# Primary calibrated PDs (isotonic)\n",
    "df_model[\"pd_tree\"] = iso.transform(df_model[\"pd_tree_raw\"].values)\n",
    "\n",
    "print(\"Isotonic calibration fit on val_cal; pd_tree uses isotonic calibration.\")\n",
    "\n",
    "# --- PD granularity diagnostics (TEST only) ---\n",
    "mask_test = df_model[\"split\"] == \"test\"\n",
    "\n",
    "def pd_granularity(p: np.ndarray, name: str) -> dict:\n",
    "    s = pd.Series(p).dropna()\n",
    "    n_unique = int(s.nunique()) if len(s) else 0\n",
    "    top_share = float(s.value_counts(normalize=True).iloc[0]) if len(s) else np.nan\n",
    "    cutoff = float(np.quantile(s, 0.8)) if len(s) else np.nan\n",
    "    unique_top = int(s[s >= cutoff].nunique()) if len(s) else 0\n",
    "    return {\"series\": name, \"n_unique\": n_unique, \"top_value_share\": top_share, \"unique_top20pct\": unique_top}\n",
    "\n",
    "pd_diag_rows = []\n",
    "for name, col in [(\"logit_pd\", \"pd_logit\"), (\"tree_raw\", \"pd_tree_raw\"), (\"tree_calibrated\", \"pd_tree\")]:\n",
    "    pd_diag_rows.append(pd_granularity(df_model.loc[mask_test, col].values, name))\n",
    "\n",
    "pd_granularity_tbl = pd.DataFrame(pd_diag_rows)\n",
    "print(\"\\nPD granularity diagnostics (TEST):\")\n",
    "display(pd_granularity_tbl)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cda9431920ba2ddf",
   "metadata": {},
   "source": [
    "def calibration_slope_intercept(y_true: np.ndarray, p: np.ndarray) -> tuple[float,float]:\n",
    "    z = logit(p)\n",
    "    Xc = sm.add_constant(z, has_constant=\"add\")\n",
    "    mdl = sm.GLM(y_true, Xc, family=sm.families.Binomial())\n",
    "    res = mdl.fit()\n",
    "    intercept, slope = res.params[0], res.params[1]\n",
    "    return float(intercept), float(slope)\n",
    "\n",
    "# Policy-aligned test metrics (calibrated PDs)\n",
    "mask_test = df_model[\"split\"] == \"test\"\n",
    "y_test = df_model.loc[mask_test, TARGET_NAME].astype(int).values\n",
    "p_test = df_model.loc[mask_test, \"pd_tree\"].values\n",
    "\n",
    "# Ranking-based capacity policy\n",
    "n_test = len(p_test)\n",
    "k = int(math.ceil(CONFIG[\"CAPACITY_PCT\"] * n_test))\n",
    "order = np.argsort(p_test)\n",
    "idx_top = order[-k:]\n",
    "\n",
    "y_pred = np.zeros_like(y_test)\n",
    "y_pred[idx_top] = 1\n",
    "\n",
    "# Confusion components\n",
    "fp = np.sum((y_pred == 1) & (y_test == 0))\n",
    "fn = np.sum((y_pred == 0) & (y_test == 1))\n",
    "tp = np.sum((y_pred == 1) & (y_test == 1))\n",
    "\n",
    "precision_at_k = tp / max(tp + fp, 1)\n",
    "recall_at_k = tp / max(tp + fn, 1)\n",
    "cap_cost = CONFIG[\"COST_FN\"] * fn + CONFIG[\"COST_FP\"] * fp\n",
    "cap_cost_per_obs = cap_cost / max(n_test, 1)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, p_test)\n",
    "pr_auc = average_precision_score(y_test, p_test)\n",
    "brier = brier_score_loss(y_test, p_test)\n",
    "\n",
    "p_test_clip = np.clip(p_test, 1e-6, 1 - 1e-6)\n",
    "cal_intercept, cal_slope = calibration_slope_intercept(y_test, p_test_clip)\n",
    "\n",
    "print(\"Test metrics (tree_calibrated):\")\n",
    "print(f\"  ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"  PR-AUC: {pr_auc:.4f}\")\n",
    "print(f\"  Recall@{CONFIG['CAPACITY_PCT']:.0%}: {recall_at_k:.4f}\")\n",
    "print(f\"  Precision@{CONFIG['CAPACITY_PCT']:.0%}: {precision_at_k:.4f}\")\n",
    "print(f\"  Expected cost (capacity): {cap_cost:.2f} (per obs {cap_cost_per_obs:.6f})\")\n",
    "print(f\"  Brier score: {brier:.6f}\")\n",
    "print(f\"  Calibration intercept: {cal_intercept:.4f}, slope: {cal_slope:.4f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "834715d35ad09148",
   "metadata": {},
   "source": [
    "#### 7B.3 Feature importance and SHAP (optional explainability)"
   ]
  },
  {
   "cell_type": "code",
   "id": "bb3bc468e8bc933d",
   "metadata": {},
   "source": [
    "# Gain-based feature importance\n",
    "importance = xgb_model.get_score(importance_type=\"gain\")\n",
    "imp_tbl = (pd.DataFrame({\"feature\": list(importance.keys()), \"gain\": list(importance.values())})\n",
    "             .sort_values(\"gain\", ascending=False))\n",
    "display(imp_tbl.head(20))\n",
    "\n",
    "# Optional: SHAP summary for a subsample (can be expensive on large panels)\n",
    "try:\n",
    "    import shap\n",
    "    shap.initjs()\n",
    "    sample_n = min(5000, X_tr.shape[0])\n",
    "    X_sample = X_tr.sample(sample_n, random_state=SEED)\n",
    "    explainer = shap.TreeExplainer(xgb_model)\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values, X_sample, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(CONFIG[\"FIG_DIR\"]) / \"shap_summary_tree.png\", dpi=160)\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"SHAP skipped:\", e)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6173d1e8bb9b174f",
   "metadata": {},
   "source": [
    "#### 7A.5 Walk-forward validation (expanding window)\n",
    "\n",
    "This section refits preprocessing inside each fold using the raw modeling snapshot (before the global train-fit transforms). That keeps the walk-forward results free of preprocessing leakage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a29ae5979fd478ba",
   "metadata": {},
   "source": "\n# =============================================================================\n# Walk-forward (expanding window) validation \u2014 Logit and XGBoost (leakage-safe)\n# =============================================================================\n# This implementation refits preprocessing (median fill, winsor bounds, scaler) within each fold.\n# That is required for honest temporal validation.\n\ntrainpool_df = df_model_raw.loc[df_model_raw[\"split\"].isin([\"train\", \"val\"]), :].copy()\nyears = sorted(trainpool_df[\"label_year\"].dropna().unique().tolist())\nyears = [int(y) for y in years]\n\n\n# --- safe metrics helpers ---\ndef _safe_auc(y, p):\n    y = np.asarray(y, int)\n    p = np.asarray(p, float)\n    if len(np.unique(y)) < 2:\n        return np.nan\n    return roc_auc_score(y, p)\n\n\ndef _safe_pr(y, p):\n    y = np.asarray(y, int)\n    p = np.asarray(p, float)\n    if len(np.unique(y)) < 2:\n        return np.nan\n    return average_precision_score(y, p)\n\n\nN_SPLITS = 4\nif len(years) < (N_SPLITS + 2):\n    print(\"Not enough years for walk-forward validation; skipping.\")\n    wf_tbl = pd.DataFrame()\nelse:\n    split_idx = np.linspace(2, len(years) - 1, N_SPLITS, dtype=int)\n\n\n\n    # Ensure walk-forward uses the same best XGBoost parameters from Section 7B.1\n    wf_best_params = CONFIG.get(\"XGB_BEST_PARAMS\")\n    if wf_best_params is None:\n        wf_best_params = best_params.copy() if \"best_params\" in globals() and best_params is not None else None\n    if wf_best_params is None:\n        raise ValueError(\"Walk-forward requires best XGB params from Section 7B.1; run that section first.\")\n    if \"best_params\" in globals() and best_params is not None and wf_best_params != best_params:\n        print(\"Warning: CONFIG['XGB_BEST_PARAMS'] differs from best_params; using CONFIG version for walk-forward.\")\n\n    def prep_fold(df_tr, df_va):\n        # continuous: median fill (train), winsor clip (train), scaler (train)\n        cont = [c for c in continuous_feats_raw if c in df_tr.columns]\n        Xtr = df_tr[cont].apply(pd.to_numeric, errors=\"coerce\")\n        Xva = df_va[cont].apply(pd.to_numeric, errors=\"coerce\")\n\n        med = Xtr.median()\n        Xtr = Xtr.fillna(med)\n        Xva = Xva.fillna(med)\n\n        bounds = {c: winsorize_train_bounds(Xtr[c], CONFIG[\"WINSOR_LO_Q\"], CONFIG[\"WINSOR_HI_Q\"]) for c in cont}\n        for c, (lo, hi) in bounds.items():\n            Xtr[c] = apply_bounds(Xtr[c], lo, hi)\n            Xva[c] = apply_bounds(Xva[c], lo, hi)\n\n        scaler = StandardScaler()\n        Ztr = pd.DataFrame(scaler.fit_transform(Xtr), columns=[f\"z_{c}\" for c in cont], index=df_tr.index)\n        Zva = pd.DataFrame(scaler.transform(Xva), columns=[f\"z_{c}\" for c in cont], index=df_va.index)\n\n        # events: keep as-is; fill missing with 0 (absence of event)\n        ev = [c for c in event_feats if c in df_tr.columns]\n        if ev:\n            Etr = df_tr[ev].fillna(0.0)\n            Eva = df_va[ev].fillna(0.0)\n            Ztr = pd.concat([Ztr, Etr], axis=1)\n            Zva = pd.concat([Zva, Eva], axis=1)\n\n        return Ztr, Zva\n\n\n    rows = []\n    for k in split_idx:\n        train_years = years[:k]\n        val_year = years[k]\n\n        df_tr = trainpool_df[trainpool_df[\"label_year\"].isin(train_years)].copy()\n        df_va = trainpool_df[trainpool_df[\"label_year\"].isin([val_year])].copy()\n\n        # drop missing labels for the fold\n        df_tr = df_tr[df_tr[TARGET_NAME].notna()].copy()\n        df_va = df_va[df_va[TARGET_NAME].notna()].copy()\n\n        X_tr, X_va = prep_fold(df_tr, df_va)\n        y_tr = df_tr[TARGET_NAME].astype(int).values\n        y_va = df_va[TARGET_NAME].astype(int).values\n\n        # ---- Logit (match 7A.1/7A.2 best setup) ----\n        C_use = float(globals().get(\"best_C\", 1.0))\n        log_mdl = LogisticRegression(\n            penalty=\"l2\",\n            C=C_use,\n            solver=\"lbfgs\",\n            max_iter=3000,\n            random_state=SEED,\n        )\n        log_mdl.fit(X_tr, y_tr)\n        p_va_log = log_mdl.predict_proba(X_va)[:, 1]\n\n        rows.append({\n            \"model\": \"logit\",\n            \"train_years_min\": min(train_years),\n            \"train_years_max\": max(train_years),\n            \"val_year\": val_year,\n            \"n_train\": int(len(y_tr)),\n            \"n_val\": int(len(y_va)),\n            \"roc_auc\": _safe_auc(y_va, p_va_log),\n            \"pr_auc\": _safe_pr(y_va, p_va_log),\n            \"brier\": float(brier_score_loss(y_va, p_va_log)),\n        })\n\n        # ---- XGBoost (policy-aligned, per-fold fit) ----\n        # Ensure y are pandas Series (so .values works exactly like your main cell)\n        y_tr = df_tr[TARGET_NAME].astype(int)\n        y_va = df_va[TARGET_NAME].astype(int)\n\n        # If a fold has only one class, XGBoost AUC/PR are undefined and training can be unstable.\n        if (y_tr.nunique() < 2) or (y_va.nunique() < 2):\n            rows.append({\n                \"model\": \"tree\",\n                \"train_years_min\": min(train_years),\n                \"train_years_max\": max(train_years),\n                \"val_year\": val_year,\n                \"n_train\": int(len(y_tr)),\n                \"n_val\": int(len(y_va)),\n                \"roc_auc\": np.nan,\n                \"pr_auc\": np.nan,\n                \"brier\": np.nan,\n                \"best_iteration\": np.nan,\n                \"note\": \"Skipped: single-class fold\"\n            })\n        else:\n            # --- decoupled weights + scale_pos_weight ---\n            n_pos = int(y_tr.sum())\n            n_neg = int((y_tr == 0).sum())\n            imbalance = (n_neg / max(n_pos, 1))\n\n            w_pos = CONFIG[\"COST_FN\"]\n            w_neg = CONFIG[\"COST_FP\"]\n\n            w_tr = np.where(y_tr.values == 1, w_pos, w_neg).astype(float)\n            w_va = np.where(y_va.values == 1, w_pos, w_neg).astype(float)\n\n            # --- DMatrix pattern with early-stopping split ---\n            X_va_es, X_va_cal, y_va_es, y_va_cal = train_test_split(\n                X_va, y_va, test_size=0.5, random_state=SEED, stratify=y_va\n            )\n            w_va_es = np.where(y_va_es.values == 1, w_pos, w_neg).astype(float)\n            w_va_cal = np.where(y_va_cal.values == 1, w_pos, w_neg).astype(float)\n\n            dtrain = xgb.DMatrix(X_tr, label=y_tr, weight=w_tr, feature_names=X_tr.columns.tolist())\n            dval_es = xgb.DMatrix(X_va_es, label=y_va_es, weight=w_va_es, feature_names=X_tr.columns.tolist())\n            dval_cal = xgb.DMatrix(X_va_cal, label=y_va_cal, weight=w_va_cal, feature_names=X_tr.columns.tolist())\n\n            def _policy_cost_eval(preds, dtrain):\n                y_true = dtrain.get_label().astype(int)\n                n = len(preds)\n                k = int(math.ceil(CONFIG[\"CAPACITY_PCT\"] * n))\n                if k <= 0:\n                    return \"cap_cost\", 0.0\n                top_idx = np.argsort(preds)[-k:]\n                y_pred = np.zeros_like(y_true)\n                y_pred[top_idx] = 1\n                fp = np.sum((y_pred == 1) & (y_true == 0))\n                fn = np.sum((y_pred == 0) & (y_true == 1))\n                cost = CONFIG[\"COST_FN\"] * fn + CONFIG[\"COST_FP\"] * fp\n                return \"cap_cost\", float(cost) / float(n)\n\n            xgb_params = wf_best_params.copy()\n            xgb_params[\"scale_pos_weight\"] = float(imbalance)\n\n            evals = [(dtrain, \"train\"), (dval_es, \"val_es\")]\n\n            xgb_model = xgb.train(\n                params=xgb_params,\n                dtrain=dtrain,\n                num_boost_round=CONFIG[\"XGB_NUM_BOOST_ROUND\"],\n                evals=evals,\n                **{XGB_TRAIN_METRIC_KW: _policy_cost_eval},\n                early_stopping_rounds=CONFIG[\"XGB_EARLY_STOPPING\"],\n                maximize=False,\n                verbose_eval=False,\n            )\n\n            # Raw probabilities (uncalibrated) \u2014 evaluate on val_cal split\n            best_iter = int(getattr(xgb_model, \"best_iteration\", 0)) + 1\n            p_val_raw = xgb_model.predict(dval_cal, iteration_range=(0, best_iter))\n\n            rows.append({\n                \"model\": \"tree\",\n                \"train_years_min\": min(train_years),\n                \"train_years_max\": max(train_years),\n                \"val_year\": val_year,\n                \"n_train\": int(len(y_tr)),\n                \"n_val\": int(len(y_va_cal)),\n                \"roc_auc\": _safe_auc(y_va_cal.values, p_val_raw),\n                \"pr_auc\": _safe_pr(y_va_cal.values, p_val_raw),\n                \"brier\": float(brier_score_loss(y_va_cal.values.astype(int), p_val_raw)),\n                \"best_iteration\": int(getattr(xgb_model, \"best_iteration\", np.nan)),\n            })\n\n    wf_tbl = pd.DataFrame(rows)\n    display(wf_tbl)\n\n\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1ffaf2ac",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation & Diagnostic Monitoring\n",
    "\n",
    "All evaluation in this section treats the **test split as untouchable**: no tuning based on test results.\n",
    "\n",
    "**Clean Evaluation Protocol Note:**\n",
    "- For **Logit**: The validation (`val`) performance reported below is an honest out-of-sample estimate because it uses a model trained on `train` only. The test performance uses a model trained on `train+val`.\n",
    "- For **Tree (XGBoost)**: The validation split is now partitioned into `val_es` (early stopping) and `val_cal` (isotonic calibration). Reported `val` results reflect this split and are less biased than before, but **test** remains the only fully unbiased estimate.\n",
    "- **Unbiased Performance**: The **test split** results are the only strictly unbiased final performance metrics.\n",
    "\n",
    "We report:\n",
    "- ROC-AUC, PR-AUC, Brier score,\n",
    "- calibration curve and calibration slope (reliability),\n",
    "- persistence benchmark,\n",
    "- collinearity and drift diagnostics.\n",
    "\n",
    "### 8.1 Out-of-sample metrics (val and test) + persistence benchmark"
   ]
  },
  {
   "cell_type": "code",
   "id": "29873ad3",
   "metadata": {},
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "\n",
    "def eval_metrics_safe(y_true: pd.Series, p: np.ndarray) -> dict:\n",
    "    y = y_true.astype(int).values\n",
    "    out = {\n",
    "        \"roc_auc\": np.nan,\n",
    "        \"pr_auc\": np.nan,\n",
    "        \"brier\": brier_score_loss(y, p),\n",
    "        \"mean_p\": float(np.mean(p)),\n",
    "        \"event_rate\": float(np.mean(y)),\n",
    "        \"n\": int(len(y)),\n",
    "    }\n",
    "    # Guard against single-class slices (can occur under transition objective + small test sets)\n",
    "    if np.unique(y).size < 2:\n",
    "        # ROC-AUC undefined; PR-AUC equals event rate for a constant predictor\n",
    "        out[\"pr_auc\"] = float(out[\"event_rate\"])\n",
    "        return out\n",
    "    out[\"roc_auc\"] = float(roc_auc_score(y, p))\n",
    "    out[\"pr_auc\"] = float(average_precision_score(y, p))\n",
    "    return out\n",
    "\n",
    "rows = []\n",
    "for sp in [\"val\",\"test\"]:\n",
    "    mask = df_model[\"split\"] == sp\n",
    "    y_sp = df_model.loc[mask, TARGET_NAME]\n",
    "\n",
    "    rows.append({\"split\": sp, \"model\":\"logit\", **eval_metrics_safe(y_sp, df_model.loc[mask, \"pd_logit\"].values)})\n",
    "    rows.append({\"split\": sp, \"model\":\"tree_calibrated\", **eval_metrics_safe(y_sp, df_model.loc[mask, \"pd_tree\"].values)})\n",
    "\n",
    "    obj = str(globals().get(\"OBJECTIVE\", \"state\")).lower()\n",
    "    if obj.startswith(\"trans\"):\n",
    "        # Under early-warning (transition) objective, PROXY_NAME==0 by construction. Use a simple \"always-0\" baseline.\n",
    "        base = np.zeros(mask.sum(), dtype=float)\n",
    "        rows.append({\"split\": sp, \"model\":\"always_0\", **eval_metrics_safe(y_sp, base)})\n",
    "    else:\n",
    "        # Surveillance objective baseline: predict next-year distress = current-year state (persistence)\n",
    "        pers = pd.to_numeric(df_model.loc[mask, PROXY_NAME], errors=\"coerce\").fillna(0).astype(int).values\n",
    "        rows.append({\"split\": sp, \"model\":\"persistence_state_t\", **eval_metrics_safe(y_sp, pers)})\n",
    "\n",
    "metrics_tbl = pd.DataFrame(rows).sort_values([\"split\",\"model\"])\n",
    "display(metrics_tbl)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "51c21d935f978c85",
   "metadata": {},
   "source": [
    "### 8.1b Early-warning vs Surveillance decomposition (state-conditional evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ff41db2c85a96e38",
   "metadata": {},
   "source": [
    "# Objective-aware evaluation breakdown.\n",
    "# - If OBJECTIVE == \"transition\": df_model already represents the *risk set* (distress_t==0), so we report overall metrics.\n",
    "# - If OBJECTIVE == \"state\": we additionally break out performance on distress_t==0 (early-warning slice) vs distress_t==1 (surveillance slice).\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "\n",
    "def safe_eval_metrics(y_true: pd.Series, p: np.ndarray) -> dict:\n",
    "    y = y_true.astype(int).values\n",
    "    out = {\n",
    "        \"roc_auc\": np.nan,\n",
    "        \"pr_auc\": np.nan,\n",
    "        \"brier\": brier_score_loss(y, p),\n",
    "        \"mean_p\": float(np.mean(p)),\n",
    "        \"event_rate\": float(np.mean(y)),\n",
    "        \"n\": int(len(y)),\n",
    "    }\n",
    "    if np.unique(y).size < 2:\n",
    "        out[\"pr_auc\"] = float(out[\"event_rate\"])\n",
    "        return out\n",
    "    out[\"roc_auc\"] = float(roc_auc_score(y, p))\n",
    "    out[\"pr_auc\"] = float(average_precision_score(y, p))\n",
    "    return out\n",
    "\n",
    "def eval_models(df_seg: pd.DataFrame, split_name: str, segment_name: str) -> list:\n",
    "    rows = []\n",
    "    if df_seg.empty:\n",
    "        return rows\n",
    "    y = df_seg[TARGET_NAME]\n",
    "\n",
    "    for col, mdl in [(\"pd_logit\",\"logit\"),\n",
    "                     (\"pd_tree\",\"tree_calibrated\")]:\n",
    "        rows.append({\n",
    "            \"split\": split_name,\n",
    "            \"segment\": segment_name,\n",
    "            \"model\": mdl,\n",
    "            **safe_eval_metrics(y, df_seg[col].values),\n",
    "        })\n",
    "\n",
    "    # Baseline(s)\n",
    "    obj = str(globals().get(\"OBJECTIVE\", \"state\")).lower()\n",
    "    if obj.startswith(\"trans\"):\n",
    "        base = np.zeros(df_seg.shape[0], dtype=float)\n",
    "        rows.append({\n",
    "            \"split\": split_name,\n",
    "            \"segment\": segment_name,\n",
    "            \"model\": \"always_0\",\n",
    "            **safe_eval_metrics(y, base),\n",
    "        })\n",
    "    else:\n",
    "        state = pd.to_numeric(df_seg[PROXY_NAME], errors=\"coerce\").fillna(0).astype(int).values\n",
    "        rows.append({\n",
    "            \"split\": split_name,\n",
    "            \"segment\": segment_name,\n",
    "            \"model\": \"state_only\",\n",
    "            **safe_eval_metrics(y, state),\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "obj = str(globals().get(\"OBJECTIVE\", \"state\")).lower()\n",
    "\n",
    "seg_rows = []\n",
    "for sp in [\"val\", \"test\"]:\n",
    "    df_sp = df_model.loc[df_model[\"split\"]==sp, :].copy()\n",
    "\n",
    "    if obj.startswith(\"trans\"):\n",
    "        seg_rows += eval_models(df_sp, sp, f\"transition risk set ({PROXY_NAME}=0 by design)\")\n",
    "    else:\n",
    "        # Early-warning vs surveillance slices (conditional on current state)\n",
    "        dcur = pd.to_numeric(df_sp[PROXY_NAME], errors=\"coerce\")\n",
    "        df_sp = df_sp.loc[dcur.notna(), :].copy()\n",
    "        df_sp[\"distress_t_int\"] = dcur.loc[dcur.notna()].astype(int)\n",
    "\n",
    "        seg_rows += eval_models(df_sp.loc[df_sp[\"distress_t_int\"]==0, :], sp, f\"early_warning ({PROXY_NAME}=0)\")\n",
    "        seg_rows += eval_models(df_sp.loc[df_sp[\"distress_t_int\"]==1, :], sp, f\"surveillance ({PROXY_NAME}=1)\")\n",
    "\n",
    "seg_metrics_tbl = pd.DataFrame(seg_rows)\n",
    "if not seg_metrics_tbl.empty:\n",
    "    seg_metrics_tbl = seg_metrics_tbl.sort_values([\"split\",\"segment\",\"model\"])\n",
    "    display(seg_metrics_tbl)\n",
    "else:\n",
    "    print(\"No segment metrics computed (empty segments).\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "14804efc",
   "metadata": {},
   "source": [
    "### 8.2 Calibration diagnostics (curve + calibration-in-the-large + slope)"
   ]
  },
  {
   "cell_type": "code",
   "id": "ac2c963a",
   "metadata": {},
   "source": [
    "def calibration_slope_intercept(y_true: np.ndarray, p: np.ndarray) -> tuple[float,float]:\n",
    "    z = logit(p)\n",
    "    Xc = sm.add_constant(z, has_constant=\"add\")\n",
    "    mdl = sm.GLM(y_true, Xc, family=sm.families.Binomial())\n",
    "    res = mdl.fit()\n",
    "    intercept, slope = res.params[0], res.params[1]\n",
    "    return float(intercept), float(slope)\n",
    "\n",
    "def plot_calibration(y_true: np.ndarray, p: np.ndarray, title: str, fname: str):\n",
    "    frac_pos, mean_pred = calibration_curve(y_true, p, n_bins=10, strategy=\"quantile\")\n",
    "    plt.figure()\n",
    "    plt.plot(mean_pred, frac_pos, marker=\"o\")\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "    plt.xlabel(\"Mean predicted probability\")\n",
    "    plt.ylabel(\"Fraction of positives\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(CONFIG[\"FIG_DIR\"]) / fname, dpi=160)\n",
    "    plt.show()\n",
    "\n",
    "for sp in [\"val\",\"test\"]:\n",
    "    mask = df_model[\"split\"] == sp\n",
    "    y_sp = df_model.loc[mask, TARGET_NAME].astype(int).values\n",
    "\n",
    "    for model_name, pcol in [(\"logit\",\"pd_logit\"), (\"tree_calibrated\",\"pd_tree\")]:\n",
    "        p = df_model.loc[mask, pcol].values\n",
    "        icpt, slope = calibration_slope_intercept(y_sp, p)\n",
    "        print(f\"{sp} | {model_name}: calibration intercept={icpt:.3f}, slope={slope:.3f}\")\n",
    "        plot_calibration(y_sp, p, f\"Calibration curve \u2014 {model_name} ({sp})\", f\"cal_curve_{model_name}_{sp}.png\")\n",
    "# --- Calibration-in-the-large by year (TEST) ---\n",
    "\n",
    "def calib_in_large_by_year(df_in: pd.DataFrame, pcol: str) -> pd.DataFrame:\n",
    "    d = df_in.loc[df_in[\"split\"]==\"test\", [\"label_year\", pcol, TARGET_NAME]].copy()\n",
    "    rows = []\n",
    "    for y, g in d.groupby(\"label_year\"):\n",
    "        mean_p = float(g[pcol].mean()) if len(g) else np.nan\n",
    "        event_rate = float(g[TARGET_NAME].mean()) if len(g) else np.nan\n",
    "        if np.isfinite(mean_p) and np.isfinite(event_rate):\n",
    "            adj_intercept = float(logit(event_rate) - logit(mean_p))\n",
    "        else:\n",
    "            adj_intercept = np.nan\n",
    "        rows.append({\"label_year\": int(y), \"mean_pd\": mean_p, \"realized_rate\": event_rate, \"intercept_only_adj\": adj_intercept, \"n\": int(len(g))})\n",
    "    return pd.DataFrame(rows).sort_values(\"label_year\")\n",
    "\n",
    "print(\"\\nCalibration-in-the-large by year (TEST):\")\n",
    "for model_name, pcol in [(\"logit\",\"pd_logit\"), (\"tree_calibrated\",\"pd_tree\")]:\n",
    "    print(f\"\\n{model_name}\")\n",
    "    display(calib_in_large_by_year(df_model, pcol))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dae2a433",
   "metadata": {},
   "source": [
    "### 8.3 Temporal stability (walk-forward fold metrics)"
   ]
  },
  {
   "cell_type": "code",
   "id": "e0c531ed",
   "metadata": {},
   "source": [
    "if 'wf_tbl' in globals() and len(wf_tbl) > 0:\n",
    "    display(wf_tbl)\n",
    "    plt.figure()\n",
    "    plt.plot(wf_tbl[\"val_year\"], wf_tbl[\"roc_auc\"], marker=\"o\", label=\"ROC-AUC\")\n",
    "    plt.plot(wf_tbl[\"val_year\"], wf_tbl[\"pr_auc\"], marker=\"o\", label=\"PR-AUC\")\n",
    "    plt.title(\"Walk-forward validation metrics \")\n",
    "    plt.xlabel(\"Validation label_year\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(CONFIG[\"FIG_DIR\"]) / \"walkforward_metrics_logit.png\", dpi=160)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1e114cc8",
   "metadata": {},
   "source": [
    "### 8.4 Collinearity checks (VIF + high-correlation pairs)"
   ]
  },
  {
   "cell_type": "code",
   "id": "a73fa727",
   "metadata": {},
   "source": [
    "# VIF on continuous z-features (train only)\n",
    "X_vif = splits[\"train\"][\"X\"][[f\"z_{c}\" for c in continuous_feats_raw]].copy()\n",
    "X_vif = sm.add_constant(X_vif, has_constant=\"add\")\n",
    "\n",
    "vif_rows = []\n",
    "for i, col in enumerate(X_vif.columns):\n",
    "    if col == \"const\":\n",
    "        continue\n",
    "    vif_rows.append({\"feature\": col, \"VIF\": float(variance_inflation_factor(X_vif.values, i))})\n",
    "\n",
    "vif_tbl = pd.DataFrame(vif_rows).sort_values(\"VIF\", ascending=False)\n",
    "display(vif_tbl)\n",
    "\n",
    "# Correlation screen (continuous only)\n",
    "corr = splits[\"train\"][\"X\"][[f\"z_{c}\" for c in continuous_feats_raw]].corr()\n",
    "high_pairs = []\n",
    "for i in range(len(corr.columns)):\n",
    "    for j in range(i+1, len(corr.columns)):\n",
    "        v = corr.iloc[i,j]\n",
    "        if abs(v) >= 0.85:\n",
    "            high_pairs.append((corr.columns[i], corr.columns[j], float(v)))\n",
    "high_pairs_tbl = pd.DataFrame(high_pairs, columns=[\"feat1\",\"feat2\",\"corr\"]).sort_values(\"corr\", key=np.abs, ascending=False)\n",
    "display(high_pairs_tbl)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5a8305ac",
   "metadata": {},
   "source": [
    "### 8.5 Drift diagnostics (standardized mean difference: train vs test)"
   ]
  },
  {
   "cell_type": "code",
   "id": "82250df7",
   "metadata": {},
   "source": [
    "feat_cols = [f\"z_{c}\" for c in continuous_feats_raw] + event_feats\n",
    "drift_rows = []\n",
    "for c in feat_cols:\n",
    "    smd = compute_smd(df_model.loc[df_model[\"split\"]==\"train\", c], df_model.loc[df_model[\"split\"]==\"test\", c])\n",
    "    drift_rows.append({\"feature\": c, \"SMD_train_vs_test\": smd})\n",
    "drift_tbl = pd.DataFrame(drift_rows).sort_values(\"SMD_train_vs_test\", key=lambda s: s.abs(), ascending=False)\n",
    "display(drift_tbl.head(25))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "03071f10",
   "metadata": {},
   "source": [
    "### 8.6 Probability distributions by class (test split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2644a37d748cd1e5",
   "metadata": {},
   "source": [
    "## 9.a Statistical uncertainty and model comparison on the test set (Stratified cluster bootstrap CIs + fold-based summaries)\n\nThis section adds (i) *stratified* cluster bootstrap by firm_id (stratified on distress presence) for 95% CIs on key metrics, and (ii) fold-based CI summaries from walk-forward validation for stability across time.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6516269832d9cc8e",
   "metadata": {},
   "source": [
    "import numpy as np\nimport pandas as pd\nfrom itertools import combinations\nfrom sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n\n# ============================================================\n# Firm-clustered uncertainty quantification (panel-robust)\n# ============================================================\n# Rationale:\n# - Firm-year observations are not i.i.d.; errors are correlated within firm over time.\n# - Row-wise (i.i.d.) bootstrap and DeLong tests are therefore overconfident in panels.\n# - We use a *cluster bootstrap by firm_id*: resample firms with replacement and keep\n#   each sampled firm's full time-series block.\n#\n# Outputs:\n# 1) Model-wise cluster-bootstrap percentile CIs for ROC-AUC, PR-AUC, and Brier score.\n# 2) Pairwise *paired* cluster-bootstrap tests for ROC-AUC differences (with Holm/Bonferroni).\n\n\nN_BOOT = 1000  # increase (e.g., 2000) for tighter CIs at higher compute cost\nALPHA = 0.05\nSEED_BOOT = 42\n\ndef _cluster_groups(df: pd.DataFrame, cluster_col: str):\n    cl = df[cluster_col].astype(str).values\n    uniq, inv = np.unique(cl, return_inverse=True)\n    groups = [np.where(inv == k)[0] for k in range(len(uniq))]\n    return groups\n\ndef _metric_safe(y: np.ndarray, p: np.ndarray, metric: str) -> float:\n    y = np.asarray(y).astype(int)\n    p = np.asarray(p).astype(float)\n    if metric == \"roc_auc\":\n        if np.unique(y).size < 2:\n            return np.nan\n        return float(roc_auc_score(y, p))\n    if metric == \"pr_auc\":\n        # Average precision requires positives to be meaningful; return NaN if no positives.\n        if y.sum() == 0 or y.sum() == len(y):\n            return np.nan\n        return float(average_precision_score(y, p))\n    if metric == \"brier\":\n        return float(brier_score_loss(y, p))\n    raise ValueError(f\"Unknown metric: {metric}\")\n\ndef cluster_bootstrap_ci(\n    df: pd.DataFrame,\n    y_col: str,\n    p_col: str,\n    metric: str,\n    cluster_col: str = \"firm_id\",\n    n_boot: int = 1000,\n    alpha: float = 0.05,\n    seed: int = 42,\n):\n    y = df[y_col].astype(int).values\n    p = df[p_col].astype(float).values\n\n    point = _metric_safe(y, p, metric)\n    groups = _cluster_groups(df, cluster_col)\n    G = len(groups)\n    if G == 0:\n        return point, np.nan, np.nan, 0\n\n    rng = np.random.default_rng(seed)\n    vals = []\n\n    for _ in range(n_boot):\n        # sample clusters with replacement (same number of clusters)\n        sampled = rng.integers(0, G, size=G)\n        idx = np.concatenate([groups[g] for g in sampled])\n        v = _metric_safe(y[idx], p[idx], metric)\n        if v == v:  # not NaN\n            vals.append(v)\n\n    if len(vals) == 0:\n        return point, np.nan, np.nan, 0\n\n    lo = float(np.quantile(vals, alpha/2))\n    hi = float(np.quantile(vals, 1 - alpha/2))\n    return point, lo, hi, len(vals)\n\ndef cluster_bootstrap_diff_test(\n    df: pd.DataFrame,\n    y_col: str,\n    p1_col: str,\n    p2_col: str,\n    metric: str = \"roc_auc\",\n    cluster_col: str = \"firm_id\",\n    n_boot: int = 1000,\n    alpha: float = 0.05,\n    seed: int = 42,\n):\n    y = df[y_col].astype(int).values\n    p1 = df[p1_col].astype(float).values\n    p2 = df[p2_col].astype(float).values\n\n    point = _metric_safe(y, p1, metric) - _metric_safe(y, p2, metric)\n\n    groups = _cluster_groups(df, cluster_col)\n    G = len(groups)\n    if G == 0:\n        return point, np.nan, np.nan, np.nan, 0\n\n    rng = np.random.default_rng(seed)\n    diffs = []\n\n    for _ in range(n_boot):\n        sampled = rng.integers(0, G, size=G)\n        idx = np.concatenate([groups[g] for g in sampled])\n        m1 = _metric_safe(y[idx], p1[idx], metric)\n        m2 = _metric_safe(y[idx], p2[idx], metric)\n        if (m1 == m1) and (m2 == m2):\n            diffs.append(m1 - m2)\n\n    if len(diffs) == 0:\n        return point, np.nan, np.nan, np.nan, 0\n\n    diffs = np.asarray(diffs, dtype=float)\n    lo = float(np.quantile(diffs, alpha/2))\n    hi = float(np.quantile(diffs, 1 - alpha/2))\n\n    # Two-sided p-value from the bootstrap sign distribution (paired)\n    p_le0 = float(np.mean(diffs <= 0))\n    p_ge0 = float(np.mean(diffs >= 0))\n    pval = 2 * min(p_le0, p_ge0)\n    pval = float(min(max(pval, 0.0), 1.0))\n\n    return point, lo, hi, pval, len(diffs)\n\n# -------------------------\n# Multiple-testing adjustment helpers\n# -------------------------\ndef p_adjust_bonferroni(pvals: np.ndarray) -> np.ndarray:\n    p = np.asarray(pvals, dtype=float)\n    return np.minimum(p * len(p), 1.0)\n\ndef p_adjust_holm(pvals: np.ndarray) -> np.ndarray:\n    p = np.asarray(pvals, dtype=float)\n    m = len(p)\n    order = np.argsort(p)\n    adj = np.empty(m, dtype=float)\n    for i, idx in enumerate(order):\n        adj[idx] = (m - i) * p[idx]\n    # enforce monotonicity in sorted order\n    adj_sorted = np.maximum.accumulate(adj[order])\n    adj[order] = np.minimum(adj_sorted, 1.0)\n    return adj\n\n# -------------------------\n# Stratified cluster bootstrap (by firm distress presence)\n# -------------------------\ndef _firm_strata(df: pd.DataFrame, cluster_col: str, y_col: str):\n    firm_flag = df.groupby(cluster_col)[y_col].max().astype(int)\n    pos_firms = firm_flag[firm_flag == 1].index.astype(str).tolist()\n    neg_firms = firm_flag[firm_flag == 0].index.astype(str).tolist()\n    return pos_firms, neg_firms\n\ndef cluster_bootstrap_ci_stratified(\n    df: pd.DataFrame,\n    y_col: str,\n    p_col: str,\n    metric: str,\n    cluster_col: str = \"firm_id\",\n    n_boot: int = 1000,\n    alpha: float = 0.05,\n    seed: int = 42,\n):\n    y = df[y_col].astype(int).values\n    p = df[p_col].astype(float).values\n\n    point = _metric_safe(y, p, metric)\n    pos_firms, neg_firms = _firm_strata(df, cluster_col, y_col)\n\n    if len(pos_firms) + len(neg_firms) == 0:\n        return point, np.nan, np.nan, 0\n\n    firm_to_idx = {\n        f: df.index[df[cluster_col].astype(str) == f].values\n        for f in (pos_firms + neg_firms)\n    }\n\n    rng = np.random.default_rng(seed)\n    vals = []\n\n    for _ in range(n_boot):\n        sampled_pos = rng.choice(pos_firms, size=len(pos_firms), replace=True) if pos_firms else []\n        sampled_neg = rng.choice(neg_firms, size=len(neg_firms), replace=True) if neg_firms else []\n        sampled_firms = list(sampled_pos) + list(sampled_neg)\n        if not sampled_firms:\n            continue\n        idx = np.concatenate([firm_to_idx[f] for f in sampled_firms])\n        v = _metric_safe(y[idx], p[idx], metric)\n        if v == v:\n            vals.append(v)\n\n    if len(vals) == 0:\n        return point, np.nan, np.nan, 0\n\n    lo = float(np.quantile(vals, alpha / 2))\n    hi = float(np.quantile(vals, 1 - alpha / 2))\n    return point, lo, hi, len(vals)\n\n# -------------------------\n# Fold-based CI summaries (walk-forward validation table)\n# -------------------------\ndef fold_ci_summary(wf_tbl: pd.DataFrame, metric: str, alpha: float = 0.05):\n    if wf_tbl.empty:\n        return pd.DataFrame()\n    rows = []\n    for model in wf_tbl[\"model\"].unique():\n        vals = wf_tbl.loc[wf_tbl[\"model\"] == model, metric].dropna().astype(float).values\n        n = len(vals)\n        if n == 0:\n            continue\n        mean = float(np.mean(vals))\n        std = float(np.std(vals, ddof=1)) if n > 1 else 0.0\n        if n > 1:\n            from scipy import stats\n            tcrit = float(stats.t.ppf(1 - alpha / 2, df=n - 1))\n            half = tcrit * std / np.sqrt(n)\n        else:\n            half = np.nan\n        rows.append({\n            \"model\": model,\n            \"metric\": metric,\n            \"fold_mean\": mean,\n            \"fold_ci_lo\": mean - half if half == half else np.nan,\n            \"fold_ci_hi\": mean + half if half == half else np.nan,\n            \"n_folds\": n,\n        })\n    return pd.DataFrame(rows)\n\n# -------------------------\n# Run on test set (df_model + TARGET_NAME)\n# -------------------------\nmask_te = df_model[\"split\"] == \"test\"\ndf_te = df_model.loc[mask_te, :].copy()\n\n# Collect probability columns available (extend easily)\ncandidate_cols = [\n    (\"logit\", \"pd_logit\"),\n    (\"tree_calibrated\", \"pd_tree\"),\n    (\"logit_fe_year\", \"pd_logit_fe_year\"),\n    (\"logit_fe_firm_year\", \"pd_logit_fe_firm_year\"),\n    (\"logit_firth\", \"pd_logit_firth\"),\n]\n\nmodels = {name: col for name, col in candidate_cols if col in df_te.columns}\n\nif len(models) == 0:\n    print(\"No model probability columns found in df_model (expected pd_logit / pd_tree).\")\nelse:\n    # --- Cluster bootstrap CIs for each model (and baseline if desired)\n    rows = []\n    for name, col in models.items():\n        for metric in [\"roc_auc\", \"pr_auc\", \"brier\"]:\n            pt, lo, hi, n_eff = cluster_bootstrap_ci(\n                df_te, TARGET_NAME, col, metric=metric, cluster_col=\"firm_id\", n_boot=N_BOOT, alpha=ALPHA, seed=SEED_BOOT\n            )\n            rows.append({\n                \"model\": name,\n                \"metric\": metric,\n                \"point\": pt,\n                \"ci_lo\": lo,\n                \"ci_hi\": hi,\n                \"boot_repl_used\": n_eff,\n            })\n\n    ci_tbl = pd.DataFrame(rows).sort_values([\"metric\", \"model\"])\n    ci_tbl[\"ci_method\"] = \"cluster_bootstrap\"\n    display(ci_tbl)\n\n    # --- Stratified (by firm distress presence) cluster-bootstrap CIs\n    rows_strat = []\n    for name, col in models.items():\n        for metric in [\"roc_auc\", \"pr_auc\", \"brier\"]:\n            pt, lo, hi, n_eff = cluster_bootstrap_ci_stratified(\n                df_te, TARGET_NAME, col, metric=metric, cluster_col=\"firm_id\", n_boot=N_BOOT, alpha=ALPHA, seed=SEED_BOOT\n            )\n            rows_strat.append({\n                \"model\": name,\n                \"metric\": metric,\n                \"point\": pt,\n                \"ci_lo\": lo,\n                \"ci_hi\": hi,\n                \"boot_repl_used\": n_eff,\n                \"ci_method\": \"cluster_bootstrap_stratified\",\n            })\n    ci_tbl_strat = pd.DataFrame(rows_strat).sort_values([\"metric\", \"model\"])\n    display(ci_tbl_strat)\n\n    # --- Fold-based CI summaries from walk-forward validation (if available)\n    if 'wf_tbl' in globals() and isinstance(wf_tbl, pd.DataFrame) and not wf_tbl.empty:\n        for metric in [\"roc_auc\", \"pr_auc\", \"brier\"]:\n            fold_tbl = fold_ci_summary(wf_tbl, metric=metric, alpha=ALPHA)\n            if not fold_tbl.empty:\n                display(fold_tbl.sort_values([\"metric\", \"model\"]))\n\n\n    # --- Pairwise paired cluster-bootstrap tests (ROC-AUC diff)\n    if len(models) < 2:\n        print(\"Skipping pairwise model comparisons: need at least two model probability columns.\")\n    else:\n        comp_rows = []\n        for (n1, c1), (n2, c2) in combinations(models.items(), 2):\n            diff_pt, diff_lo, diff_hi, pval, n_eff = cluster_bootstrap_diff_test(\n                df_te, TARGET_NAME, c1, c2, metric=\"roc_auc\", cluster_col=\"firm_id\", n_boot=N_BOOT, alpha=ALPHA, seed=123\n            )\n            comp_rows.append({\n                \"model_1\": n1,\n                \"model_2\": n2,\n                \"roc_auc_diff_(1-2)\": diff_pt,\n                \"diff_ci_lo\": diff_lo,\n                \"diff_ci_hi\": diff_hi,\n                \"p_value\": pval,\n                \"boot_repl_used\": n_eff,\n            })\n\n        comp_tbl = pd.DataFrame(comp_rows)\n        if not comp_tbl.empty:\n            comp_tbl[\"p_bonferroni\"] = p_adjust_bonferroni(comp_tbl[\"p_value\"].values)\n            comp_tbl[\"p_holm\"] = p_adjust_holm(comp_tbl[\"p_value\"].values)\n            display(comp_tbl.sort_values(\"p_value\"))\n        else:\n            print(\"No pairwise comparisons computed.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ce4a5617",
   "metadata": {},
   "source": [
    "mask = df_model[\"split\"]==\"test\"\n",
    "y_true = df_model.loc[mask, TARGET_NAME].astype(int)\n",
    "\n",
    "for model_name, pcol in [(\"logit\",\"pd_logit\"), (\"tree_calibrated\",\"pd_tree\")]:\n",
    "    p = df_model.loc[mask, pcol]\n",
    "    plt.figure()\n",
    "    plt.hist(p[y_true==0], bins=50, alpha=0.6, label=\"y=0\")\n",
    "    plt.hist(p[y_true==1], bins=50, alpha=0.6, label=\"y=1\")\n",
    "    plt.title(f\"Test PD histogram by class \u2014 {model_name}\")\n",
    "    plt.xlabel(\"Predicted PD\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(CONFIG[\"FIG_DIR\"]) / f\"pd_hist_{model_name}_test.png\", dpi=160)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "50013862",
   "metadata": {},
   "source": [
    "## 9b. Operational Risk Management Layer (Events + PDs)\n",
    "This section uses a **two-layer** design:\n",
    "\n",
    "- **Model layer (prediction):** calibrated probability of next-year distress (**PD**) from accounting ratios and structural predictors.\n",
    "- **Indicator layer (events):** discrete `evt_*` early-warning indicators **not used as predictors**. They serve governance, monitoring, and decision support.\n",
    "\n",
    "This structure matches four operational functions commonly discussed in risk-management systems:\n",
    "\n",
    "1. **Risk awareness:** documented prior knowledge of which indicators flag trouble (event dictionary + empirical lift).\n",
    "2. **Monitoring and warning:** continuous tracking of event activation/persistence and PD levels over the panel.\n",
    "3. **Communication:** translating signals into decision-maker-friendly views (risk tiers/deciles, transitions, reason codes).\n",
    "4. **Response capability:** predefined action rules (screen / monitor / no action) based on PDs and events under explicit costs and capacity constraints.\n",
    "\n",
    "### 9.1 Risk awareness \u2014 event dictionary and conditional risk (lift)"
   ]
  },
  {
   "cell_type": "code",
   "id": "94c3906b",
   "metadata": {},
   "source": [
    "EVT_COLS = event_dict[\"event\"].tolist()\n",
    "print(\"Decision-support events:\", EVT_COLS)\n",
    "\n",
    "# Use post-imputation snapshot (pre-scaling) for event diagnostics.\n",
    "# Dividend events still require adjacent-year observations, so some missingness may remain.\n",
    "df_events = df_model_raw.copy() if \"df_model_raw\" in globals() else df_model.copy()\n",
    "\n",
    "# Optional: enrich the event dictionary with a simple mechanism taxonomy (appendix-ready)\n",
    "event_dict_enriched = event_dict.copy()\n",
    "mech_map = {\n",
    "    \"evt_divcut\": \"Payout policy\",\n",
    "    \"evt_divsusp\": \"Payout policy\",\n",
    "    \"evt_divinit\": \"Payout policy\",\n",
    "    \"evt_liq_squeeze\": \"Liquidity\",\n",
    "    \"evt_quick_squeeze\": \"Liquidity\",\n",
    "    \"evt_ebitdadrop\": \"Operating deterioration\",\n",
    "    \"evt_cfdrop\": \"Operating deterioration\",\n",
    "}\n",
    "event_dict_enriched[\"mechanism\"] = event_dict_enriched[\"event\"].map(mech_map).fillna(\"Other/unspecified\")\n",
    "display(event_dict_enriched)\n",
    "\n",
    "def event_lift_table(df_in: pd.DataFrame, events: list[str], y_col: str) -> pd.DataFrame:\n",
    "    # Event lift with explicit missingness handling.\n",
    "    # - prevalence_obs: among observations where the event is observed (not NA)\n",
    "    # - missing_rate: definitional missingness (insufficient inputs)\n",
    "    # - cond_distress_rate: P(y=1 | evt=1, evt observed)\n",
    "\n",
    "    base = df_in[y_col].astype(float).mean()\n",
    "    rows = []\n",
    "    for e in events:\n",
    "        if e not in df_in.columns:\n",
    "            continue\n",
    "\n",
    "        s = pd.to_numeric(df_in[e], errors=\"coerce\")  # may contain NA by construction\n",
    "        miss = float(s.isna().mean())\n",
    "        obs = s.notna()\n",
    "        if obs.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        s_obs = s[obs].astype(int)\n",
    "        n_event = int((s_obs == 1).sum())\n",
    "        if n_event == 0:\n",
    "            continue\n",
    "\n",
    "        rate = df_in.loc[obs & (s == 1), y_col].astype(float).mean()\n",
    "        prev = float((s_obs == 1).mean())\n",
    "\n",
    "        rows.append({\n",
    "            \"event\": e,\n",
    "            \"mechanism\": mech_map.get(e, \"Other/unspecified\"),\n",
    "            \"n_obs_event\": int(obs.sum()),\n",
    "            \"n_event\": n_event,\n",
    "            \"missing_rate\": miss,\n",
    "            \"prevalence_obs\": prev,\n",
    "            \"cond_distress_rate\": float(rate),\n",
    "            \"base_rate\": float(base),\n",
    "            \"lift_vs_base\": float(rate/base) if base > 0 else np.nan,\n",
    "        })\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    if not out.empty:\n",
    "        out = out.sort_values([\"lift_vs_base\", \"n_event\"], ascending=[False, False])\n",
    "    return out\n",
    "\n",
    "for sp in [\"train\", \"test\"]:\n",
    "    df_sp = df_events.loc[df_events[\"split\"] == sp, :].copy()\n",
    "    print(f\"\\nEvent lift (labels: {TARGET_NAME}) \u2014 {sp}\")\n",
    "    display(event_lift_table(df_sp, EVT_COLS, TARGET_NAME).head(25))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "35162ffd",
   "metadata": {},
   "source": [
    "### 9.2 Monitoring and warning \u2014 event dynamics, persistence, and PD\u00d7event risk grids\n",
    "\n",
    "Monitoring should reflect (i) **activation** (0\u21921), (ii) **persistence** (1\u21921), and (iii) how event regimes interact with PDs.\n",
    "We treat events as *operational indicators* (not predictors) and monitor them jointly with calibrated PDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "aaa8fd1b",
   "metadata": {},
   "source": [
    "# --- 9.2A Event activation and persistence (adjacency-safe) ---\n",
    "def transition_stats(df_in: pd.DataFrame, event: str) -> dict:\n",
    "    # Robust transition stats that enforce year adjacency and handle NaNs.\n",
    "    s = pd.to_numeric(df_in[event], errors=\"coerce\")\n",
    "    s_l1 = lag(df_in, event, 1)\n",
    "\n",
    "    valid = s.notna() & s_l1.notna()\n",
    "    if valid.sum() == 0:\n",
    "        return {\"event\": event, \"activation_01_rate\": np.nan, \"persistence_11_rate\": np.nan, \"n_transitions\": 0}\n",
    "\n",
    "    s0 = s_l1[valid].astype(int)\n",
    "    s1 = s[valid].astype(int)\n",
    "\n",
    "    act_01 = ((s0 == 0) & (s1 == 1)).mean()\n",
    "    pers_11 = ((s0 == 1) & (s1 == 1)).mean()\n",
    "    return {\n",
    "        \"event\": event,\n",
    "        \"activation_01_rate\": float(act_01),\n",
    "        \"persistence_11_rate\": float(pers_11),\n",
    "        \"n_transitions\": int(valid.sum()),\n",
    "    }\n",
    "\n",
    "rows = [transition_stats(df_model, e) for e in EVT_COLS]\n",
    "trans_tbl = pd.DataFrame(rows)\n",
    "if not trans_tbl.empty:\n",
    "    trans_tbl = trans_tbl.sort_values(\"activation_01_rate\", ascending=False)\n",
    "display(trans_tbl)\n",
    "\n",
    "# --- 9.2B Monitoring summary by fiscal year (panel-level tracking) ---\n",
    "def monitoring_by_year(df_in: pd.DataFrame, p_col: str, y_col: str, evt_cols: list[str]) -> pd.DataFrame:\n",
    "    d = df_in[[\"fyear\", \"split\", p_col, y_col] + evt_cols].copy()\n",
    "\n",
    "    # Event aggregation: triggered count; missingness summarized separately.\n",
    "    evt_mat = d[evt_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    d[\"evt_missing_rate_mean\"] = evt_mat.isna().mean(axis=1)\n",
    "    d[\"evt_count\"] = (evt_mat.fillna(0) == 1).sum(axis=1)\n",
    "    d[\"evt_any\"] = (d[\"evt_count\"] > 0).astype(int)\n",
    "\n",
    "    out = (d.groupby([\"split\", \"fyear\"])\n",
    "             .agg(\n",
    "                 n=(\"fyear\",\"size\"),\n",
    "                 mean_pd=(p_col,\"mean\"),\n",
    "                 realized_rate=(y_col,\"mean\"),\n",
    "                 evt_any_rate=(\"evt_any\",\"mean\"),\n",
    "                 mean_evt_count=(\"evt_count\",\"mean\"),\n",
    "                 mean_evt_missing=(\"evt_missing_rate_mean\",\"mean\"),\n",
    "             )\n",
    "             .reset_index()\n",
    "             .sort_values([\"split\",\"fyear\"]))\n",
    "    return out\n",
    "\n",
    "print(\"\\nMonitoring by year \u2014 calibrated tree PD (pd_tree)\")\n",
    "display(monitoring_by_year(df_model, \"pd_tree\", TARGET_NAME, EVT_COLS).head(40))\n",
    "\n",
    "# --- 9.2C PD \u00d7 Event risk grid (operational triangulation) ---\n",
    "def pd_event_grid(df_in: pd.DataFrame, p_col: str, y_col: str, evt_cols: list[str], n_bins: int = 10) -> pd.DataFrame:\n",
    "    d = df_in[[p_col, y_col] + evt_cols].dropna(subset=[p_col, y_col]).copy()\n",
    "    evt_mat = d[evt_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    d[\"evt_count\"] = (evt_mat.fillna(0) == 1).sum(axis=1)\n",
    "\n",
    "    d[\"evt_bucket\"] = pd.cut(d[\"evt_count\"], bins=[-0.1, 0.5, 1.5, 10**6], labels=[\"0\", \"1\", \"2+\"])\n",
    "    d[\"pd_decile\"] = pd.qcut(d[p_col], q=n_bins, labels=False, duplicates=\"drop\") + 1\n",
    "\n",
    "    g = (d.groupby([\"pd_decile\",\"evt_bucket\"])\n",
    "           .agg(\n",
    "               n=(\"pd_decile\",\"size\"),\n",
    "               mean_pd=(p_col,\"mean\"),\n",
    "               realized_rate=(y_col,\"mean\"),\n",
    "           )\n",
    "           .reset_index())\n",
    "    return g.sort_values([\"pd_decile\",\"evt_bucket\"])\n",
    "\n",
    "print(\"\\nTest split PD \u00d7 Events grid \u2014 calibrated tree\")\n",
    "display(pd_event_grid(df_model.loc[df_model[\"split\"]==\"test\", :], \"pd_tree\", TARGET_NAME, EVT_COLS, n_bins=10))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "89c18d2c",
   "metadata": {},
   "source": [
    "### 9.3 Communication \u2014 risk tiers, transitions, and reason codes\n",
    "\n",
    "Communication should translate model outputs and indicator triggers into decision-maker-friendly artifacts:\n",
    "\n",
    "- **Risk tiers/deciles:** expected vs realized risk by PD bucket.\n",
    "- **Transitions:** PD movements and event activations/persistence.\n",
    "- **Reason codes:** simple, interpretable attributions for material PD jumps (based on newly triggered events).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "48e1cb6a",
   "metadata": {},
   "source": [
    "def decile_table(df_in: pd.DataFrame, p_col: str, y_col: str) -> pd.DataFrame:\n",
    "    d = df_in[[p_col, y_col]].dropna().copy()\n",
    "    d[\"decile\"] = pd.qcut(d[p_col], 10, labels=False, duplicates=\"drop\") + 1\n",
    "    out = d.groupby(\"decile\").agg(\n",
    "        n=(\"decile\",\"size\"),\n",
    "        mean_pd=(p_col,\"mean\"),\n",
    "        realized_rate=(y_col,\"mean\"),\n",
    "    ).reset_index()\n",
    "    out[\"calibration_gap\"] = out[\"realized_rate\"] - out[\"mean_pd\"]\n",
    "    return out\n",
    "\n",
    "for model_name, pcol in [(\"logit\",\"pd_logit\"), (\"tree_calibrated\",\"pd_tree\")]:\n",
    "    print(f\"\\nTest deciles \u2014 {model_name}\")\n",
    "    dt = decile_table(df_model.loc[df_model[\"split\"]==\"test\", :], pcol, TARGET_NAME)\n",
    "    display(dt)\n",
    "    # --- 9.3A Reason codes for large PD jumps (event-based) ---\n",
    "# When PD decile increases materially year-over-year, summarize which events newly activated.\n",
    "\n",
    "def reason_codes_for_pd_jumps(df_in: pd.DataFrame, p_col: str, evt_cols: list[str], min_decile_jump: int = 3, split: str = \"test\") -> pd.DataFrame:\n",
    "    d = df_in.loc[df_in[\"split\"] == split, [\"firm_id\",\"fyear\", p_col] + evt_cols].copy()\n",
    "    d = d.sort_values([\"firm_id\",\"fyear\"])\n",
    "\n",
    "    # PD deciles within the split (communication tiering)\n",
    "    d[\"pd_decile\"] = pd.qcut(d[p_col], 10, labels=False, duplicates=\"drop\") + 1\n",
    "    d[\"pd_decile_l1\"] = lag(d, \"pd_decile\", 1)\n",
    "    d[\"decile_jump\"] = d[\"pd_decile\"] - d[\"pd_decile_l1\"]\n",
    "\n",
    "    jump_mask = d[\"decile_jump\"].notna() & (d[\"decile_jump\"] >= min_decile_jump)\n",
    "    if int(jump_mask.sum()) == 0:\n",
    "        return pd.DataFrame(columns=[\"event\",\"n_new_activation_in_jumps\",\"share_of_jumps\"])\n",
    "\n",
    "    n_jumps = int(jump_mask.sum())\n",
    "    rows = []\n",
    "    for e in evt_cols:\n",
    "        s = pd.to_numeric(d[e], errors=\"coerce\")\n",
    "        s_l1 = lag(d, e, 1)\n",
    "        valid = jump_mask & s.notna() & s_l1.notna()\n",
    "        if int(valid.sum()) == 0:\n",
    "            continue\n",
    "        new_act = int(((s_l1[valid].astype(int) == 0) & (s[valid].astype(int) == 1)).sum())\n",
    "        if new_act > 0:\n",
    "            rows.append({\n",
    "                \"event\": e,\n",
    "                \"n_new_activation_in_jumps\": new_act,\n",
    "                \"share_of_jumps\": float(new_act / n_jumps),\n",
    "            })\n",
    "\n",
    "    out = pd.DataFrame(rows).sort_values(\"n_new_activation_in_jumps\", ascending=False)\n",
    "    return out\n",
    "\n",
    "print(\"\\nReason codes \u2014 events newly activating during large PD jumps (test split)\")\n",
    "display(reason_codes_for_pd_jumps(df_model, \"pd_tree\", EVT_COLS, min_decile_jump=3, split=\"test\").head(20))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2fc0a52a",
   "metadata": {},
   "source": [
    "### 9.4 Response capability \u2014 predefined action rules under costs and capacity\n",
    "\n",
    "We translate PDs and `evt_*` indicators into an operational policy with three actions:\n",
    "\n",
    "- **Screen / Review** (capacity-limited): highest-risk firms warrant immediate attention.\n",
    "- **Monitor more closely**: elevated risk, but not high enough for immediate screening.\n",
    "- **No action**: routine monitoring only.\n",
    "\n",
    "We compare:\n",
    "- **PD-only policy** (threshold on PD),\n",
    "- **Hybrid policy** (PD + event burden) that can prioritize \u201cindicator-led\u201d cases without retraining the model."
   ]
  },
  {
   "cell_type": "code",
   "id": "66ca9b6f",
   "metadata": {},
   "source": [
    "COST_FN = float(CONFIG[\"COST_FN\"])\n",
    "COST_FP = float(CONFIG[\"COST_FP\"])\n",
    "CAPACITY_PCT = float(CONFIG[\"CAPACITY_PCT\"])\n",
    "MONITOR_PCT = float(CONFIG.get(\"MONITOR_PCT\", min(0.20, 2*CAPACITY_PCT)))  # fallback: monitor top 20% or 2x capacity\n",
    "\n",
    "ALPHA_EVT = 0.05\n",
    "BETA_EVT = 0.10\n",
    "\n",
    "\n",
    "def expected_cost(y_true: np.ndarray, y_hat: np.ndarray) -> float:\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_hat).ravel()\n",
    "    return COST_FN*fn + COST_FP*fp\n",
    "\n",
    "\n",
    "def confusion_counts(y_true: np.ndarray, y_hat: np.ndarray) -> dict:\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_hat).ravel()\n",
    "    return {\"tp\": int(tp), \"fp\": int(fp), \"tn\": int(tn), \"fn\": int(fn)}\n",
    "\n",
    "\n",
    "def apply_pd_only_policy(p: np.ndarray, thr_screen: float, thr_monitor: float) -> dict:\n",
    "    screen = (p >= thr_screen).astype(int)\n",
    "    monitor = ((p >= thr_monitor) & (p < thr_screen)).astype(int)\n",
    "    return {\"screen\": screen, \"monitor\": monitor}\n",
    "\n",
    "\n",
    "def hybrid_score(p: np.ndarray, evt_count: np.ndarray, alpha: float = ALPHA_EVT, beta: float = BETA_EVT) -> np.ndarray:\n",
    "    evt_any = (evt_count > 0).astype(int)\n",
    "    return p + alpha*evt_any + beta*(evt_count >= 2).astype(int)\n",
    "\n",
    "\n",
    "def apply_hybrid_policy_from_score(score: np.ndarray, thr_screen: float, thr_monitor: float) -> dict:\n",
    "    screen = (score >= thr_screen).astype(int)\n",
    "    monitor = ((score >= thr_monitor) & (score < thr_screen)).astype(int)\n",
    "    return {\"screen\": screen, \"monitor\": monitor}\n",
    "\n",
    "\n",
    "def build_evt_count(df_in: pd.DataFrame, evt_cols: list[str]) -> np.ndarray:\n",
    "    evt_mat = df_in[evt_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    return (evt_mat.fillna(0) == 1).sum(axis=1).values\n",
    "\n",
    "# --- Threshold selection (validation only) for PD-only and hybrid policies ---\n",
    "grid = np.linspace(0.01, 0.99, 99)\n",
    "mask_val = df_model[\"split\"] == \"val\"\n",
    "y_val = df_model.loc[mask_val, TARGET_NAME].astype(int).values\n",
    "evt_count_val = build_evt_count(df_model.loc[mask_val, :], EVT_COLS)\n",
    "\n",
    "thr_tbls = {}\n",
    "for model_name, pcol in [(\"logit\",\"pd_logit\"), (\"tree_calibrated\",\"pd_tree\")]:\n",
    "    p_val = df_model.loc[mask_val, pcol].values\n",
    "\n",
    "    # Cost-optimal PD-only threshold\n",
    "    costs = [expected_cost(y_val, (p_val >= thr).astype(int)) for thr in grid]\n",
    "    thr_cost_opt = float(grid[int(np.argmin(costs))])\n",
    "\n",
    "    # Capacity and monitoring thresholds (operational)\n",
    "    thr_capacity = float(np.quantile(p_val, 1-CAPACITY_PCT))\n",
    "    thr_monitor = float(np.quantile(p_val, 1-MONITOR_PCT))\n",
    "\n",
    "    # Hybrid score thresholds\n",
    "    score_val = hybrid_score(p_val, evt_count_val)\n",
    "    hybrid_costs = [expected_cost(y_val, (score_val >= thr).astype(int)) for thr in grid]\n",
    "    thr_cost_opt_hybrid = float(grid[int(np.argmin(hybrid_costs))])\n",
    "    thr_capacity_hybrid = float(np.quantile(score_val, 1-CAPACITY_PCT))\n",
    "    thr_monitor_hybrid = float(np.quantile(score_val, 1-MONITOR_PCT))\n",
    "\n",
    "    thr_tbls[model_name] = {\n",
    "        \"thr_cost_opt\": thr_cost_opt,\n",
    "        \"thr_capacity\": thr_capacity,\n",
    "        \"thr_monitor\": thr_monitor,\n",
    "        \"thr_cost_opt_hybrid\": thr_cost_opt_hybrid,\n",
    "        \"thr_capacity_hybrid\": thr_capacity_hybrid,\n",
    "        \"thr_monitor_hybrid\": thr_monitor_hybrid,\n",
    "    }\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(grid, costs)\n",
    "    plt.title(f\"Validation expected cost vs PD threshold \u2014 {model_name}\")\n",
    "    plt.xlabel(\"PD threshold\")\n",
    "    plt.ylabel(\"Expected misclassification cost\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(CONFIG[\"FIG_DIR\"]) / f\"cost_curve_{model_name}_val.png\", dpi=160)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(grid, hybrid_costs)\n",
    "    plt.title(f\"Validation expected cost vs hybrid score threshold \u2014 {model_name}\")\n",
    "    plt.xlabel(\"Hybrid score threshold\")\n",
    "    plt.ylabel(\"Expected misclassification cost\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(CONFIG[\"FIG_DIR\"]) / f\"cost_curve_hybrid_{model_name}_val.png\", dpi=160)\n",
    "    plt.show()\n",
    "\n",
    "display(pd.DataFrame(thr_tbls).T)\n",
    "\n",
    "# --- Policy comparison on TEST: capacity-controlled vs cost-optimal regimes ---\n",
    "mask_test = df_model[\"split\"] == \"test\"\n",
    "y_test = df_model.loc[mask_test, TARGET_NAME].astype(int).values\n",
    "evt_count_test = build_evt_count(df_model.loc[mask_test, :], EVT_COLS)\n",
    "\n",
    "rows = []\n",
    "for model_name, pcol in [(\"logit\",\"pd_logit\"), (\"tree_calibrated\",\"pd_tree\")]:\n",
    "    p_test = df_model.loc[mask_test, pcol].values\n",
    "    score_test = hybrid_score(p_test, evt_count_test)\n",
    "\n",
    "    for regime in [\"capacity_controlled\", \"cost_optimal\"]:\n",
    "        if regime == \"capacity_controlled\":\n",
    "            thr_screen = thr_tbls[model_name][\"thr_capacity\"]\n",
    "            thr_monitor = thr_tbls[model_name][\"thr_monitor\"]\n",
    "            thr_screen_hybrid = thr_tbls[model_name][\"thr_capacity_hybrid\"]\n",
    "            thr_monitor_hybrid = thr_tbls[model_name][\"thr_monitor_hybrid\"]\n",
    "        else:\n",
    "            thr_screen = thr_tbls[model_name][\"thr_cost_opt\"]\n",
    "            thr_monitor = thr_tbls[model_name][\"thr_monitor\"]\n",
    "            thr_screen_hybrid = thr_tbls[model_name][\"thr_cost_opt_hybrid\"]\n",
    "            thr_monitor_hybrid = thr_tbls[model_name][\"thr_monitor_hybrid\"]\n",
    "\n",
    "        # PD-only (screen decision)\n",
    "        polA = apply_pd_only_policy(p_test, thr_screen, thr_monitor)\n",
    "        costA = expected_cost(y_test, polA[\"screen\"])\n",
    "        capA = float(polA[\"screen\"].mean())\n",
    "        tprA = float(((polA[\"screen\"]==1) & (y_test==1)).sum() / max(1, (y_test==1).sum()))\n",
    "        ppvA = float(((polA[\"screen\"]==1) & (y_test==1)).sum() / max(1, (polA[\"screen\"]==1).sum()))\n",
    "        countsA = confusion_counts(y_test, polA[\"screen\"])\n",
    "\n",
    "        rows.append({\n",
    "            \"policy_regime\": regime,\n",
    "            \"model\": model_name,\n",
    "            \"policy\": \"PD-only\",\n",
    "            \"screen_rate\": capA,\n",
    "            \"monitor_rate\": float(polA[\"monitor\"].mean()),\n",
    "            \"tpr_screen\": tprA,\n",
    "            \"ppv_screen\": ppvA,\n",
    "            \"tp_screen\": countsA[\"tp\"],\n",
    "            \"fp_screen\": countsA[\"fp\"],\n",
    "            \"expected_cost\": costA,\n",
    "            \"thr_screen_pd\": thr_screen,\n",
    "            \"thr_monitor_pd\": thr_monitor,\n",
    "        })\n",
    "\n",
    "        # Hybrid (screen decision derived from composite score)\n",
    "        polB = apply_hybrid_policy_from_score(score_test, thr_screen_hybrid, thr_monitor_hybrid)\n",
    "        costB = expected_cost(y_test, polB[\"screen\"])\n",
    "        capB = float(polB[\"screen\"].mean())\n",
    "        tprB = float(((polB[\"screen\"]==1) & (y_test==1)).sum() / max(1, (y_test==1).sum()))\n",
    "        ppvB = float(((polB[\"screen\"]==1) & (y_test==1)).sum() / max(1, (polB[\"screen\"]==1).sum()))\n",
    "        countsB = confusion_counts(y_test, polB[\"screen\"])\n",
    "\n",
    "        overlap_both = float(((polA[\"screen\"]==1) & (polB[\"screen\"]==1)).mean())\n",
    "        overlap_pd_only = float((polA[\"screen\"]==1).mean())\n",
    "        overlap_evt_override = float(((polB[\"screen\"]==1) & (polA[\"screen\"]==0)).mean())\n",
    "\n",
    "        rows.append({\n",
    "            \"policy_regime\": regime,\n",
    "            \"model\": model_name,\n",
    "            \"policy\": \"Hybrid (PD + events)\",\n",
    "            \"screen_rate\": capB,\n",
    "            \"monitor_rate\": float(polB[\"monitor\"].mean()),\n",
    "            \"tpr_screen\": tprB,\n",
    "            \"ppv_screen\": ppvB,\n",
    "            \"tp_screen\": countsB[\"tp\"],\n",
    "            \"fp_screen\": countsB[\"fp\"],\n",
    "            \"expected_cost\": costB,\n",
    "            \"thr_screen_score\": thr_screen_hybrid,\n",
    "            \"thr_monitor_score\": thr_monitor_hybrid,\n",
    "            \"alpha_evt_any\": ALPHA_EVT,\n",
    "            \"beta_evt_2plus\": BETA_EVT,\n",
    "            \"share_screen_pd_only\": overlap_pd_only,\n",
    "            \"share_screen_both\": overlap_both,\n",
    "            \"share_screen_evt_override\": overlap_evt_override,\n",
    "        })\n",
    "\n",
    "policy_cmp = pd.DataFrame(rows).sort_values([\"policy_regime\", \"model\", \"policy\"])\n",
    "print(\"\\nPolicy comparison on TEST (screen decision):\")\n",
    "display(policy_cmp)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6e71b677",
   "metadata": {},
   "source": [
    "### 9.4.1 Decision curve analysis (net benefit)\n",
    "\n",
    "Decision curves provide an alternative view of \u201cresponse capability\u201d: the net benefit of acting at different PD thresholds (treat-all vs treat-none baselines)."
   ]
  },
  {
   "cell_type": "code",
   "id": "fce9eda2",
   "metadata": {},
   "source": [
    "def net_benefit(y_true: np.ndarray, p: np.ndarray, pt: float) -> float:\n",
    "    y_hat = (p >= pt).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_hat).ravel()\n",
    "    n = len(y_true)\n",
    "    w = pt/(1-pt)\n",
    "    return (tp/n) - (fp/n)*w\n",
    "\n",
    "mask = df_model[\"split\"]==\"test\"\n",
    "y_test_np = df_model.loc[mask, TARGET_NAME].astype(int).values\n",
    "\n",
    "pts = np.linspace(0.01, 0.50, 50)\n",
    "plt.figure()\n",
    "for model_name, pcol in [(\"logit\",\"pd_logit\"), (\"tree_calibrated\",\"pd_tree\")]:\n",
    "    p = df_model.loc[mask, pcol].values\n",
    "    nb = [net_benefit(y_test_np, p, pt) for pt in pts]\n",
    "    plt.plot(pts, nb, label=model_name)\n",
    "\n",
    "# Treat-all and treat-none baselines\n",
    "event_rate = y_test_np.mean()\n",
    "nb_all = [event_rate - (1-event_rate)*(pt/(1-pt)) for pt in pts]\n",
    "nb_none = [0 for _ in pts]\n",
    "plt.plot(pts, nb_all, linestyle=\"--\", label=\"treat-all\")\n",
    "plt.plot(pts, nb_none, linestyle=\"--\", label=\"treat-none\")\n",
    "\n",
    "plt.title(\"Decision curves (test split): net benefit vs threshold probability\")\n",
    "plt.xlabel(\"Threshold probability (pt)\")\n",
    "plt.ylabel(\"Net benefit\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(CONFIG[\"FIG_DIR\"]) / \"decision_curves_test.png\", dpi=160)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "11412605",
   "metadata": {},
   "source": [
    "### 9.5 Scenario analysis (liquidity insurance vs liquidity fragility)\n",
    "\n",
    "Scenario analysis is a **communication and response** instrument: a structured way to translate plausible managerial / creditor actions and stress environments into PD sensitivity and triage policies\u2014not as causal inference.\n",
    "\n",
    "Below are scenario-analysis use cases that are materially more defensible (and operationally useful) than \"raise current ratio to 1.2\" or \"CFO +10% of assets,\" while still fitting the two-layer architecture (PD model + non-predictor event layer):\n",
    "\n",
    "**2) Liquidity insurance vs liquidity fragility scenarios (cash, working capital, and credit lines)**\n",
    "\n",
    "**Why it is a better use case:**\n",
    "- Instead of targeting an arbitrary current ratio, use liquidity scenarios that map to liquidity insurance mechanisms documented in the literature: cash buffers and bank lines of credit.\n",
    "- Corporate cash holdings are a fundamental margin of safety with systematic determinants and implications.\n",
    "- Lines of credit are a core liquidity management instrument; their availability is state-contingent and interacts with profitability/cash flow.\n",
    "\n",
    "**What to implement (scenario templates):**\n",
    "- **Cash buffer stress:** \"cash burn\" scenario: `che \u2190 che \u2212 \u0394` (bounded at 0), optionally `oancf \u2190 oancf \u2212 \u0394` if you want a consistent flow hit.\n",
    "- **Working-capital release** (high realism, accounting-consistent): Reduce receivables and inventory (`rect`, `invt`) by a fraction; increase cash by the same amount. This is typically more plausible than \"CFO +10% of assets\" because it corresponds to collections and inventory liquidation.\n",
    "- **Liquidity squeeze + maturity wall:** shift a portion of `dltt` into `dlc` (or increase `dlc` share) to mimic refinancing risk; recompute short-term debt share and liquidity ratios.\n",
    "\n",
    "**Decision-support outputs:**\n",
    "- PD sensitivity to liquidity burn speed (months of runway proxy; even with annual data, you can approximate).\n",
    "- A liquidity \"traffic light\" that combines PD tier + liquidity events (`evt_liq_squeeze` / `evt_quick_squeeze`) for escalation."
   ]
  },
  {
   "cell_type": "code",
   "id": "350a73f0",
   "metadata": {},
   "source": [
    "required_names = ['df_model','df','continuous_feats_raw','event_feats','train_medians','winsor_bounds','scaler','logit_clf','xgb_model','best_iter','TRAIN_FEATURE_LIST','iso']\n",
    "missing = [name for name in required_names if name not in globals()]\n",
    "if missing:\n",
    "    ip = get_ipython(); user_ns = ip.user_ns if ip else {}; import builtins as _b\n",
    "    for name in missing:\n",
    "        if name in user_ns: globals()[name] = user_ns[name]\n",
    "        elif hasattr(_b, name): globals()[name] = getattr(_b, name)\n",
    "missing = [name for name in required_names if name not in globals()]\n",
    "if missing: raise NameError(f\"Missing required globals for scenario analysis: {', '.join(missing)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 9.5 Scenario analysis (liquidity insurance vs. fragility)\n",
    "# =============================================================================\n",
    "# Key design choice:\n",
    "# - Do NOT re-run dataset-wide cleaning here (medians, winsor bounds, scaler are already fit upstream).\n",
    "# - For scenarios, start from the already-clean base feature vector (df_model[continuous_feats_raw]) and\n",
    "#   ONLY overwrite features that are mechanically affected by the raw accounting perturbation.\n",
    "# - Then apply the *same* fitted (train) transforms: median fill -> winsor clip -> scaler transform.\n",
    "\n",
    "import traceback, sys, time\n",
    "print(\"[9.5] start\")\n",
    "t0 = time.time()\n",
    "def _checkpoint(msg): print(f\"[9.5] {msg} (t={time.time()-t0:.2f}s)\")\n",
    "\n",
    "def _safe_div(n, d):\n",
    "    try:\n",
    "        n_f = float(n); d_f = float(d)\n",
    "        if pd.isna(n_f) or pd.isna(d_f) or d_f == 0:\n",
    "            return np.nan\n",
    "        return n_f / d_f\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def _build_continuous_features_from_raw(row_raw: pd.Series) -> dict:\n",
    "    \"\"\"Compute only the continuous model features needed, from raw accounting items.\"\"\"\n",
    "    # 1) raw items\n",
    "    at    = row_raw.get(\"at\", np.nan)\n",
    "    che   = row_raw.get(\"che\", np.nan)\n",
    "    act   = row_raw.get(\"act\", np.nan)\n",
    "    lct   = row_raw.get(\"lct\", np.nan)\n",
    "    rect  = row_raw.get(\"rect\", np.nan)\n",
    "    invt  = row_raw.get(\"invt\", np.nan)\n",
    "    lt    = row_raw.get(\"lt\", np.nan)\n",
    "    dlc   = row_raw.get(\"dlc\", np.nan)\n",
    "    dltt  = row_raw.get(\"dltt\", np.nan)\n",
    "    oibdp = row_raw.get(\"oibdp\", np.nan)\n",
    "    dp    = row_raw.get(\"dp\", np.nan)\n",
    "    xint  = row_raw.get(\"xint\", np.nan)\n",
    "    ceq   = row_raw.get(\"ceq\", np.nan)\n",
    "    capx  = row_raw.get(\"capx\", np.nan)\n",
    "    ppent = row_raw.get(\"ppent\", np.nan)\n",
    "    intan = row_raw.get(\"intan\", np.nan)\n",
    "    oancf = row_raw.get(\"oancf\", np.nan)\n",
    "    re    = row_raw.get(\"re\", np.nan)\n",
    "    niadj = row_raw.get(\"niadj\", np.nan)\n",
    "    prstkc = row_raw.get(\"prstkc\", np.nan)\n",
    "\n",
    "    # 2) totals/derived bases\n",
    "    d_vals = [v for v in [dlc, dltt] if pd.notna(v)]\n",
    "    total_debt = float(np.sum(d_vals)) if len(d_vals) > 0 else np.nan\n",
    "\n",
    "    # 3) ratios\n",
    "    feat = {}\n",
    "    feat[\"ln_at\"] = np.log(at) if pd.notna(at) and at > 0 else np.nan\n",
    "    feat[\"cash_at\"] = _safe_div(che, at)\n",
    "\n",
    "    feat[\"current_ratio\"] = _safe_div(act, lct)\n",
    "    if pd.notna(act) and pd.notna(invt) and pd.notna(lct) and lct != 0:\n",
    "        feat[\"quick_ratio\"] = _safe_div(act - invt, lct)\n",
    "    elif pd.notna(che) and pd.notna(rect) and pd.notna(lct) and lct != 0:\n",
    "        feat[\"quick_ratio\"] = _safe_div(che + rect, lct)\n",
    "    else:\n",
    "        feat[\"quick_ratio\"] = feat[\"current_ratio\"]\n",
    "\n",
    "    feat[\"nwc_at\"] = _safe_div((act - lct), at)\n",
    "\n",
    "    feat[\"rect_act\"] = _safe_div(rect, act)\n",
    "    feat[\"invt_act\"] = _safe_div(invt, act)\n",
    "\n",
    "    feat[\"lt_at\"] = _safe_div(lt, at)\n",
    "    feat[\"dlc_at\"] = _safe_div(dlc, at)\n",
    "    feat[\"dltt_at\"] = _safe_div(dltt, at)\n",
    "    feat[\"debt_at\"] = _safe_div(total_debt, at)\n",
    "    feat[\"st_debt_share\"] = _safe_div(dlc, total_debt)\n",
    "\n",
    "    feat[\"ebitda_at\"] = _safe_div(oibdp, at)\n",
    "    feat[\"xint_at\"] = _safe_div(xint, at)\n",
    "    feat[\"interest_coverage\"] = _safe_div(oibdp, xint)\n",
    "    feat[\"debt_to_ebitda\"] = _safe_div(total_debt, oibdp)\n",
    "    feat[\"ebit_to_capital\"] = _safe_div((oibdp - dp), (total_debt + ceq))\n",
    "\n",
    "    feat[\"ppent_at\"] = _safe_div(ppent, at)\n",
    "    feat[\"intan_at\"] = _safe_div(intan, at)\n",
    "    feat[\"ocf_to_debt\"] = _safe_div(oancf, total_debt)\n",
    "    feat[\"fcf_to_debt\"] = _safe_div((oancf - capx), total_debt)\n",
    "    feat[\"re_at\"] = _safe_div(re, at)\n",
    "\n",
    "    feat[\"ceq_at\"] = _safe_div(ceq, at)\n",
    "    feat[\"niadj_at\"] = _safe_div(niadj, at)\n",
    "    feat[\"prstkc_at\"] = _safe_div(prstkc, at)\n",
    "    return feat\n",
    "\n",
    "\n",
    "def clip_audit_from_raw(row_raw: pd.Series, base_feature_row: pd.Series, top_n: int = 6) -> tuple[int, pd.DataFrame]:\n",
    "    feat_updates = _build_continuous_features_from_raw(row_raw)\n",
    "    audit_rows = []\n",
    "    for k, raw_val in feat_updates.items():\n",
    "        if k not in winsor_bounds:\n",
    "            continue\n",
    "        v = raw_val\n",
    "        if pd.isna(v):\n",
    "            v = train_medians[k]\n",
    "        lo, hi = winsor_bounds[k]\n",
    "        if pd.isna(lo) or pd.isna(hi):\n",
    "            continue\n",
    "        clipped_val = float(np.clip(v, lo, hi))\n",
    "        if v < lo or v > hi:\n",
    "            audit_rows.append({\n",
    "                \"feature\": k,\n",
    "                \"raw_value\": float(v),\n",
    "                \"winsor_lo\": float(lo),\n",
    "                \"winsor_hi\": float(hi),\n",
    "                \"clipped_value\": clipped_val,\n",
    "                \"excess\": float(max(lo - v, v - hi)),\n",
    "            })\n",
    "    audit = pd.DataFrame(audit_rows).sort_values(\"excess\", ascending=False)\n",
    "    return int(len(audit_rows)), audit.head(top_n)\n",
    "\n",
    "\n",
    "def build_model_features_from_raw_scenario(row_raw: pd.Series, base_feature_row: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the model feature vector for a scenario without redoing dataset-wide cleaning.\n",
    "    - Start from already-clean feature values (base_feature_row[continuous_feats_raw]).\n",
    "    - Overwrite features mechanically implied by the scenario raw row.\n",
    "    - Apply the fitted preprocessing (train medians, winsor bounds, scaler).\n",
    "    \"\"\"\n",
    "    out_cont = pd.Series({c: float(base_feature_row[c]) for c in continuous_feats_raw})\n",
    "\n",
    "    feat_updates = _build_continuous_features_from_raw(row_raw)\n",
    "    for k, v in feat_updates.items():\n",
    "        if k in out_cont.index:\n",
    "            out_cont[k] = v\n",
    "\n",
    "    out = pd.DataFrame([out_cont.to_dict()])\n",
    "\n",
    "    for c in continuous_feats_raw:\n",
    "        v = out[c].replace([np.inf, -np.inf], np.nan)\n",
    "        v = v.fillna(train_medians[c])\n",
    "        lo, hi = winsor_bounds[c]\n",
    "        v = apply_bounds(v, lo, hi)\n",
    "        out[c] = v\n",
    "\n",
    "    Z = scaler.transform(out[continuous_feats_raw].astype(float))\n",
    "    for j, c in enumerate(continuous_feats_raw):\n",
    "        out[f\"z_{c}\"] = Z[:, j]\n",
    "\n",
    "    # keep events from base (decision-support layer not redefined here)\n",
    "    for e in event_feats:\n",
    "        out[e] = int(base_feature_row[e]) if e in base_feature_row.index else 0\n",
    "\n",
    "    X_out = out[[f\"z_{c}\" for c in continuous_feats_raw] + event_feats]\n",
    "    return X_out[TRAIN_FEATURE_LIST]\n",
    "\n",
    "\n",
    "def predict_pd_from_features(X_row: pd.DataFrame) -> dict:\n",
    "    try:\n",
    "        X_row = X_row[TRAIN_FEATURE_LIST]\n",
    "        pd_logit = float(logit_clf.predict_proba(X_row)[:, 1][0])\n",
    "        drow = xgb.DMatrix(X_row, feature_names=TRAIN_FEATURE_LIST)\n",
    "        pd_tree_raw = float(xgb_model.predict(drow, iteration_range=(0, best_iter))[0])\n",
    "        pd_tree = float(iso.transform([pd_tree_raw])[0])\n",
    "        return {\"pd_logit\": pd_logit, \"pd_tree_raw\": pd_tree_raw, \"pd_tree\": pd_tree}\n",
    "    except Exception as e:\n",
    "        print(\"[9.5] Exception in predict_pd_from_features:\", repr(e))\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "\n",
    "def compute_liquidity_from_raw_row(row_data: pd.Series) -> dict:\n",
    "    act  = row_data.get(\"act\", np.nan)\n",
    "    lct  = row_data.get(\"lct\", np.nan)\n",
    "    invt = row_data.get(\"invt\", np.nan)\n",
    "    che  = row_data.get(\"che\", np.nan)\n",
    "    rect = row_data.get(\"rect\", np.nan)\n",
    "\n",
    "    cr = _safe_div(act, lct)\n",
    "    if pd.notna(act) and pd.notna(invt) and pd.notna(lct) and lct != 0:\n",
    "        qr = _safe_div(act - invt, lct)\n",
    "    elif pd.notna(che) and pd.notna(rect) and pd.notna(lct) and lct != 0:\n",
    "        qr = _safe_div(che + rect, lct)\n",
    "    else:\n",
    "        qr = cr\n",
    "\n",
    "    return {\n",
    "        \"current_ratio\": cr,\n",
    "        \"quick_ratio\": qr,\n",
    "        \"evt_liq_squeeze\": 1.0 if (pd.notna(cr) and cr < 1.0) else 0.0,\n",
    "        \"evt_quick_squeeze\": 1.0 if (pd.notna(qr) and qr < 0.8) else 0.0,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_traffic_light(pd_logit: float, evt_liq: float, evt_quick: float) -> str:\n",
    "    if pd_logit < 0.2:\n",
    "        pd_tier = \"Low\"\n",
    "    elif pd_logit < 0.5:\n",
    "        pd_tier = \"Medium\"\n",
    "    else:\n",
    "        pd_tier = \"High\"\n",
    "\n",
    "    has_liq = (evt_liq > 0.5) or (evt_quick > 0.5)\n",
    "    if pd_tier == \"Low\" and not has_liq:\n",
    "        return \"Green\"\n",
    "    if pd_tier == \"High\" and has_liq:\n",
    "        return \"Red\"\n",
    "    return \"Yellow\"\n",
    "\n",
    "\n",
    "def sensitivity_audit(base_X: pd.DataFrame, scen_X: pd.DataFrame, threshold: float = 1e-4, top_k: int = 15) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for feat in continuous_feats_raw:\n",
    "        zb = float(base_X[f\"z_{feat}\"].iloc[0])\n",
    "        zs = float(scen_X[f\"z_{feat}\"].iloc[0])\n",
    "        delta = zs - zb\n",
    "        rows.append({\"feature\": feat, \"z_base\": zb, \"z_scenario\": zs, \"z_delta\": delta})\n",
    "\n",
    "    df_audit = pd.DataFrame(rows, columns=[\"feature\", \"z_base\", \"z_scenario\", \"z_delta\"])\n",
    "    df_audit[\"abs_z_delta\"] = df_audit[\"z_delta\"].abs()\n",
    "    df_audit = df_audit.sort_values(\"abs_z_delta\", ascending=False)\n",
    "\n",
    "    df_sig = df_audit.loc[df_audit[\"abs_z_delta\"] > threshold].copy() if threshold > 0 else df_audit.copy()\n",
    "    df_top = df_audit.head(int(top_k)).copy()\n",
    "\n",
    "    df_top.attrs[\"material_count\"] = int(len(df_sig))\n",
    "    df_top.attrs[\"material_table\"] = df_sig\n",
    "    return df_top\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Select a sensitivity-safe representative observation\n",
    "# -----------------------------------------------------------------------------\n",
    "_checkpoint(\"select representative observation\")\n",
    "test_df = df_model.loc[df_model[\"split\"] == \"test\", :].copy()\n",
    "\n",
    "# Keep rect/invt in key_items even though WC release is deleted: quick ratio uses invt.\n",
    "key_items = [\"at\",\"che\",\"act\",\"lct\",\"rect\",\"invt\",\"dlc\",\"dltt\",\"oibdp\",\"xint\",\"oancf\",\"capx\",\"ceq\"]\n",
    "\n",
    "cand_mask = (test_df[\"pd_logit\"].between(0.15, 0.85)) & (test_df[\"pd_tree\"].between(0.15, 0.85))\n",
    "\n",
    "candidate_indices = []\n",
    "for idx in test_df.loc[cand_mask].index:\n",
    "    row_chk = df.loc[idx, :]\n",
    "    if not all(pd.notna(row_chk.get(k, np.nan)) for k in key_items):\n",
    "        continue\n",
    "    cr = _safe_div(row_chk.get(\"act\", np.nan), row_chk.get(\"lct\", np.nan))\n",
    "    if pd.notna(cr) and (0.2 <= cr <= 5.0):\n",
    "        candidate_indices.append(idx)\n",
    "\n",
    "if len(candidate_indices) == 0:\n",
    "    print(\"Warning: No observation meets strict criteria. Relaxing PD bounds to [0.10, 0.90].\")\n",
    "    cand_mask = (test_df[\"pd_logit\"].between(0.10, 0.90)) & (test_df[\"pd_tree\"].between(0.10, 0.90))\n",
    "    for idx in test_df.loc[cand_mask].index:\n",
    "        row_chk = df.loc[idx, :]\n",
    "        if not all(pd.notna(row_chk.get(k, np.nan)) for k in key_items):\n",
    "            continue\n",
    "        cr = _safe_div(row_chk.get(\"act\", np.nan), row_chk.get(\"lct\", np.nan))\n",
    "        if pd.notna(cr) and (0.2 <= cr <= 5.0):\n",
    "            candidate_indices.append(idx)\n",
    "\n",
    "if len(candidate_indices) == 0:\n",
    "    print(\"Warning: Fallback to highest-PD test observation with required raw items.\")\n",
    "    for idx in test_df.sort_values(\"pd_logit\", ascending=False).index:\n",
    "        row_chk = df.loc[idx, :]\n",
    "        if all(pd.notna(row_chk.get(k, np.nan)) for k in key_items):\n",
    "            candidate_indices = [idx]\n",
    "            break\n",
    "\n",
    "if len(candidate_indices) == 0:\n",
    "    raise ValueError(\"No suitable representative observation found. Inspect data quality and keys.\")\n",
    "\n",
    "cand_df = test_df.loc[candidate_indices, [\"pd_logit\", \"pd_tree\"]].copy()\n",
    "clip_counts = {}\n",
    "for idx in cand_df.index:\n",
    "    row_chk = df.loc[idx, :]\n",
    "    n_clip, _ = clip_audit_from_raw(row_chk, test_df.loc[idx, :])\n",
    "    clip_counts[idx] = n_clip\n",
    "cand_df[\"clip_count\"] = pd.Series(clip_counts)\n",
    "min_clip = int(cand_df[\"clip_count\"].min())\n",
    "low_clip_df = cand_df.loc[cand_df[\"clip_count\"] <= min_clip].copy()\n",
    "if low_clip_df.empty:\n",
    "    low_clip_df = cand_df.copy()\n",
    "median_pd = float(low_clip_df[\"pd_logit\"].median())\n",
    "rep_idx = (low_clip_df[\"pd_logit\"] - median_pd).abs().idxmin()\n",
    "\n",
    "row0_raw  = df.loc[rep_idx, :].copy()\n",
    "row0_feat = df_model.loc[rep_idx, :].copy()\n",
    "\n",
    "# base feature row already contains z_ columns from upstream pipeline\n",
    "base_X = row0_feat[[f\"z_{c}\" for c in continuous_feats_raw] + event_feats].to_frame().T\n",
    "base_pd = {\"pd_logit\": float(row0_feat[\"pd_logit\"]), \"pd_tree\": float(row0_feat[\"pd_tree\"])}\n",
    "base_liq = compute_liquidity_from_raw_row(row0_raw)\n",
    "\n",
    "base_clip_count, base_clip_top = clip_audit_from_raw(row0_raw, row0_feat)\n",
    "\n",
    "print(\"Representative observation (sensitivity-safe selection):\")\n",
    "display(df_model.loc[rep_idx, [\"firm_id\",\"fyear\",\"label_year\",\"pd_logit\",\"pd_tree\",\"target_next_v1\",\"target_next_v2\",\"target_next_v3\"]])\n",
    "print(\"Base PDs:\", base_pd)\n",
    "print(f\"Base liquidity: CR={base_liq['current_ratio']:.3f}, QR={base_liq['quick_ratio']:.3f}, \"\n",
    "      f\"evt_liq={base_liq['evt_liq_squeeze']:.0f}, evt_quick={base_liq['evt_quick_squeeze']:.0f}\")\n",
    "print(f\"Base winsor clipping count: {base_clip_count}\")\n",
    "if base_clip_count > 0:\n",
    "    print(\"Base clipped features (top drivers):\")\n",
    "    display(base_clip_top)\n",
    "\n",
    "print(\"=== Raw Accounting Sanity Check (Base) ===\")\n",
    "display(pd.DataFrame([{\"scenario\":\"base\", **{k: base_liq[k] for k in [\"current_ratio\",\"quick_ratio\",\"evt_liq_squeeze\",\"evt_quick_squeeze\"]}}]))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Run scenarios (ONLY cash burn + maturity wall)\n",
    "# -----------------------------------------------------------------------------\n",
    "_checkpoint(\"run scenarios\")\n",
    "scenario_rows = []\n",
    "\n",
    "def _append_result(name: str, row_s_raw: pd.Series):\n",
    "    Xs = build_model_features_from_raw_scenario(row_s_raw, row0_feat)\n",
    "    pds = predict_pd_from_features(Xs)\n",
    "    liq = compute_liquidity_from_raw_row(row_s_raw)\n",
    "    n_clip, clip_top = clip_audit_from_raw(row_s_raw, row0_feat)\n",
    "    scenario_rows.append({\n",
    "        \"scenario\": name,\n",
    "        **pds,\n",
    "        \"current_ratio\": liq[\"current_ratio\"],\n",
    "        \"quick_ratio\": liq[\"quick_ratio\"],\n",
    "        \"evt_liq_squeeze\": liq[\"evt_liq_squeeze\"],\n",
    "        \"evt_quick_squeeze\": liq[\"evt_quick_squeeze\"],\n",
    "        \"traffic_light\": get_traffic_light(pds[\"pd_logit\"], liq[\"evt_liq_squeeze\"], liq[\"evt_quick_squeeze\"]),\n",
    "        \"n_clipped_features\": n_clip,\n",
    "        \"top_clipped_features\": \", \".join(clip_top[\"feature\"].tolist()) if n_clip > 0 else \"\",\n",
    "    })\n",
    "    if n_clip > 0:\n",
    "        print(f\"[Scenario clipping audit] {name}: {n_clip} features clipped\")\n",
    "        display(clip_top)\n",
    "    return Xs\n",
    "\n",
    "# Base\n",
    "_ = _append_result(\"base\", row0_raw)\n",
    "\n",
    "# (A) Cash burn: burn fraction of ACT, reduce che/act/at; reduce ceq partially for rough coherence\n",
    "act_base = float(row0_raw.get(\"act\", 0.0) if pd.notna(row0_raw.get(\"act\", np.nan)) else 0.0)\n",
    "burn_rates = [0.10, 0.20, 0.30]\n",
    "\n",
    "for burn in burn_rates:\n",
    "    row_s = row0_raw.copy()\n",
    "    delta = burn * act_base\n",
    "\n",
    "    che0 = float(row_s.get(\"che\", 0.0) if pd.notna(row_s.get(\"che\", np.nan)) else 0.0)\n",
    "    act0 = float(row_s.get(\"act\", 0.0) if pd.notna(row_s.get(\"act\", np.nan)) else 0.0)\n",
    "    at0  = float(row_s.get(\"at\", 0.0)  if pd.notna(row_s.get(\"at\", np.nan))  else 0.0)\n",
    "    ceq0 = row_s.get(\"ceq\", np.nan)\n",
    "\n",
    "    row_s[\"che\"] = max(0.0, che0 - delta)\n",
    "    row_s[\"act\"] = max(0.0, act0 - delta)\n",
    "    row_s[\"at\"]  = max(1e-6, at0 - delta)  # keep positive for ln_at\n",
    "    if pd.notna(ceq0):\n",
    "        row_s[\"ceq\"] = max(0.0, float(ceq0) - 0.5 * delta)\n",
    "\n",
    "    Xs = _append_result(f\"cash_burn_{int(burn*100)}pct\", row_s)\n",
    "\n",
    "    audit_top = sensitivity_audit(base_X, Xs, threshold=1e-4, top_k=15)\n",
    "    mat_n = audit_top.attrs.get(\"material_count\", 0)\n",
    "\n",
    "    print(f\"=== Sensitivity Audit: cash_burn_{int(burn*100)}pct ===\")\n",
    "    print(f\"Features changed (|\u0394z|>1e-4): {mat_n}\")\n",
    "    display(audit_top)\n",
    "\n",
    "# (C) Maturity wall: reclassify dltt->dlc and increase lct accordingly (dlc is part of lct)\n",
    "dlc0 = float(row0_raw.get(\"dlc\", 0.0) if pd.notna(row0_raw.get(\"dlc\", np.nan)) else 0.0)\n",
    "dltt0 = float(row0_raw.get(\"dltt\", 0.0) if pd.notna(row0_raw.get(\"dltt\", np.nan)) else 0.0)\n",
    "lct0 = float(row0_raw.get(\"lct\", 0.0) if pd.notna(row0_raw.get(\"lct\", np.nan)) else 0.0)\n",
    "total_debt0 = dlc0 + dltt0\n",
    "\n",
    "shift_rates = [0.10, 0.20]\n",
    "for sh in shift_rates:\n",
    "    row_s = row0_raw.copy()\n",
    "    shift_amt = sh * total_debt0\n",
    "\n",
    "    row_s[\"dltt\"] = max(0.0, dltt0 - shift_amt)\n",
    "    row_s[\"dlc\"]  = dlc0 + shift_amt\n",
    "    row_s[\"lct\"]  = lct0 + shift_amt\n",
    "\n",
    "    Xs = _append_result(f\"maturity_wall_{int(sh*100)}pct\", row_s)\n",
    "\n",
    "    audit_top = sensitivity_audit(base_X, Xs, threshold=1e-4, top_k=15)\n",
    "    mat_n = audit_top.attrs.get(\"material_count\", 0)\n",
    "\n",
    "    print(f\"=== Sensitivity Audit: maturity_wall_{int(sh*100)}pct ===\")\n",
    "    print(f\"Features changed (|\u0394z|>1e-4): {mat_n}\")\n",
    "    display(audit_top)\n",
    "\n",
    "scenario_tbl = pd.DataFrame(scenario_rows)\n",
    "\n",
    "print(\"=== Scenario Analysis Results ===\")\n",
    "cols = [\"scenario\",\"pd_logit\",\"pd_tree_raw\",\"pd_tree\",\"current_ratio\",\"quick_ratio\",\"evt_liq_squeeze\",\"evt_quick_squeeze\",\"traffic_light\",\"n_clipped_features\",\"top_clipped_features\"]\n",
    "display(scenario_tbl[cols])\n",
    "\n",
    "print(\"=== Liquidity Traffic Light Summary ===\")\n",
    "traffic_summary = scenario_tbl[[\"scenario\",\"pd_logit\",\"evt_liq_squeeze\",\"evt_quick_squeeze\",\"traffic_light\"]].copy()\n",
    "traffic_summary[\"pd_tier\"] = traffic_summary[\"pd_logit\"].apply(lambda x: \"Low\" if x < 0.2 else (\"Medium\" if x < 0.5 else \"High\"))\n",
    "display(traffic_summary)\n",
    "\n",
    "print(\"[9.5] finished OK\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d6f5b3b2",
   "metadata": {},
   "source": [
    "### 9.6 Audit / Assertions"
   ]
  },
  {
   "cell_type": "code",
   "id": "f1f30485",
   "metadata": {},
   "source": [
    "# Final audit checks to surface defects early\n",
    "\n",
    "# 1) Decile collapse check (tree calibrated PD)\n",
    "try:\n",
    "    dt_tree = decile_table(df_model.loc[df_model[\"split\"]==\"test\", :], \"pd_tree\", TARGET_NAME)\n",
    "    n_deciles = int(dt_tree[\"decile\"].nunique()) if not dt_tree.empty else 0\n",
    "    assert n_deciles == 10, f\"Decile collapse detected: {n_deciles} bins\"\n",
    "except Exception as e:\n",
    "    warnings.warn(f\"Audit: decile collapse check failed: {e}\")\n",
    "\n",
    "# 2) Granularity check (unique values and mass-point)\n",
    "try:\n",
    "    gran_tbl = pd_granularity_tbl.copy() if 'pd_granularity_tbl' in globals() else pd.DataFrame()\n",
    "    if not gran_tbl.empty:\n",
    "        tree_row = gran_tbl.loc[gran_tbl[\"series\"]==\"tree_calibrated\"].iloc[0]\n",
    "        min_unique = max(50, int(0.02 * len(df_model.loc[df_model[\"split\"]==\"test\", :])))\n",
    "        assert tree_row[\"n_unique\"] >= min_unique, f\"Low unique PDs: {tree_row['n_unique']} < {min_unique}\"\n",
    "        assert tree_row[\"top_value_share\"] <= 0.20, f\"Mass point too large: {tree_row['top_value_share']:.2%}\"\n",
    "except Exception as e:\n",
    "    warnings.warn(f\"Audit: granularity check failed: {e}\")\n",
    "\n",
    "# 3) Policy table regime labels\n",
    "try:\n",
    "    assert \"policy_regime\" in policy_cmp.columns, \"policy_regime missing\"\n",
    "    assert policy_cmp[\"policy_regime\"].notna().all(), \"policy_regime has missing values\"\n",
    "except Exception as e:\n",
    "    warnings.warn(f\"Audit: policy regime labeling failed: {e}\")\n",
    "\n",
    "# 4) Scenario engine response (tree PD should vary)\n",
    "try:\n",
    "    if 'scenario_tbl' in globals() and not scenario_tbl.empty:\n",
    "        pd_range = float(scenario_tbl[\"pd_tree\"].max() - scenario_tbl[\"pd_tree\"].min())\n",
    "        assert pd_range > 1e-6, \"Scenario tree PD appears constant\"\n",
    "except Exception as e:\n",
    "    warnings.warn(f\"Audit: scenario PD response check failed: {e}\")\n",
    "\n",
    "# 5) Scenario clipping saturation\n",
    "try:\n",
    "    if 'scenario_tbl' in globals() and \"n_clipped_features\" in scenario_tbl.columns:\n",
    "        max_clip = int(scenario_tbl[\"n_clipped_features\"].max())\n",
    "        assert max_clip <= 8, f\"Scenario clipping saturated (max clipped features = {max_clip})\"\n",
    "except Exception as e:\n",
    "    warnings.warn(f\"Audit: scenario clipping check failed: {e}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "757172aa",
   "metadata": {},
   "source": [
    "## 10. Results Summary & Interpretation Guardrails\n\n### 10.1 Diagnostic synthesis (what the plots imply)\n\n**A) Discrimination is solid; PR-AUC is the bottleneck (rare-event reality)**\n- Walk-forward PR-AUC: the tree model is consistently above logit in every reported validation year, but the absolute PR-AUC values are still modest (roughly ~0.10\u20130.24 in the plot), which is typical for rare transition events. (See the \u201cPR-AUC by validation year\u201d plot.)\n- Walk-forward ROC-AUC: the tree model is materially stronger (roughly ~0.78\u20130.88) than logit (~0.63\u20130.75). This means ranking is good, but positive precision at high recall is still hard.\n- **Implication:** you are already extracting nonlinear signal; the next gains come from (i) aligning training to the top-K/capacity decision rule and (ii) controlling probability extremeness / calibration stability.\n\n**B) Raw XGBoost probabilities are severely miscalibrated (overconfident), isotonic helps a lot**\n- Tree raw (val/test) calibration curves are far below the 45\u00b0 line even at high predicted PDs (e.g., mean predicted ~0.9\u20131.0 mapping to realized fraction positives ~0.18\u20130.22). That is extreme overconfidence.\n- Tree calibrated (val) is essentially on the diagonal (excellent\u2014expected because you fit isotonic on val). Tree calibrated (test) improves a lot but still shows some residual overprediction at the high-PD end.\n- Logit is also miscalibrated (overpredicts modestly), but less extreme than raw tree.\n- **Implication:** your isotonic step is doing real work; previously, miscalibration was amplified by compounded weighting and boosting dynamics. With decoupled weights now in place, any remaining tail issues likely stem from model capacity or sparse positives in the extreme tail.\n\n**C) Decision usefulness exists but is concentrated at low thresholds (consistent with COST_FN \u226b COST_FP)**\n- The decision curve shows both \u201clogit\u201d and \u201ctree calibrated\u201d above \u201ctreat-none\u201d for small thresholds; \u201ctreat-all\u201d is strongly negative. Net benefit rapidly collapses as thresholds increase.\n- The expected cost vs threshold curves also indicate the minimum expected cost occurs at very low PD thresholds, which is coherent with COST_FN=25 and rare positives.\n- **Implication:** AUC-PR is helpful, but your actual operating point is a capacity policy (top 20%)\u2014your training/early stopping should be aligned to top-K / expected-cost under capacity, not generic AUC-PR alone.\n\n**D) SHAP pattern is economically coherent (good sign)**\n- Top drivers in the SHAP summary are sensible: leverage (z_lt_at), profitability/cash-flow proxies (z_ebitda_at, z_ocf_to_debt, z_fcf_to_debt), size (z_ln_at), interest burden/coverage (z_xint_at, z_interest_coverage), and debt intensity (z_debt_to_ebitda). This is consistent with a transition-to-distress mechanism.\n\n### 10.2 Why you are getting extreme raw PDs (structural cause)\n\nThe most likely driver was the *previous* weighting scheme that conflated imbalance with misclassification costs. The fix is now in place:\n- **Imbalance handling** uses `scale_pos_weight = imbalance` in XGBoost.\n- **Decision-theoretic costs** are handled via sample weights only: `w_pos = COST_FN`, `w_neg = COST_FP`.\nThis prevents 25\u00d750-style compounding (e.g., 1250:1) and reduces the incentive for extreme PDs, improving calibration stability.\n\n### 10.3 What to change (prioritized), assuming compute/storage is not a constraint\n\n**Priority 1 \u2014 Align training/early stopping to your actual policy (CAPACITY_PCT=20%)**\n- Right now you early-stop on aucpr. That is better than ROC-AUC for rarity, but it still optimizes an integrated ranking metric, not your top-20% screening + asymmetric costs.\n- **Change:** add a custom evaluation metric for early stopping that matches your policy, e.g.\n  - Recall@K (K = 20% of validation sample), or\n  - Expected cost under capacity (screen top 20%, treat them as positives, compute FN/FP with your costs).\n- **Why it will help:** your plots show net benefit and cost improvements live almost entirely in the low-threshold / top-slice region. Optimizing directly for top-K performance typically improves what you care about most, even if PR-AUC barely moves.\n\n**Priority 2 \u2014 Validate weighting stability (now decoupled)**\n- The training setup now separates imbalance handling (`scale_pos_weight`) from cost-sensitive sample weights (`COST_FN`/`COST_FP`).\n- **Action:** stress-test calibration and tail behavior under small variations in `scale_pos_weight` and cost ratios, and document whether isotonic remains smooth in the tails.\n- **Expected result:** raw PDs are less extreme and calibration is more stable; any remaining miscalibration should now be attributable to model capacity or sparse tail support rather than compounded weights.\n\n**Priority 3 \u2014 Make the booster less \u201cspiky\u201d and more generalizable (your current params are not truly conservative)**\n- Your current setup has `eta=0.1`, `depth=5`, no gamma, no alpha, and relies mostly on `lambda=10` and `min_child_weight=5`.\n- For rare transitions, I would expect better out-of-time stability with:\n  - lower eta and more trees,\n  - shallower trees or more split penalty,\n  - stronger child-weight / gamma, and\n  - possibly DART or ensembling for variance reduction.\n- Concrete parameter directions (search ranges):\n  - **Core tree shape**\n    - `max_depth`: try 2\u20134 (depth 5 is often too expressive for rare events unless regularized heavily)\n    - `min_child_weight`: try 10\u2013100 (forces splits to be supported by more weighted mass)\n    - `gamma` (a.k.a. `min_split_loss`): try 0.5\u201310 (explicit split penalty)\n  - **Learning dynamics**\n    - `eta`: try 0.01\u20130.05\n    - `num_boost_round`: increase to 20k\u201350k (compute is not your constraint)\n    - `early_stopping_rounds`: reduce to 50\u2013100 once eta is smaller (200 with eta=0.1 can wander into overfit)\n  - **Regularization**\n    - `reg_lambda`: search 1\u201350 (10 is fine but not necessarily optimal)\n    - `reg_alpha`: search 0\u20132 (L1 can help stability and reduce noisy splits)\n  - **Sampling**\n    - `subsample`: search 0.5\u20130.9\n    - `colsample_bytree`: search 0.3\u20130.8 (0.8 can still overfit if many correlated ratios)\n  - **Stability knobs**\n    - `max_delta_step`: set to 1\u20135 (helps logistic boosting with imbalance; often improves stability)\n  - **Booster variants (if you really want robustness)**\n    - Try `booster=\"dart\"` with moderate dropout (variance reduction at the cost of compute)\n- These changes target exactly what your PDF suggests: strong ranking but unstable/extreme probability behavior.\n\n**Priority 4 \u2014 Calibration protocol upgrade (your current isotonic is directionally right, but can be cleaner)**\n- Right now you fit isotonic on validation, and that same validation set is also used for early stopping.\n- If you can afford it:\n  - Split your current validation into two parts:\n    - `val_es` for early stopping\n    - `val_cal` for isotonic fit\n- This reduces \u201cdouble use\u201d and usually improves test calibration reliability (especially for isotonic).\n- Also consider comparing isotonic vs sigmoid (Platt) as a robustness check; isotonic can overfit when positives are sparse. Your test calibration curve still deviates at the top end, consistent with limited tail support.\n\n**Priority 5 \u2014 Ensembling for free stability (compute-heavy, usually worth it)**\n- Train M models (e.g., 5\u201320) with:\n  - different seed,\n  - slightly jittered subsample/colsample,\n  - same walk-forward protocol,\n  - and average the predicted probabilities before calibration.\n- This typically:\n  - improves out-of-time PR performance a bit,\n  - materially stabilizes calibration curves,\n  - reduces year-to-year variance (which your walk-forward plots show).\n\n### 10.4 What \u201cbetter\u201d should mean for your transition case (what to monitor)\n\nGiven your capacity + cost policy, treat these as primary:\n- Recall@20% (how many transitions you capture when screening top 20%)\n- Precision@20% (how \u201cclean\u201d your screened set is)\n- Expected cost under capacity (using `COST_FN`/`COST_FP`)\n- Calibration diagnostics on test: Brier score, calibration slope/intercept, plus your curve\n\nAUC-PR remains useful as a secondary global metric, but your decision plots already show the operating region is the extreme left tail.\n\n### 10.5 One immediate red-flag check in your existing snippet (very important)\n\n- You set `eval_metric=\"aucpr\"` and you use heavy weights. That is fine, but you should confirm that your early stopping is selecting the model that actually improves top-20% cost/recall, not merely aucpr noise.\n- This is exactly why Priority 1 (policy-aligned early stopping metric) is the highest ROI change.\n\n### 10.6 Interpretation guardrails (publication-ready language)\n\n- The label is a **constructed proxy** for balance-sheet/coverage stress; it is not a legal default outcome.\n- Coefficients and SHAP values are **associational and predictive**, not causal effects.\n- Even with leakage controls, residual mechanical endogeneity may remain because accounting choices jointly affect both predictors and the proxy label.\n- Attrition (missing next-year observations) can create sample-selection distortions; diagnostics are reported via `has_next_year_obs`.\n\n### 10.7 Replication artifacts\n\nThe following tables/exports are written to `outputs/` for downstream paper workflow:\n- `config_summary.json`\n- `distress_rule.json`\n- `event_dictionary.csv`\n- `logit_inference_table.csv`\n- `metrics_table.csv`\n- `predictions.csv`\n\n### 10.8 Export tables, thresholds, and predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b9838ad0",
   "metadata": {},
   "source": [
    "out_dir = Path(CONFIG[\"OUTPUT_DIR\"])\n",
    "\n",
    "# Config + distress rule\n",
    "(out_dir / \"config_summary.json\").write_text(json.dumps(CONFIG, indent=2))\n",
    "(out_dir / \"distress_rule.json\").write_text(json.dumps(DISTRESS_RULE, indent=2))\n",
    "\n",
    "# Event dictionary\n",
    "event_dict.to_csv(out_dir / \"event_dictionary.csv\", index=False)\n",
    "\n",
    "# Logit inference table\n",
    "infer_tbl.reset_index().to_csv(out_dir / \"logit_inference_table.csv\", index=False)\n",
    "\n",
    "# Metrics table\n",
    "metrics_tbl.to_csv(out_dir / \"metrics_table.csv\", index=False)\n",
    "\n",
    "# Predictions export (replication-friendly)\n",
    "export_cols = [\"firm_id\",\"gvkey\",\"fyear\",\"label_year\",\"split\",\"target_next_v1\",\"target_next_v2\",\"target_next_v3\",\"pd_logit\",\"pd_tree\"]\n",
    "export_cols = [c for c in export_cols if c in df_model.columns]\n",
    "export_cols += [c for c in event_feats if c in df_model.columns]\n",
    "pred_export = df_model[export_cols].copy()\n",
    "pred_export.to_csv(out_dir / \"predictions.csv\", index=False)\n",
    "\n",
    "print(\"Wrote artifacts to:\", out_dir.resolve())\n",
    "print_df(pred_export, n=10, name=\"predictions.csv preview\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "50eece1d",
   "metadata": {},
   "source": [
    "### 10.4 Deployment and maintenance (future work)\n",
    "\n",
    "This notebook produces a research-grade replication pipeline. For production use (not required for journal replication), a minimal MLOps extension would include:\n",
    "- scheduled re-scoring and monitoring for drift in feature distributions and target prevalence,\n",
    "- retraining triggers and versioned model registry,\n",
    "- data validation contracts (schema + unit tests) for the upstream Compustat extraction process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}