{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e170ebd6",
   "metadata": {},
   "source": [
    "# Financial Distress Prediction Pipeline (Adjusted)\n",
    "\n",
    "This notebook follows the same structure as your current streamlined pipeline while fixing the material methodological and technical issues:\n",
    "\n",
    "- Panel-safe lag/lead construction (sorting enforced)\n",
    "- Missingness-aware distress proxy (avoids NaN → False “healthy” bias)\n",
    "- Leakage-free event threshold calibration (train-only)\n",
    "- Event indicators restricted to non-proxy channels (no coverage/leverage/EBITDA-proxy events)\n",
    "- Stable preprocessing (train-fitted clipping + median imputation + scaling)\n",
    "- Correct TreeSHAP extraction for XGBoost\n",
    "- Scenario analysis that propagates through engineered features (no deleveraging/coverage scenarios; no proxy-related shocks)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "245e0eb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:13.422995Z",
     "start_time": "2026-01-04T21:21:13.391983Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# 0. Project Overview — Financial Distress Prediction Pipeline\n",
    "# =============================================================================\n",
    "# This notebook follows the standard Data Science Lifecycle:\n",
    "#   (1) Data Cleaning and Quality Diagnostics\n",
    "#   (2) Missing-Data Handling (leakage-aware)\n",
    "#   (3) Feature Engineering and Label Construction\n",
    "#   (4) Event Indicators (interpretable drivers; non-proxy channels only)\n",
    "#   (5) Train / Validation / Test Split and Preprocessing\n",
    "#   (6) Logit Models (supervised benchmark + inference audit)\n",
    "#   (7) Tree-based Model (XGBoost with native TreeSHAP explainability)\n",
    "#   (8) Evaluation and Benchmarks (Persistence vs. Early Warning)\n",
    "#   (9) Decision Support and Scenario Analysis (primitive shocks; recompute features/events)\n",
    "#\n",
    "# Key design constraint (top-tier measurement integrity):\n",
    "#   - The distress outcome is a constructed proxy. To avoid circularity, the modeling feature set\n",
    "#     EXCLUDES leverage/coverage ratios that mechanically define the proxy.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, log_loss\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display\n"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "1f77ea01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:15.226223Z",
     "start_time": "2026-01-04T21:21:13.485594Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# 1. Data Import and Cleaning\n",
    "# =============================================================================\n",
    "\n",
    "DATA_PATH = \"data.csv\"  # file must be in the same folder\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "df.columns = df.columns.str.lower().str.strip()\n",
    "\n",
    "# Keep a stable panel identifier (Compustat-style gvkey) and fiscal year\n",
    "if \"gvkey\" not in df.columns or \"fyear\" not in df.columns:\n",
    "    raise ValueError(\"Input must include columns: gvkey, fyear\")\n",
    "\n",
    "# Drop duplicates (keep last record for a given firm-year) and enforce ordering BEFORE any lag/lead ops\n",
    "df = df.drop_duplicates(subset=[\"gvkey\", \"fyear\"], keep=\"last\").copy()\n",
    "\n",
    "# Normalize identifiers\n",
    "df = df[df[\"gvkey\"].notna()].copy()\n",
    "df[\"gvkey\"] = (\n",
    "    df[\"gvkey\"]\n",
    "      .astype(str)\n",
    "      .str.strip()\n",
    "      .str.replace(r\"\\.0$\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "df[\"fyear\"] = pd.to_numeric(df[\"fyear\"], errors=\"coerce\")\n",
    "df = df[df[\"fyear\"].notna()].copy()\n",
    "df[\"fyear\"] = df[\"fyear\"].astype(int)\n",
    "\n",
    "# Convert \"likely numeric\" columns (best-effort), but keep gvkey as string\n",
    "for col in df.columns:\n",
    "    if col == \"gvkey\":\n",
    "        continue\n",
    "    # Do not force-convert obvious non-numeric columns; keep best-effort\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"ignore\")\n",
    "\n",
    "df = df.sort_values([\"gvkey\", \"fyear\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]:,} firm-year observations, {df.shape[1]} variables.\")\n",
    "print(f\"Years: {int(df['fyear'].min())}–{int(df['fyear'].max())}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1A. EDA (pre-imputation): Missingness & basic distributions\n",
    "# =============================================================================\n",
    "\n",
    "TRAIN_END_YEAR  = 2020\n",
    "VAL_YEAR        = 2021\n",
    "TEST_START_YEAR = 2022\n",
    "\n",
    "train_mask_for_imputation = df[\"fyear\"] <= TRAIN_END_YEAR\n",
    "\n",
    "RAW_INPUT_CANDIDATES = [\n",
    "    # Magnitudes / size proxies\n",
    "    \"at\", \"mkvalt\",\n",
    "    # Debt/capital structure\n",
    "    \"dlc\", \"dltt\", \"seq\", \"mibt\",\n",
    "    # Operating performance & coverage inputs\n",
    "    \"oibdp\", \"xint\", \"txt\", \"txdc\", \"txach\",\n",
    "    # Cash flow statement\n",
    "    \"oancf\", \"capx\",\n",
    "    # Liquidity & payout policy (for non-proxy event indicators)\n",
    "    \"che\", \"dv\",\n",
    "    # Liquidity ratio inputs (optional)\n",
    "    \"act\", \"lct\",\n",
    "    # Share repurchases (optional; for a broad DCF proxy)\n",
    "    \"prstkc\",\n",
    "]\n",
    "RAW_INPUTS = [c for c in RAW_INPUT_CANDIDATES if c in df.columns]\n",
    "\n",
    "df_raw_pre = df[RAW_INPUTS].copy()\n",
    "\n",
    "pre_miss = pd.DataFrame({\n",
    "    \"col\": RAW_INPUTS,\n",
    "    \"n\": [int(len(df_raw_pre)) for _ in RAW_INPUTS],\n",
    "    \"n_na_pre\": [int(df_raw_pre[c].isna().sum()) for c in RAW_INPUTS],\n",
    "    \"pct_na_pre\": [float(df_raw_pre[c].isna().mean() * 100.0) for c in RAW_INPUTS],\n",
    "    \"train_pct_na_pre\": [\n",
    "        float(df_raw_pre.loc[train_mask_for_imputation, c].isna().mean() * 100.0) for c in RAW_INPUTS\n",
    "    ],\n",
    "}).sort_values(\"pct_na_pre\", ascending=False)\n",
    "\n",
    "print(\"\\n=== Missingness on raw inputs (before imputation) ===\")\n",
    "display(pre_miss)\n",
    "\n",
    "print(\"\\n=== Distribution snapshot (raw inputs; before imputation) ===\")\n",
    "desc = df_raw_pre.apply(pd.to_numeric, errors=\"coerce\").describe(\n",
    "    percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]\n",
    ").T\n",
    "display(desc)\n",
    "\n",
    "# =============================================================================\n",
    "# 1B. Missing-Data Handling (leakage-aware)\n",
    "# =============================================================================\n",
    "# Goal: minimize mechanical label drift and leakage.\n",
    "#   - Create missingness flags (miss_*) to preserve informative missingness.\n",
    "#   - Construct size_decile from TRAIN distribution of log(assets) to respect scale heterogeneity.\n",
    "#   - Impute raw accounting inputs using TRAIN-only information:\n",
    "#         (i) within-firm lag-1 carryforward (economically plausible)\n",
    "#        (ii) peer medians (training years) by size_decile, with year×size_decile when available\n",
    "#       (iii) KNN imputation (TRAIN-fit) as a final fill for selected balance-sheet items\n",
    "\n",
    "# --- Missingness indicators ---\n",
    "for c in RAW_INPUTS:\n",
    "    df[f\"miss_{c}\"] = df[c].isna().astype(\"int8\")\n",
    "\n",
    "# --- Size deciles (TRAIN-only cutpoints) ---\n",
    "if \"at\" in df.columns:\n",
    "    at = pd.to_numeric(df[\"at\"], errors=\"coerce\")\n",
    "    log_at_raw = pd.Series(np.where(at > 0, np.log(at), np.nan), index=df.index)\n",
    "else:\n",
    "    log_at_raw = pd.Series(np.nan, index=df.index)\n",
    "\n",
    "train_log_at = log_at_raw.loc[train_mask_for_imputation].dropna()\n",
    "\n",
    "if len(train_log_at) >= 200:\n",
    "    try:\n",
    "        _, bins = pd.qcut(train_log_at, q=10, retbins=True, duplicates=\"drop\")\n",
    "        bins = np.unique(bins)\n",
    "        # Ensure open-ended bins for stable assignment\n",
    "        bins[0] = -np.inf\n",
    "        bins[-1] = np.inf\n",
    "        size_decile = pd.cut(log_at_raw, bins=bins, labels=False, include_lowest=True)\n",
    "        df[\"size_decile\"] = (size_decile + 1).astype(\"Int64\").fillna(5).astype(int)\n",
    "    except Exception:\n",
    "        df[\"size_decile\"] = 5\n",
    "else:\n",
    "    df[\"size_decile\"] = 5\n",
    "\n",
    "# --- Step (i): within-firm lag-1 carryforward for selected level variables ---\n",
    "lag_fill_candidates = [\n",
    "    \"at\", \"mkvalt\", \"dlc\", \"dltt\", \"seq\", \"mibt\",\n",
    "    \"oibdp\", \"xint\", \"oancf\", \"capx\", \"che\", \"dv\", \"act\", \"lct\", \"prstkc\",\n",
    "]\n",
    "lag_fill_cols = [c for c in lag_fill_candidates if c in df.columns]\n",
    "\n",
    "for c in lag_fill_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df[c] = df.groupby(\"gvkey\")[c].transform(lambda s: s.fillna(s.shift(1)))\n",
    "\n",
    "# --- Step (ii): peer medians from TRAIN only (size_decile, optionally year×size_decile within TRAIN years) ---\n",
    "def peer_median_impute_inplace(df_in: pd.DataFrame, cols: list[str]) -> None:\n",
    "    tr = df_in.loc[train_mask_for_imputation, [\"fyear\", \"size_decile\"] + cols].copy()\n",
    "    for c in cols:\n",
    "        if c not in df_in.columns:\n",
    "            continue\n",
    "        tr_c = pd.to_numeric(tr[c], errors=\"coerce\")\n",
    "        if tr_c.notna().sum() == 0:\n",
    "            continue\n",
    "\n",
    "        med_global = float(tr_c.median())\n",
    "        med_by_dec = tr.groupby(\"size_decile\")[c].median()\n",
    "        med_by_year_dec = tr.groupby([\"fyear\", \"size_decile\"])[c].median()\n",
    "\n",
    "        miss_idx = df_in.index[df_in[c].isna()]\n",
    "        if len(miss_idx) == 0:\n",
    "            continue\n",
    "\n",
    "        tmp = df_in.loc[miss_idx, [\"fyear\", \"size_decile\"]].copy()\n",
    "        year_dec_key = list(zip(tmp[\"fyear\"].astype(int), tmp[\"size_decile\"].astype(int)))\n",
    "        fill_year_dec = pd.Series(year_dec_key, index=tmp.index).map(med_by_year_dec)\n",
    "        fill_dec = tmp[\"size_decile\"].map(med_by_dec)\n",
    "\n",
    "        fill = fill_year_dec.where(fill_year_dec.notna(), fill_dec)\n",
    "        fill = fill.fillna(med_global)\n",
    "\n",
    "        df_in.loc[miss_idx, c] = fill.values\n",
    "\n",
    "peer_median_cols = [c for c in RAW_INPUTS if c in df.columns]\n",
    "peer_median_impute_inplace(df, peer_median_cols)\n",
    "\n",
    "# --- Step (iii): KNN imputation (TRAIN-fit) as final fill for selected balance-sheet items ---\n",
    "knn_candidates = [\"at\", \"dlc\", \"dltt\", \"che\", \"act\", \"lct\", \"seq\"]\n",
    "knn_cols = [c for c in knn_candidates if c in df.columns]\n",
    "\n",
    "def signed_log1p(x: np.ndarray) -> np.ndarray:\n",
    "    return np.sign(x) * np.log1p(np.abs(x))\n",
    "\n",
    "def signed_expm1(z: np.ndarray) -> np.ndarray:\n",
    "    return np.sign(z) * np.expm1(np.abs(z))\n",
    "\n",
    "if len(knn_cols) >= 2:\n",
    "    X_knn = df[knn_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    X_knn_log = signed_log1p(X_knn.to_numpy(dtype=float))\n",
    "\n",
    "    imputer = KNNImputer(n_neighbors=5, weights=\"distance\")\n",
    "    imputer.fit(X_knn_log[train_mask_for_imputation.values, :])\n",
    "\n",
    "    X_imp = imputer.transform(X_knn_log)\n",
    "    X_imp = signed_expm1(X_imp)\n",
    "    X_imp = pd.DataFrame(X_imp, columns=knn_cols, index=df.index)\n",
    "\n",
    "    for c in knn_cols:\n",
    "        m = df[c].isna()\n",
    "        if int(m.sum()) > 0:\n",
    "            df.loc[m, c] = X_imp.loc[m, c]\n",
    "\n",
    "# --- Imputation impact audit ---\n",
    "df_raw_post = df[RAW_INPUTS].copy()\n",
    "post_miss = pd.DataFrame({\n",
    "    \"col\": RAW_INPUTS,\n",
    "    \"n_na_pre\": [int(df_raw_pre[c].isna().sum()) for c in RAW_INPUTS],\n",
    "    \"n_na_post\": [int(df_raw_post[c].isna().sum()) for c in RAW_INPUTS],\n",
    "    \"pct_na_pre\": [float(df_raw_pre[c].isna().mean() * 100.0) for c in RAW_INPUTS],\n",
    "    \"pct_na_post\": [float(df_raw_post[c].isna().mean() * 100.0) for c in RAW_INPUTS],\n",
    "})\n",
    "post_miss[\"n_filled\"] = post_miss[\"n_na_pre\"] - post_miss[\"n_na_post\"]\n",
    "post_miss[\"pct_filled_of_na\"] = np.where(\n",
    "    post_miss[\"n_na_pre\"] > 0,\n",
    "    100.0 * post_miss[\"n_filled\"] / post_miss[\"n_na_pre\"],\n",
    "    np.nan\n",
    ")\n",
    "post_miss = post_miss.sort_values(\"pct_na_pre\", ascending=False)\n",
    "\n",
    "print(\"\\n=== Missingness AFTER imputation (audit) ===\")\n",
    "display(post_miss)\n",
    "\n",
    "# (Optional) quick visual: top-missing variables before vs after\n",
    "top = post_miss.head(12).copy()\n",
    "plt.figure()\n",
    "plt.barh(top[\"col\"], top[\"pct_na_pre\"], label=\"Pre\")\n",
    "plt.barh(top[\"col\"], top[\"pct_na_post\"], label=\"Post\")\n",
    "plt.xlabel(\"% missing\")\n",
    "plt.title(\"Top missing raw inputs: before vs after imputation\")\n",
    "plt.legend()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 75,005 firm-year observations, 89 variables.\n",
      "Years: 2014–2024\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "2585ef2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:15.335282Z",
     "start_time": "2026-01-04T21:21:15.320289Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# 2. Helper Functions\n",
    "# =============================================================================\n",
    "\n",
    "def safe_divide(a, b):\n",
    "    \"\"\"Numerically stable division with pandas alignment; returns NaN for non-finite results.\"\"\"\n",
    "    a = pd.to_numeric(a, errors=\"coerce\")\n",
    "    b = pd.to_numeric(b, errors=\"coerce\")\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        res = a / b\n",
    "    if isinstance(res, pd.Series):\n",
    "        res = res.replace([np.inf, -np.inf], np.nan)\n",
    "    else:\n",
    "        res = np.where(np.isfinite(res), res, np.nan)\n",
    "    return res\n",
    "\n",
    "def safe_log(x):\n",
    "    \"\"\"log(x) for x>0 else NaN.\"\"\"\n",
    "    x = pd.to_numeric(x, errors=\"coerce\")\n",
    "    out = pd.Series(np.nan, index=x.index, dtype=\"float64\")\n",
    "    m = x > 0\n",
    "    out.loc[m] = np.log(x.loc[m])\n",
    "    return out\n"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "cbb98be7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:15.514169Z",
     "start_time": "2026-01-04T21:21:15.336329Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# 3. Feature Engineering and Label Construction\n",
    "# =============================================================================\n",
    "# Outcome design:\n",
    "#   - distress_dummy(t) is a constructed proxy (high leverage OR negative equity)\n",
    "#   - target_next_year_distress(t) = distress_dummy(t+1) within the same firm\n",
    "#\n",
    "# Measurement guardrail:\n",
    "#   - comparisons are missingness-aware: we do not silently treat NaNs as \"healthy\"\n",
    "#   - for the PROXY ratios only: non-positive denominators are treated as tail states (set to +inf)\n",
    "\n",
    "firm_col = \"gvkey\"\n",
    "\n",
    "# --- Debt and capital components (missingness-aware aggregation) ---\n",
    "dlc  = pd.to_numeric(df.get(\"dlc\", np.nan), errors=\"coerce\")\n",
    "dltt = pd.to_numeric(df.get(\"dltt\", np.nan), errors=\"coerce\")\n",
    "df[\"total_debt\"] = pd.concat([dlc, dltt], axis=1).sum(axis=1, min_count=1)\n",
    "\n",
    "seq  = pd.to_numeric(df.get(\"seq\", np.nan), errors=\"coerce\")\n",
    "mibt = pd.to_numeric(df.get(\"mibt\", np.nan), errors=\"coerce\") if \"mibt\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "df[\"equity_plus_mi_sp\"] = pd.concat([seq, mibt], axis=1).sum(axis=1, min_count=1)\n",
    "\n",
    "df[\"total_capital_sp\"] = df[\"total_debt\"] + df[\"equity_plus_mi_sp\"]\n",
    "\n",
    "# --- Operating inputs ---\n",
    "oibdp = pd.to_numeric(df.get(\"oibdp\", np.nan), errors=\"coerce\")  # EBITDA proxy\n",
    "xint  = pd.to_numeric(df.get(\"xint\", np.nan), errors=\"coerce\")\n",
    "\n",
    "# --- FFO proxy (tax adjustment only where available; avoids hard-coding zeros) ---\n",
    "txt   = pd.to_numeric(df.get(\"txt\", np.nan), errors=\"coerce\") if \"txt\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "txdc  = pd.to_numeric(df.get(\"txdc\", np.nan), errors=\"coerce\") if \"txdc\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "txach = pd.to_numeric(df.get(\"txach\", np.nan), errors=\"coerce\") if \"txach\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "\n",
    "tax_adj = (txt - txdc - txach)\n",
    "ffo_base = oibdp - xint\n",
    "ffo_adj = ffo_base.copy()\n",
    "ffo_adj.loc[tax_adj.notna()] = (ffo_base - tax_adj).loc[tax_adj.notna()]\n",
    "\n",
    "# --- Cash flow capacity ratios (NON-proxy modeling channels) ---\n",
    "oancf = pd.to_numeric(df.get(\"oancf\", np.nan), errors=\"coerce\")\n",
    "capx  = pd.to_numeric(df.get(\"capx\", np.nan), errors=\"coerce\") if \"capx\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "\n",
    "df[\"focf\"] = oancf - capx\n",
    "\n",
    "dv = pd.to_numeric(df.get(\"dv\", np.nan), errors=\"coerce\") if \"dv\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "prstkc = pd.to_numeric(df.get(\"prstkc\", np.nan), errors=\"coerce\") if \"prstkc\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "\n",
    "# Defensive convention: if payout/repurchase is missing, treat as 0 for DCF proxy,\n",
    "# while retaining miss_dv/miss_prstkc indicators from the imputation block.\n",
    "dv0 = dv.fillna(0.0)\n",
    "prstkc0 = prstkc.fillna(0.0)\n",
    "\n",
    "df[\"dcf\"] = df[\"focf\"] - dv0 - prstkc0\n",
    "\n",
    "td = df[\"total_debt\"]\n",
    "td_pos = td.notna() & (td > 0)\n",
    "\n",
    "df[\"sp_cfo_to_debt\"]  = np.where(td_pos, safe_divide(oancf, td), np.nan)\n",
    "df[\"sp_focf_to_debt\"] = np.where(td_pos, safe_divide(df[\"focf\"], td), np.nan)\n",
    "df[\"sp_dcf_to_debt\"]  = np.where(td_pos, safe_divide(df[\"dcf\"], td), np.nan)\n",
    "\n",
    "# --- Size / market variables (explicit inside notebook for reproducibility) ---\n",
    "if \"at\" in df.columns:\n",
    "    df[\"log_at\"] = safe_log(df[\"at\"])\n",
    "if \"mkvalt\" in df.columns:\n",
    "    df[\"log_mkvalt\"] = safe_log(df[\"mkvalt\"])\n",
    "\n",
    "# =============================================================================\n",
    "# Distress proxy (measurement is the outcome; treat non-positive denominators as tail states)\n",
    "# =============================================================================\n",
    "\n",
    "cap = df[\"total_capital_sp\"]\n",
    "\n",
    "# Proxy ratios (tail-handling)\n",
    "ffo_to_debt_pct = pd.Series(np.nan, index=df.index, dtype=\"float64\")\n",
    "m_ffo = td_pos & ffo_adj.notna()\n",
    "ffo_to_debt_pct.loc[m_ffo] = 100.0 * (ffo_adj.loc[m_ffo] / td.loc[m_ffo])\n",
    "\n",
    "debt_to_capital_pct = pd.Series(np.nan, index=df.index, dtype=\"float64\")\n",
    "m_cap_pos = td_pos & cap.notna() & (cap > 0)\n",
    "debt_to_capital_pct.loc[m_cap_pos] = 100.0 * (td.loc[m_cap_pos] / cap.loc[m_cap_pos])\n",
    "m_cap_nonpos = td_pos & cap.notna() & (cap <= 0)\n",
    "debt_to_capital_pct.loc[m_cap_nonpos] = np.inf\n",
    "\n",
    "debt_to_ebitda = pd.Series(np.nan, index=df.index, dtype=\"float64\")\n",
    "m_eb_pos = td_pos & oibdp.notna() & (oibdp > 0)\n",
    "debt_to_ebitda.loc[m_eb_pos] = td.loc[m_eb_pos] / oibdp.loc[m_eb_pos]\n",
    "m_eb_nonpos = td_pos & oibdp.notna() & (oibdp <= 0)\n",
    "debt_to_ebitda.loc[m_eb_nonpos] = np.inf\n",
    "\n",
    "valid_hl = ffo_to_debt_pct.notna() & debt_to_capital_pct.notna() & debt_to_ebitda.notna()\n",
    "\n",
    "hl_ffo = valid_hl & (ffo_to_debt_pct < 15)\n",
    "hl_cap = valid_hl & (debt_to_capital_pct > 55)\n",
    "hl_deb = valid_hl & (debt_to_ebitda > 4.5)\n",
    "\n",
    "is_highly_leveraged = hl_ffo & hl_cap & hl_deb\n",
    "\n",
    "valid_seq = seq.notna()\n",
    "is_equity_negative = valid_seq & (seq < 0)\n",
    "\n",
    "distress = pd.Series(np.nan, index=df.index, dtype=\"float64\")\n",
    "info_mask = valid_hl | valid_seq\n",
    "distress.loc[info_mask] = (is_highly_leveraged | is_equity_negative).loc[info_mask].astype(\"int8\")\n",
    "\n",
    "df[\"distress_dummy\"] = distress  # keep NaN where label is not defensible\n",
    "\n",
    "# Target: next year's distress (panel-safe due to sorting above)\n",
    "df[\"target_next_year_distress\"] = df.groupby(firm_col)[\"distress_dummy\"].shift(-1)\n",
    "\n",
    "# Modeling sample restriction: require defensible current distress AND next-year label\n",
    "df_model = df[df[\"target_next_year_distress\"].notna() & df[\"distress_dummy\"].notna()].copy()\n",
    "df_model[\"target_next_year_distress\"] = df_model[\"target_next_year_distress\"].astype(\"int8\")\n",
    "df_model[\"distress_dummy\"] = df_model[\"distress_dummy\"].astype(\"int8\")\n",
    "\n",
    "print(f\"Modeling sample: {len(df_model):,} firm-years with defensible current distress and next-year labels.\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3A. EDA (post-label): base rates and attrition diagnostics\n",
    "# =============================================================================\n",
    "\n",
    "rate_by_year = df_model.groupby(\"fyear\")[[\"distress_dummy\", \"target_next_year_distress\"]].mean()\n",
    "print(\"\\n=== Base rates by fiscal year (current vs next-year proxy) ===\")\n",
    "display(rate_by_year)\n",
    "\n",
    "if \"size_decile\" in df_model.columns:\n",
    "    rate_by_size = df_model.groupby(\"size_decile\")[[\"distress_dummy\", \"target_next_year_distress\"]].mean()\n",
    "    print(\"\\n=== Base rates by TRAIN-derived size decile ===\")\n",
    "    display(rate_by_size)\n",
    "\n",
    "# Quick distribution check for modeling channels\n",
    "eda_cols = [c for c in [\"sp_cfo_to_debt\", \"sp_focf_to_debt\", \"sp_dcf_to_debt\", \"log_at\", \"log_mkvalt\"] if c in df_model.columns]\n",
    "if len(eda_cols) > 0:\n",
    "    df_model[eda_cols].hist(bins=40, figsize=(12, 6))\n",
    "    plt.suptitle(\"Distributions of key modeling channels (post-imputation/engineering)\")\n",
    "    plt.show()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling sample: 63,599 firm-years with defensible current distress and next-year labels.\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "07d1cfd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:15.612397Z",
     "start_time": "2026-01-04T21:21:15.519678Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# 4. Event Indicators — Interpretable Drivers (NON-proxy channels only)\n",
    "# =============================================================================\n",
    "# Constraint: exclude coverage/leverage/EBITDA-proxy events (anything mechanically embedded in distress_proxy).\n",
    "# We therefore focus on:\n",
    "#   - Dividend policy moments (cuts / suspensions / initiations)\n",
    "#   - Cash-flow shocks (CFO / FOCF)\n",
    "#   - Liquidity deterioration (cash drawdowns; current-ratio squeeze if available)\n",
    "#\n",
    "# Thresholds are calibrated on TRAIN ONLY (<= TRAIN_END_YEAR) to avoid leakage.\n",
    "\n",
    "train_mask = df_model[\"fyear\"] <= TRAIN_END_YEAR\n",
    "\n",
    "# --- Dividend moments ---\n",
    "if \"dv\" in df_model.columns:\n",
    "    dv = pd.to_numeric(df_model[\"dv\"], errors=\"coerce\")\n",
    "    df_model[\"dv_l1\"] = df_model.groupby(\"gvkey\")[\"dv\"].shift(1)\n",
    "\n",
    "    # Among observed payers in TRAIN: calibrate \"cut\" threshold on low percentile of YoY ratio\n",
    "    dv_ratio = safe_divide(dv, df_model[\"dv_l1\"])\n",
    "    valid_dv = df_model[\"dv_l1\"] > 0\n",
    "\n",
    "    cut_q = dv_ratio[train_mask & valid_dv].quantile(0.10)\n",
    "    cut_thr = float(np.clip(cut_q, 0.50, 0.95))  # bounded for stability\n",
    "\n",
    "    df_model[\"evt_div_suspend\"] = (valid_dv & (dv == 0)).astype(\"int8\")\n",
    "    df_model[\"evt_div_cut\"]     = (valid_dv & (dv_ratio < cut_thr) & (dv > 0)).astype(\"int8\")\n",
    "    df_model[\"evt_div_init\"]    = ((df_model[\"dv_l1\"].fillna(0) == 0) & (dv > 0)).astype(\"int8\")\n",
    "else:\n",
    "    cut_thr = 0.75\n",
    "    df_model[\"evt_div_suspend\"] = 0\n",
    "    df_model[\"evt_div_cut\"]     = 0\n",
    "    df_model[\"evt_div_init\"]    = 0\n",
    "\n",
    "# --- CFO shocks ---\n",
    "if \"oancf\" in df_model.columns:\n",
    "    cfo = pd.to_numeric(df_model[\"oancf\"], errors=\"coerce\")\n",
    "    df_model[\"oancf_l1\"] = df_model.groupby(\"gvkey\")[\"oancf\"].shift(1)\n",
    "\n",
    "    df_model[\"evt_cfo_neg\"] = (cfo < 0).astype(\"int8\")\n",
    "\n",
    "    cfo_ratio = safe_divide(cfo, df_model[\"oancf_l1\"])\n",
    "    valid_cfo = df_model[\"oancf_l1\"] > 0\n",
    "    cfo_drop_q = cfo_ratio[train_mask & valid_cfo].quantile(0.05)\n",
    "    cfo_drop_thr = float(np.clip(cfo_drop_q, 0.10, 0.90))\n",
    "\n",
    "    df_model[\"evt_cfo_collapse\"] = (valid_cfo & (cfo_ratio < cfo_drop_thr)).astype(\"int8\")\n",
    "else:\n",
    "    cfo_drop_thr = 0.75\n",
    "    df_model[\"evt_cfo_neg\"] = 0\n",
    "    df_model[\"evt_cfo_collapse\"] = 0\n",
    "\n",
    "# --- FOCF shocks (oancf - capx) ---\n",
    "if \"focf\" in df_model.columns:\n",
    "    focf = pd.to_numeric(df_model[\"focf\"], errors=\"coerce\")\n",
    "    df_model[\"focf_l1\"] = df_model.groupby(\"gvkey\")[\"focf\"].shift(1)\n",
    "\n",
    "    df_model[\"evt_focf_neg\"] = (focf < 0).astype(\"int8\")\n",
    "\n",
    "    focf_ratio = safe_divide(focf, df_model[\"focf_l1\"])\n",
    "    valid_focf = df_model[\"focf_l1\"] > 0\n",
    "    focf_drop_q = focf_ratio[train_mask & valid_focf].quantile(0.05)\n",
    "    focf_drop_thr = float(np.clip(focf_drop_q, 0.10, 0.90))\n",
    "\n",
    "    df_model[\"evt_focf_collapse\"] = (valid_focf & (focf_ratio < focf_drop_thr)).astype(\"int8\")\n",
    "else:\n",
    "    focf_drop_thr = 0.75\n",
    "    df_model[\"evt_focf_neg\"] = 0\n",
    "    df_model[\"evt_focf_collapse\"] = 0\n",
    "\n",
    "# --- Liquidity drawdown: cash drop ---\n",
    "if \"che\" in df_model.columns:\n",
    "    che = pd.to_numeric(df_model[\"che\"], errors=\"coerce\")\n",
    "    df_model[\"che_l1\"] = df_model.groupby(\"gvkey\")[\"che\"].shift(1)\n",
    "\n",
    "    che_ratio = safe_divide(che, df_model[\"che_l1\"])\n",
    "    valid_che = df_model[\"che_l1\"] > 0\n",
    "    che_drop_q = che_ratio[train_mask & valid_che].quantile(0.05)\n",
    "    che_drop_thr = float(np.clip(che_drop_q, 0.10, 0.90))\n",
    "\n",
    "    df_model[\"evt_cash_drawdown\"] = (valid_che & (che_ratio < che_drop_thr)).astype(\"int8\")\n",
    "else:\n",
    "    che_drop_thr = 0.75\n",
    "    df_model[\"evt_cash_drawdown\"] = 0\n",
    "\n",
    "# --- Liquidity squeeze: current ratio deterioration (optional; non-proxy) ---\n",
    "if (\"act\" in df_model.columns) and (\"lct\" in df_model.columns):\n",
    "    act = pd.to_numeric(df_model[\"act\"], errors=\"coerce\")\n",
    "    lct = pd.to_numeric(df_model[\"lct\"], errors=\"coerce\")\n",
    "\n",
    "    df_model[\"current_ratio\"] = safe_divide(act, lct)\n",
    "    df_model[\"current_ratio_l1\"] = df_model.groupby(\"gvkey\")[\"current_ratio\"].shift(1)\n",
    "\n",
    "    cr_ratio = safe_divide(df_model[\"current_ratio\"], df_model[\"current_ratio_l1\"])\n",
    "    valid_cr = df_model[\"current_ratio_l1\"] > 0\n",
    "    cr_drop_q = cr_ratio[train_mask & valid_cr].quantile(0.05)\n",
    "    cr_drop_thr = float(np.clip(cr_drop_q, 0.10, 0.90))\n",
    "\n",
    "    df_model[\"evt_liquidity_squeeze\"] = (valid_cr & (cr_ratio < cr_drop_thr)).astype(\"int8\")\n",
    "else:\n",
    "    cr_drop_thr = 0.75\n",
    "    df_model[\"evt_liquidity_squeeze\"] = 0\n",
    "\n",
    "event_feats = [c for c in df_model.columns if c.startswith(\"evt_\")]\n",
    "print(f\"Event indicators included: {event_feats}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event indicators included: ['evt_div_suspend', 'evt_div_initiate', 'evt_div_cut', 'evt_cfo_neg', 'evt_cfo_collapse', 'evt_focf_neg', 'evt_focf_collapse', 'evt_cash_drawdown']\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "36bb9a99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:15.869134Z",
     "start_time": "2026-01-04T21:21:15.615611Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# 5. Train / Validation / Test Split and Preprocessing\n",
    "# =============================================================================\n",
    "\n",
    "train = df_model[df_model[\"fyear\"] <= TRAIN_END_YEAR].copy()\n",
    "val   = df_model[df_model[\"fyear\"] == VAL_YEAR].copy()\n",
    "test  = df_model[df_model[\"fyear\"] >= TEST_START_YEAR].copy()\n",
    "\n",
    "TARGET_COL = \"target_next_year_distress\"\n",
    "\n",
    "# Core NON-proxy continuous channels (avoid mechanical overlap with distress proxy definition)\n",
    "continuous_feats = [c for c in [\"sp_cfo_to_debt\", \"sp_focf_to_debt\", \"sp_dcf_to_debt\"] if c in df_model.columns]\n",
    "\n",
    "# Market / size controls are explicitly allowed\n",
    "for opt in [\"log_at\", \"log_mkvalt\"]:\n",
    "    if opt in df_model.columns:\n",
    "        continuous_feats.append(opt)\n",
    "\n",
    "event_feats = [c for c in df_model.columns if c.startswith(\"evt_\")]\n",
    "\n",
    "MODEL_FEATS = continuous_feats + event_feats\n",
    "\n",
    "# --- Stabilize continuous inputs: train-fitted clipping + train-median imputation ---\n",
    "WINSOR_LO_Q = 0.01\n",
    "WINSOR_HI_Q = 0.99\n",
    "\n",
    "clip_bounds = {}\n",
    "train_medians = {}\n",
    "\n",
    "for col in continuous_feats:\n",
    "    s = pd.to_numeric(train[col], errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "    lo = float(s.quantile(WINSOR_LO_Q))\n",
    "    hi = float(s.quantile(WINSOR_HI_Q))\n",
    "    clip_bounds[col] = (lo, hi)\n",
    "    train_medians[col] = float(s.median())\n",
    "\n",
    "def clip_and_impute(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_out = df_in.copy()\n",
    "    for col in continuous_feats:\n",
    "        lo, hi = clip_bounds[col]\n",
    "        x = pd.to_numeric(df_out[col], errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "        x = x.clip(lower=lo, upper=hi)\n",
    "        df_out[col] = x.fillna(train_medians[col])\n",
    "    for col in event_feats:\n",
    "        df_out[col] = pd.to_numeric(df_out[col], errors=\"coerce\").fillna(0).astype(\"int8\")\n",
    "    return df_out\n",
    "\n",
    "train = clip_and_impute(train)\n",
    "val   = clip_and_impute(val)\n",
    "test  = clip_and_impute(test)\n",
    "\n",
    "# Standardize continuous features (train statistics)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train[continuous_feats])\n",
    "\n",
    "train.loc[:, continuous_feats] = scaler.transform(train[continuous_feats])\n",
    "val.loc[:, continuous_feats]   = scaler.transform(val[continuous_feats])\n",
    "test.loc[:, continuous_feats]  = scaler.transform(test[continuous_feats])\n",
    "\n",
    "print(f\"Split sizes: train={len(train):,} | val={len(val):,} | test={len(test):,}\")\n",
    "print(f\"Features: {len(MODEL_FEATS)} (continuous={len(continuous_feats)} + events={len(event_feats)})\")\n",
    "print(\"Continuous feats:\", continuous_feats)\n",
    "print(\"Event feats:\", event_feats)\n",
    "\n",
    "# Quick EDA: event prevalence in TRAIN\n",
    "if len(event_feats) > 0:\n",
    "    evt_prev = train[event_feats].mean().sort_values(ascending=False)\n",
    "    print(\"\\n=== Event prevalence (TRAIN) ===\")\n",
    "    display(evt_prev.to_frame(\"train_prevalence\").head(30))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: train=44,780 | val=6,415 | test=12,404\n",
      "Features: 15 (continuous=7 + events=8)\n",
      "Continuous feats: ['sp_debt_to_capital', 'sp_debt_to_ebitda', 'sp_ffo_to_debt', 'sp_cfo_to_debt', 'sp_focf_to_debt', 'log_at', 'log_mkvalt']\n",
      "Event feats: ['evt_div_suspend', 'evt_div_initiate', 'evt_div_cut', 'evt_cfo_neg', 'evt_cfo_collapse', 'evt_focf_neg', 'evt_focf_collapse', 'evt_cash_drawdown']\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "8fc19d35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:15.986169Z",
     "start_time": "2026-01-04T21:21:15.871150Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# 6. Logit Model (Benchmark)\n",
    "# =============================================================================\n",
    "\n",
    "X_train, y_train = train[MODEL_FEATS], train[TARGET_COL].astype(int)\n",
    "X_val, y_val     = val[MODEL_FEATS],   val[TARGET_COL].astype(int)\n",
    "X_test, y_test   = test[MODEL_FEATS],  test[TARGET_COL].astype(int)\n",
    "\n",
    "def evaluate_split(y_true, p_pred):\n",
    "    return pd.Series({\n",
    "        \"AUC\": roc_auc_score(y_true, p_pred),\n",
    "        \"AP\": average_precision_score(y_true, p_pred),\n",
    "        \"Brier\": brier_score_loss(y_true, p_pred),\n",
    "        \"LogLoss\": log_loss(y_true, p_pred),\n",
    "        \"PosRate\": float(np.mean(y_true)),\n",
    "        \"N\": int(len(y_true)),\n",
    "    })\n",
    "\n",
    "# --- Validation-tuned regularization (out-of-time) ---\n",
    "C_grid = [0.01, 0.1, 1.0, 10.0]\n",
    "tune_rows = []\n",
    "\n",
    "for C in C_grid:\n",
    "    clf = LogisticRegression(C=C, max_iter=2000, solver=\"lbfgs\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    p = clf.predict_proba(X_val)[:, 1]\n",
    "    m = evaluate_split(y_val, p)\n",
    "    tune_rows.append(pd.concat([pd.Series({\"C\": C}), m]))\n",
    "\n",
    "tune_df = pd.DataFrame(tune_rows).sort_values(\"AUC\", ascending=False).reset_index(drop=True)\n",
    "print(\"\\n=== Logit tuning (choose by VAL AUC) ===\")\n",
    "display(tune_df)\n",
    "\n",
    "best_C = float(tune_df.loc[0, \"C\"])\n",
    "logit = LogisticRegression(C=best_C, max_iter=2000, solver=\"lbfgs\")\n",
    "logit.fit(X_train, y_train)\n",
    "\n",
    "p_val  = logit.predict_proba(X_val)[:, 1]\n",
    "p_test = logit.predict_proba(X_test)[:, 1]\n",
    "\n",
    "eval_val  = evaluate_split(y_val, p_val)\n",
    "eval_test = evaluate_split(y_test, p_test)\n",
    "\n",
    "print(f\"\\nChosen C={best_C}\")\n",
    "print(\"\\nValidation performance (Logit):\\n\", eval_val.round(4))\n",
    "print(\"\\nTest performance (Logit):\\n\", eval_test.round(4))\n",
    "\n",
    "# --- Coefficient audit (predictive; not causal) ---\n",
    "coef = pd.Series(logit.coef_.ravel(), index=MODEL_FEATS).sort_values(key=lambda s: s.abs(), ascending=False)\n",
    "print(\"\\nTop |coefficients| (Logit):\")\n",
    "display(coef.head(25).to_frame(\"coef\"))\n",
    "\n",
    "# --- Inference audit via statsmodels Logit with firm-clustered SEs ---\n",
    "# Purpose: descriptive stability check under within-firm dependence (NOT causal inference).\n",
    "try:\n",
    "    X_sm = sm.add_constant(train[MODEL_FEATS], has_constant=\"add\")\n",
    "    res = sm.Logit(y_train, X_sm).fit(disp=False, maxiter=200)\n",
    "    res_cl = res.get_robustcov_results(cov_type=\"cluster\", groups=train[\"gvkey\"])\n",
    "    summ = res_cl.summary2().tables[1].copy()\n",
    "    # Keep a compact view for the notebook\n",
    "    keep_cols = [c for c in [\"Coef.\", \"Std.Err.\", \"z\", \"P>|z|\"] if c in summ.columns]\n",
    "    print(\"\\n=== Statsmodels logit (firm-clustered SE) — coefficient table ===\")\n",
    "    display(summ[keep_cols].sort_values(\"Coef.\", key=lambda s: s.abs(), ascending=False).head(30))\n",
    "except Exception as e:\n",
    "    print(\"Statsmodels inference audit skipped (convergence/collinearity). Error:\", repr(e))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation performance (Logit):\n",
      " AUC           0.6953\n",
      "AP            0.3729\n",
      "Brier         0.1260\n",
      "LogLoss       0.4123\n",
      "PosRate       0.1668\n",
      "N          6415.0000\n",
      "dtype: float64\n",
      "Test performance (Logit):\n",
      " AUC            0.6885\n",
      "AP             0.3725\n",
      "Brier          0.1365\n",
      "LogLoss        0.4384\n",
      "PosRate        0.1831\n",
      "N          12404.0000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "cfc47a7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:17.284760Z",
     "start_time": "2026-01-04T21:21:15.988224Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# 7. Tree-based Model (XGBoost)\n",
    "# =============================================================================\n",
    "\n",
    "xgb_params = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"aucpr\",\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    n_estimators=500,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(**xgb_params)\n",
    "xgb_clf.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "p_val_xgb = xgb_clf.predict_proba(X_val)[:, 1]\n",
    "p_test_xgb = xgb_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "eval_val_xgb = evaluate_split(y_val, p_val_xgb)\n",
    "eval_test_xgb = evaluate_split(y_test, p_test_xgb)\n",
    "\n",
    "print(\"Validation performance (XGB):\\n\", eval_val_xgb.round(4))\n",
    "print(\"Test performance (XGB):\\n\", eval_test_xgb.round(4))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation performance (XGB):\n",
      " AUC           0.9068\n",
      "AP            0.7292\n",
      "Brier         0.0767\n",
      "LogLoss       0.2603\n",
      "PosRate       0.1668\n",
      "N          6415.0000\n",
      "dtype: float64\n",
      "Test performance (XGB):\n",
      " AUC            0.9112\n",
      "AP             0.7540\n",
      "Brier          0.0788\n",
      "LogLoss        0.2671\n",
      "PosRate        0.1831\n",
      "N          12404.0000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "419eee4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:18.282903Z",
     "start_time": "2026-01-04T21:21:17.380987Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# 8. Explainability (TreeSHAP via XGBoost pred_contribs)\n",
    "# =============================================================================\n",
    "\n",
    "booster = xgb_clf.get_booster()\n",
    "dval = xgb.DMatrix(X_val, feature_names=MODEL_FEATS)\n",
    "\n",
    "# pred_contribs=True returns SHAP contributions per feature plus a bias term (last column)\n",
    "shap_val = booster.predict(dval, pred_contribs=True)\n",
    "shap_cols = MODEL_FEATS + [\"bias\"]\n",
    "shap_df = pd.DataFrame(shap_val, columns=shap_cols)\n",
    "\n",
    "abs_mean = shap_df[MODEL_FEATS].abs().mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"Mean absolute SHAP contributions (validation):\")\n",
    "display(abs_mean.head(20))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute SHAP contributions (validation):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sp_debt_to_capital    1.133036\n",
       "sp_ffo_to_debt        0.527658\n",
       "log_at                0.275150\n",
       "sp_debt_to_ebitda     0.186086\n",
       "log_mkvalt            0.145138\n",
       "sp_focf_to_debt       0.142577\n",
       "sp_cfo_to_debt        0.136545\n",
       "evt_cfo_neg           0.071492\n",
       "evt_focf_neg          0.054369\n",
       "evt_cfo_collapse      0.019340\n",
       "evt_cash_drawdown     0.014455\n",
       "evt_div_cut           0.007996\n",
       "evt_focf_collapse     0.005306\n",
       "evt_div_suspend       0.003284\n",
       "evt_div_initiate      0.002192\n",
       "dtype: float32"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "dbd538d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:18.787694Z",
     "start_time": "2026-01-04T21:21:18.386795Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# 9. Evaluation: Persistence vs. Early Warning\n",
    "# =============================================================================\n",
    "\n",
    "eps = 1e-3\n",
    "p_persist_val  = np.clip(val[\"distress_dummy\"].astype(float),  eps, 1 - eps)\n",
    "p_persist_test = np.clip(test[\"distress_dummy\"].astype(float), eps, 1 - eps)\n",
    "\n",
    "benchmarks = pd.DataFrame([\n",
    "    [\"VAL\",  \"Persistence\", *evaluate_split(y_val,  p_persist_val)],\n",
    "    [\"VAL\",  \"Logit\",       *eval_val],\n",
    "    [\"VAL\",  \"XGB\",         *eval_val_xgb],\n",
    "    [\"TEST\", \"Persistence\", *evaluate_split(y_test, p_persist_test)],\n",
    "    [\"TEST\", \"Logit\",       *eval_test],\n",
    "    [\"TEST\", \"XGB\",         *eval_test_xgb],\n",
    "], columns=[\"Split\", \"Model\", \"AUC\", \"AP\", \"Brier\", \"LogLoss\", \"PosRate\", \"N\"])\n",
    "\n",
    "display(benchmarks)\n",
    "\n",
    "# Early-warning subset: not distressed at t but distressed at t+1 (0→1 transitions)\n",
    "val_ew  = (val[\"distress_dummy\"] == 0) & (val[TARGET_COL] == 1)\n",
    "test_ew = (test[\"distress_dummy\"] == 0) & (test[TARGET_COL] == 1)\n",
    "\n",
    "def ew_summary(mask, p_pred, label):\n",
    "    mask = mask.fillna(False)\n",
    "    if int(mask.sum()) == 0:\n",
    "        return pd.Series({\"Subset\": label, \"N\": 0, \"MeanPD\": np.nan, \"MedianPD\": np.nan})\n",
    "    return pd.Series({\n",
    "        \"Subset\": label,\n",
    "        \"N\": int(mask.sum()),\n",
    "        \"MeanPD\": float(np.mean(p_pred[mask.values])),\n",
    "        \"MedianPD\": float(np.median(p_pred[mask.values])),\n",
    "    })\n",
    "\n",
    "ew = pd.DataFrame([\n",
    "    ew_summary(val_ew,  p_val,      \"VAL: 0→1 transitions (Logit PD)\"),\n",
    "    ew_summary(val_ew,  p_val_xgb,  \"VAL: 0→1 transitions (XGB PD)\"),\n",
    "    ew_summary(test_ew, p_test,     \"TEST: 0→1 transitions (Logit PD)\"),\n",
    "    ew_summary(test_ew, p_test_xgb, \"TEST: 0→1 transitions (XGB PD)\"),\n",
    "])\n",
    "\n",
    "print(\"\\n=== Early-warning PD levels on 0→1 transitions ===\")\n",
    "display(ew)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Split        Model       AUC        AP     Brier   LogLoss   PosRate  \\\n",
       "0   VAL  Persistence  0.806236  0.528628  0.100968  0.699751  0.166797   \n",
       "1   VAL        Logit  0.695316  0.372922  0.126003  0.412337  0.166797   \n",
       "2   VAL          XGB  0.906799  0.729185  0.076717  0.260257  0.166797   \n",
       "3  TEST  Persistence  0.816405  0.569372  0.099930  0.692567  0.183086   \n",
       "4  TEST        Logit  0.688549  0.372482  0.136455  0.438371  0.183086   \n",
       "5  TEST          XGB  0.911201  0.754015  0.078791  0.267070  0.183086   \n",
       "\n",
       "         N  \n",
       "0   6415.0  \n",
       "1   6415.0  \n",
       "2   6415.0  \n",
       "3  12404.0  \n",
       "4  12404.0  \n",
       "5  12404.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Split</th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC</th>\n",
       "      <th>AP</th>\n",
       "      <th>Brier</th>\n",
       "      <th>LogLoss</th>\n",
       "      <th>PosRate</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VAL</td>\n",
       "      <td>Persistence</td>\n",
       "      <td>0.806236</td>\n",
       "      <td>0.528628</td>\n",
       "      <td>0.100968</td>\n",
       "      <td>0.699751</td>\n",
       "      <td>0.166797</td>\n",
       "      <td>6415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VAL</td>\n",
       "      <td>Logit</td>\n",
       "      <td>0.695316</td>\n",
       "      <td>0.372922</td>\n",
       "      <td>0.126003</td>\n",
       "      <td>0.412337</td>\n",
       "      <td>0.166797</td>\n",
       "      <td>6415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VAL</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.906799</td>\n",
       "      <td>0.729185</td>\n",
       "      <td>0.076717</td>\n",
       "      <td>0.260257</td>\n",
       "      <td>0.166797</td>\n",
       "      <td>6415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST</td>\n",
       "      <td>Persistence</td>\n",
       "      <td>0.816405</td>\n",
       "      <td>0.569372</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.692567</td>\n",
       "      <td>0.183086</td>\n",
       "      <td>12404.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST</td>\n",
       "      <td>Logit</td>\n",
       "      <td>0.688549</td>\n",
       "      <td>0.372482</td>\n",
       "      <td>0.136455</td>\n",
       "      <td>0.438371</td>\n",
       "      <td>0.183086</td>\n",
       "      <td>12404.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TEST</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.911201</td>\n",
       "      <td>0.754015</td>\n",
       "      <td>0.078791</td>\n",
       "      <td>0.267070</td>\n",
       "      <td>0.183086</td>\n",
       "      <td>12404.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "                           Subset    N    MeanPD  MedianPD\n",
       "0   VAL: 0→1 transitions (XGB PD)  356  0.183557  0.139482\n",
       "1  TEST: 0→1 transitions (XGB PD)  716  0.203632  0.162034"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subset</th>\n",
       "      <th>N</th>\n",
       "      <th>MeanPD</th>\n",
       "      <th>MedianPD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VAL: 0→1 transitions (XGB PD)</td>\n",
       "      <td>356</td>\n",
       "      <td>0.183557</td>\n",
       "      <td>0.139482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST: 0→1 transitions (XGB PD)</td>\n",
       "      <td>716</td>\n",
       "      <td>0.203632</td>\n",
       "      <td>0.162034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "fa9c9eda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:19.160416Z",
     "start_time": "2026-01-04T21:21:18.850641Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# 10. Decision Support and Scenario Analysis (non-proxy shocks)\n",
    "# =============================================================================\n",
    "# Requirements:\n",
    "#   - No deleveraging / leverage-ratio shocks (proxy-mechanical)\n",
    "#   - No coverage shocks\n",
    "#   - Scenarios must propagate through engineered features and event indicators\n",
    "#\n",
    "# We implement \"primitive shocks\" (dv, oancf, capx, che) and RECOMPUTE:\n",
    "#   - continuous ratios (sp_cfo_to_debt, focf, sp_focf_to_debt, dcf, sp_dcf_to_debt)\n",
    "#   - event indicators that depend on current vs lag values (stored in *_l1 columns)\n",
    "\n",
    "EVENT_THRESHOLDS = {\n",
    "    \"div_cut_thr\": float(locals().get(\"cut_thr\", 0.75)),\n",
    "    \"cfo_drop_thr\": float(locals().get(\"cfo_drop_thr\", 0.75)),\n",
    "    \"focf_drop_thr\": float(locals().get(\"focf_drop_thr\", 0.75)),\n",
    "    \"cash_drop_thr\": float(locals().get(\"che_drop_thr\", 0.75)),\n",
    "    \"cr_drop_thr\": float(locals().get(\"cr_drop_thr\", 0.75)),\n",
    "}\n",
    "\n",
    "def recompute_features_for_rows(df_rows: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df_rows.copy()\n",
    "\n",
    "    # Recompute flows\n",
    "    out[\"oancf\"] = pd.to_numeric(out.get(\"oancf\"), errors=\"coerce\")\n",
    "    out[\"capx\"]  = pd.to_numeric(out.get(\"capx\"), errors=\"coerce\") if \"capx\" in out.columns else np.nan\n",
    "    out[\"focf\"]  = out[\"oancf\"] - out[\"capx\"]\n",
    "\n",
    "    dv = pd.to_numeric(out.get(\"dv\"), errors=\"coerce\") if \"dv\" in out.columns else pd.Series(np.nan, index=out.index)\n",
    "    prstkc = pd.to_numeric(out.get(\"prstkc\"), errors=\"coerce\") if \"prstkc\" in out.columns else pd.Series(np.nan, index=out.index)\n",
    "    dv0 = dv.fillna(0.0)\n",
    "    prstkc0 = prstkc.fillna(0.0)\n",
    "    out[\"dcf\"] = out[\"focf\"] - dv0 - prstkc0\n",
    "\n",
    "    # Ratios (NON-proxy channels)\n",
    "    td = pd.to_numeric(out.get(\"total_debt\"), errors=\"coerce\")\n",
    "    td_pos = td.notna() & (td > 0)\n",
    "    out[\"sp_cfo_to_debt\"]  = np.where(td_pos, safe_divide(out[\"oancf\"], td), np.nan)\n",
    "    out[\"sp_focf_to_debt\"] = np.where(td_pos, safe_divide(out[\"focf\"], td), np.nan)\n",
    "    out[\"sp_dcf_to_debt\"]  = np.where(td_pos, safe_divide(out[\"dcf\"], td), np.nan)\n",
    "\n",
    "    # Dividend events (need dv_l1)\n",
    "    if \"dv\" in out.columns and \"dv_l1\" in out.columns:\n",
    "        dv_l1 = pd.to_numeric(out[\"dv_l1\"], errors=\"coerce\")\n",
    "        dv_ratio = safe_divide(dv, dv_l1)\n",
    "        valid_dv = dv_l1 > 0\n",
    "\n",
    "        out[\"evt_div_suspend\"] = (valid_dv & (dv == 0)).astype(\"int8\")\n",
    "        out[\"evt_div_cut\"]     = (valid_dv & (dv_ratio < EVENT_THRESHOLDS[\"div_cut_thr\"]) & (dv > 0)).astype(\"int8\")\n",
    "        out[\"evt_div_init\"]    = ((dv_l1.fillna(0) == 0) & (dv > 0)).astype(\"int8\")\n",
    "\n",
    "    # CFO events (need oancf_l1)\n",
    "    if \"oancf_l1\" in out.columns:\n",
    "        cfo_l1 = pd.to_numeric(out[\"oancf_l1\"], errors=\"coerce\")\n",
    "        cfo_ratio = safe_divide(out[\"oancf\"], cfo_l1)\n",
    "        valid_cfo = cfo_l1 > 0\n",
    "\n",
    "        out[\"evt_cfo_neg\"] = (out[\"oancf\"] < 0).astype(\"int8\")\n",
    "        out[\"evt_cfo_collapse\"] = (valid_cfo & (cfo_ratio < EVENT_THRESHOLDS[\"cfo_drop_thr\"])).astype(\"int8\")\n",
    "\n",
    "    # FOCF events (need focf_l1)\n",
    "    if \"focf_l1\" in out.columns:\n",
    "        focf_l1 = pd.to_numeric(out[\"focf_l1\"], errors=\"coerce\")\n",
    "        focf_ratio = safe_divide(out[\"focf\"], focf_l1)\n",
    "        valid_focf = focf_l1 > 0\n",
    "\n",
    "        out[\"evt_focf_neg\"] = (out[\"focf\"] < 0).astype(\"int8\")\n",
    "        out[\"evt_focf_collapse\"] = (valid_focf & (focf_ratio < EVENT_THRESHOLDS[\"focf_drop_thr\"])).astype(\"int8\")\n",
    "\n",
    "    # Cash drawdown (need che_l1)\n",
    "    if \"che\" in out.columns and \"che_l1\" in out.columns:\n",
    "        che = pd.to_numeric(out[\"che\"], errors=\"coerce\")\n",
    "        che_l1 = pd.to_numeric(out[\"che_l1\"], errors=\"coerce\")\n",
    "        che_ratio = safe_divide(che, che_l1)\n",
    "        valid_che = che_l1 > 0\n",
    "        out[\"evt_cash_drawdown\"] = (valid_che & (che_ratio < EVENT_THRESHOLDS[\"cash_drop_thr\"])).astype(\"int8\")\n",
    "\n",
    "    # Liquidity squeeze (current ratio) if present\n",
    "    if (\"act\" in out.columns) and (\"lct\" in out.columns) and (\"current_ratio_l1\" in out.columns):\n",
    "        act = pd.to_numeric(out[\"act\"], errors=\"coerce\")\n",
    "        lct = pd.to_numeric(out[\"lct\"], errors=\"coerce\")\n",
    "        out[\"current_ratio\"] = safe_divide(act, lct)\n",
    "        cr_ratio = safe_divide(out[\"current_ratio\"], pd.to_numeric(out[\"current_ratio_l1\"], errors=\"coerce\"))\n",
    "        valid_cr = pd.to_numeric(out[\"current_ratio_l1\"], errors=\"coerce\") > 0\n",
    "        out[\"evt_liquidity_squeeze\"] = (valid_cr & (cr_ratio < EVENT_THRESHOLDS[\"cr_drop_thr\"])).astype(\"int8\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# Choose a representative case: highest XGB PD in TEST\n",
    "test_pd = pd.Series(p_test_xgb, index=test.index, name=\"pd_xgb\")\n",
    "base_idx = test_pd.idxmax()\n",
    "base_row = test.loc[base_idx].copy()\n",
    "base_pd = float(test_pd.loc[base_idx])\n",
    "\n",
    "print(f\"Base case (TEST) index={base_idx} | PD={base_pd:.4f}\")\n",
    "\n",
    "scenarios = {\n",
    "    \"Base\": {},\n",
    "    \"Dividend suspension (dv=0)\": {\"dv\": 0.0} if \"dv\" in test.columns else {},\n",
    "    \"CFO shock (-30%)\": {\"oancf\": float(base_row.get(\"oancf\", np.nan)) * 0.70} if \"oancf\" in test.columns else {},\n",
    "    \"Capex surge (+25%)\": {\"capx\": float(base_row.get(\"capx\", np.nan)) * 1.25} if \"capx\" in test.columns else {},\n",
    "    \"Cash drawdown (-20%)\": {\"che\": float(base_row.get(\"che\", np.nan)) * 0.80} if \"che\" in test.columns else {},\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, adj in scenarios.items():\n",
    "    row_s = base_row.copy()\n",
    "    for k, v in adj.items():\n",
    "        row_s[k] = v\n",
    "\n",
    "    tmp = pd.DataFrame([row_s])\n",
    "    tmp = recompute_features_for_rows(tmp)\n",
    "\n",
    "    # Apply the same preprocessing pipeline as training\n",
    "    tmp = clip_and_impute(tmp)\n",
    "    tmp.loc[:, continuous_feats] = scaler.transform(tmp[continuous_feats])\n",
    "\n",
    "    x_in = tmp[MODEL_FEATS]\n",
    "    pd_s = float(xgb_clf.predict_proba(x_in)[:, 1][0])\n",
    "    results.append((name, pd_s, pd_s - base_pd))\n",
    "\n",
    "pd_results = pd.DataFrame(results, columns=[\"Scenario\", \"PD\", \"ΔPD\"]).sort_values(\"PD\", ascending=False)\n",
    "display(pd_results)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                     Scenario        PD       ΔPD\n",
       "0                        Base  0.194363  0.139428\n",
       "1  Dividend suspension (dv=0)  0.194363  0.139428\n",
       "2            CFO shock (-30%)  0.195867  0.140932\n",
       "3          Capex surge (+25%)  0.191345  0.136410\n",
       "4        Cash drawdown (-20%)  0.194363  0.139428"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scenario</th>\n",
       "      <th>PD</th>\n",
       "      <th>ΔPD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Base</td>\n",
       "      <td>0.194363</td>\n",
       "      <td>0.139428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dividend suspension (dv=0)</td>\n",
       "      <td>0.194363</td>\n",
       "      <td>0.139428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CFO shock (-30%)</td>\n",
       "      <td>0.195867</td>\n",
       "      <td>0.140932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Capex surge (+25%)</td>\n",
       "      <td>0.191345</td>\n",
       "      <td>0.136410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cash drawdown (-20%)</td>\n",
       "      <td>0.194363</td>\n",
       "      <td>0.139428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 35
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}