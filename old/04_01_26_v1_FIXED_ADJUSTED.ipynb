{
 "cells": [
  {
   "cell_type": "code",
   "id": "7e0ff41063681d9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T08:14:08.221214Z",
     "start_time": "2026-01-05T08:14:06.444873Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, average_precision_score, confusion_matrix, classification_report\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# =============================================================================\n",
    "# Project: Next-year financial distress prediction (firm-year panel)\n",
    "# Data Science Lifecycle focus in this cell:\n",
    "#   - Setup & configuration (temporal split conventions, leakage control parameters)\n",
    "#   - Panel integrity (types, deduplication, stable firm identifier)\n",
    "#   - Define label_year used consistently for splitting and target alignment\n",
    "# =============================================================================\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration (split & preprocessing parameters)\n",
    "# ----------------------------\n",
    "FILE_NAME = \"data.csv\"\n",
    "\n",
    "TRAIN_CUTOFF_LABEL_YEAR = 2022   # label_year <= cutoff → train/val pool; after cutoff → test\n",
    "VAL_YEARS = 1                    # last N years within the pool are validation\n",
    "N_SPLITS_TIME_CV = 4             # rolling time-based folds for sanity checks\n",
    "\n",
    "WINSOR_LOWER_Q = 0.01            # winsorization lower quantile (train-only)\n",
    "WINSOR_UPPER_Q = 0.99            # winsorization upper quantile (train-only)\n",
    "\n",
    "REQUIRED_KEYS = [\"gvkey\", \"fyear\"]\n",
    "TARGET_COL = \"target_next_year_distress\"\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Utilities (robust numeric ops for ratios)\n",
    "# ----------------------------\n",
    "\n",
    "def to_float_numpy(x) -> np.ndarray:\n",
    "    \"\"\"Convert series/array-like to float numpy array, coercing non-numeric to NaN.\"\"\"\n",
    "    s = pd.to_numeric(x, errors=\"coerce\")\n",
    "    return s.to_numpy(dtype=float) if hasattr(s, \"to_numpy\") else np.asarray(s, dtype=float)\n",
    "def safe_divide(a, b) -> np.ndarray:\n",
    "    \"\"\"Elementwise divide a/b with NaN when division is invalid (0 or non-finite).\"\"\"\n",
    "    a = to_float_numpy(a)\n",
    "    b = to_float_numpy(b)\n",
    "    out = np.full_like(a, np.nan, dtype=float)\n",
    "    np.divide(a, b, out=out, where=(b != 0) & np.isfinite(a) & np.isfinite(b))\n",
    "    return out\n",
    "\n",
    "def rolling_year_folds(\n",
    "    df_in: pd.DataFrame, year_col: str = \"label_year\", n_splits: int = 4, min_train_years: int = 3\n",
    ") -> list[tuple[np.ndarray, np.ndarray, np.ndarray, int]]:\n",
    "    \"\"\"\n",
    "    Create expanding-window time folds:\n",
    "      train years: first (min_train_years + k) years\n",
    "      val year:    next year\n",
    "    Returns: list of (train_idx, val_idx, train_years, val_year)\n",
    "    \"\"\"\n",
    "    years_sorted = np.sort(df_in[year_col].dropna().unique())\n",
    "    if len(years_sorted) <= min_train_years:\n",
    "        return []\n",
    "    n_splits = min(n_splits, len(years_sorted) - min_train_years)\n",
    "\n",
    "    folds_out = []\n",
    "    for k in range(n_splits):\n",
    "        train_years = years_sorted[: min_train_years + k]\n",
    "        val_year = int(years_sorted[min_train_years + k])\n",
    "\n",
    "        train_idx = df_in.index[df_in[year_col].isin(train_years)].to_numpy()\n",
    "        val_idx = df_in.index[df_in[year_col] == val_year].to_numpy()\n",
    "        folds_out.append((train_idx, val_idx, train_years, val_year))\n",
    "\n",
    "    return folds_out\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) Data acquisition & panel hygiene\n",
    "# =============================================================================\n",
    "df = pd.read_csv(FILE_NAME, low_memory=False)\n",
    "\n",
    "# Convert datadate if present\n",
    "if \"datadate\" in df.columns:\n",
    "    df[\"datadate\"] = pd.to_datetime(df[\"datadate\"], errors=\"coerce\")\n",
    "\n",
    "# Create stable firm id + de-duplicate firm-year (keep last record)\n",
    "df[\"firm_id\"] = df[\"gvkey\"]\n",
    "df = (\n",
    "    df.sort_values([\"firm_id\", \"fyear\"])\n",
    "      .drop_duplicates(subset=[\"firm_id\", \"fyear\"], keep=\"last\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Label year: predict distress in the next fiscal year\n",
    "df[\"label_year\"] = df[\"fyear\"] + 1\n",
    "\n",
    "# =============================================================================\n",
    "# 2) Split scaffolding (define train/val pool years via label_year)\n",
    "# =============================================================================\n",
    "pool_mask = df[\"label_year\"] <= TRAIN_CUTOFF_LABEL_YEAR\n",
    "pool_years = np.sort(df.loc[pool_mask, \"label_year\"].dropna().unique())\n",
    "val_years = pool_years[-VAL_YEARS:] if len(pool_years) else np.array([], dtype=int)\n",
    "\n",
    "# This mask is ONLY used for imputations (train-only information)\n",
    "train_mask_for_imputation = pool_mask & (~df[\"label_year\"].isin(val_years))\n"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "b33657b33f838671",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-01-05T08:14:08.236647Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# Data Cleaning & Missing-Data Handling (leakage-aware)\n",
    "# Purpose:\n",
    "#   - Quantify missingness and distribution properties before intervention\n",
    "#   - Preserve informative missingness via miss_* indicators\n",
    "#   - Impute financial statement inputs using TRAIN-only information:\n",
    "#       (1) within-firm past values (lag-1) where economically sensible\n",
    "#       (2) peer medians by year×size_decile on size-scaled ratios (ratio space)\n",
    "#       (3) KNN imputation (TRAIN-fit) for selected core balance-sheet items\n",
    "#   - Re-run EDA after imputation to audit how strongly imputations alter the data\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "RAW_INPUTS_FOR_FE = [\n",
    "    \"aco\",\"act\",\"ao\",\"aoloch\",\"ap\",\"apalch\",\"aqc\",\"at\",\"caps\",\"capx\",\"ceq\",\"che\",\"chech\",\"csho\",\"cstk\",\"cstke\",\n",
    "    \"datadate\",\"dlc\",\"dlcch\",\"dltis\",\"dltr\",\"dltt\",\"do\",\"dp\",\"dpc\",\"dv\",\"dvc\",\"dvp\",\"dvt\",\"esubc\",\"exre\",\n",
    "    \"fiao\",\"fincf\",\"fopo\",\"fyear\",\"gvkey\",\"ib\",\"ibadj\",\"ibc\",\"intan\",\"invch\",\"invt\",\"ismod\",\"ivaco\",\"ivaeq\",\n",
    "    \"ivao\",\"ivch\",\"ivncf\",\"ivstch\",\"lco\",\"lct\",\"lt\",\"mibt\",\"mkvalt\",\"niadj\",\"nopi\",\"oancf\",\"oibdp\",\"ppent\",\n",
    "    \"prcc_c\",\"prcc_f\",\"prstkc\",\"pstk\",\"pstkn\",\"pstkr\",\"re\",\"recch\",\"rect\",\"seq\",\"siv\",\"spi\",\"sppe\",\"sppiv\",\n",
    "    \"sstk\",\"tstk\",\"txach\",\"txbcof\",\"txdc\",\"txditc\",\"txp\",\"txt\",\"xi\",\"xido\",\"xidoc\",\"xint\",\n",
    "    # optional identifiers present in many extracts:\n",
    "    \"conm\",\"consol\",\"datafmt\",\"indfmt\",\n",
    "]\n",
    "raw = [c for c in RAW_INPUTS_FOR_FE if c in df.columns]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.0 Ensure keys exist + types\n",
    "# ---------------------------------------------------------------------------\n",
    "if \"firm_id\" not in df.columns:\n",
    "    if \"gvkey\" in df.columns:\n",
    "        df[\"firm_id\"] = df[\"gvkey\"]\n",
    "    else:\n",
    "        raise ValueError(\"Need either firm_id or gvkey in df to run panel imputations.\")\n",
    "\n",
    "if \"fyear\" in df.columns:\n",
    "    df[\"fyear\"] = pd.to_numeric(df[\"fyear\"], errors=\"coerce\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.1 Drop rows with missing critical identifiers (do not impute these)\n",
    "# ---------------------------------------------------------------------------\n",
    "NON_IMPUTE_DROP = [c for c in [\"gvkey\", \"datadate\", \"fyear\", \"conm\", \"datafmt\", \"indfmt\", \"consol\"] if c in df.columns]\n",
    "if NON_IMPUTE_DROP:\n",
    "    before_n = df.shape[0]\n",
    "    df = df.dropna(subset=NON_IMPUTE_DROP).copy()\n",
    "    after_n = df.shape[0]\n",
    "    if after_n < before_n:\n",
    "        print(f\"[INFO] Dropped {before_n - after_n:,} rows due to missing non-imputable ID/meta fields: {NON_IMPUTE_DROP}\")\n",
    "\n",
    "# Ensure train mask aligns after drops\n",
    "if isinstance(train_mask_for_imputation, pd.Series):\n",
    "    train_mask_for_imputation = train_mask_for_imputation.reindex(df.index).fillna(False).astype(bool)\n",
    "\n",
    "# Rebuild raw after potential drop\n",
    "raw = [c for c in RAW_INPUTS_FOR_FE if c in df.columns]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.2 EDA BEFORE imputation (missingness + distribution snapshot)\n",
    "# ---------------------------------------------------------------------------\n",
    "df_raw_pre = df[raw].copy(deep=True)\n",
    "\n",
    "pre_miss = pd.DataFrame(\n",
    "    {\n",
    "        \"col\": raw,\n",
    "        \"n\": [int(df_raw_pre[c].shape[0]) for c in raw],\n",
    "        \"n_na_pre\": [int(df_raw_pre[c].isna().sum()) for c in raw],\n",
    "        \"pct_na_pre\": [float(df_raw_pre[c].isna().mean() * 100.0) for c in raw],\n",
    "        \"train_n\": [int(train_mask_for_imputation.sum()) for _ in raw],\n",
    "        \"train_pct_na_pre\": [\n",
    "            float(df_raw_pre.loc[train_mask_for_imputation, c].isna().mean() * 100.0) for c in raw\n",
    "        ],\n",
    "    }\n",
    ").sort_values(\"pct_na_pre\", ascending=False)\n",
    "\n",
    "print(\"\\n=== EDA (BEFORE imputation): Missingness on raw inputs ===\")\n",
    "print(pre_miss.round(4).head(50))\n",
    "\n",
    "# Numeric distribution summary (coerce non-numeric to NaN)\n",
    "if raw:\n",
    "    x_pre = df_raw_pre[raw].apply(pd.to_numeric, errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "    q_pre = x_pre.quantile([0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99]).T\n",
    "    pre_dist = pd.DataFrame(\n",
    "        {\n",
    "            \"n_nonmiss_pre\": x_pre.notna().sum(),\n",
    "            \"mean_pre\": x_pre.mean(),\n",
    "            \"std_pre\": x_pre.std(ddof=0),\n",
    "            \"min_pre\": x_pre.min(),\n",
    "            \"p01_pre\": q_pre[0.01],\n",
    "            \"p05_pre\": q_pre[0.05],\n",
    "            \"p25_pre\": q_pre[0.25],\n",
    "            \"p50_pre\": q_pre[0.50],\n",
    "            \"p75_pre\": q_pre[0.75],\n",
    "            \"p95_pre\": q_pre[0.95],\n",
    "            \"p99_pre\": q_pre[0.99],\n",
    "            \"max_pre\": x_pre.max(),\n",
    "        }\n",
    "    )\n",
    "    print(\"\\n=== EDA (BEFORE imputation): Distribution summary (raw inputs) ===\")\n",
    "    print(pre_dist.round(4).sort_values(\"n_nonmiss_pre\", ascending=True).head(50))\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.3 Missingness flags (ALWAYS create before imputations)\n",
    "# ---------------------------------------------------------------------------\n",
    "for c in raw:\n",
    "    df[f\"miss_{c}\"] = df[c].isna().astype(\"int8\")\n",
    "\n",
    "# Helper: originally-observed indicator\n",
    "def _obs(col: str) -> pd.Series:\n",
    "    m = f\"miss_{col}\"\n",
    "    if m in df.columns:\n",
    "        return (df[m] == 0)\n",
    "    return df[col].notna()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.4 Grouping key for peer-based imputations (no industry used)\n",
    "#   - Use TRAIN-derived size_decile to avoid mixing microcaps with mega-caps\n",
    "# ---------------------------------------------------------------------------\n",
    "df = df.sort_values([\"firm_id\", \"fyear\"]).copy()\n",
    "\n",
    "# TRAIN-derived size deciles on log(at) using observed TRAIN values only\n",
    "if \"at\" in df.columns:\n",
    "    at_train = pd.to_numeric(df.loc[train_mask_for_imputation, \"at\"], errors=\"coerce\")\n",
    "    log_at_train = np.log(at_train.where(at_train > 0))\n",
    "    qs = np.linspace(0, 1, 11)\n",
    "    edges = np.unique(np.nanquantile(log_at_train.dropna(), qs))\n",
    "    if edges.size >= 3:\n",
    "        log_at_all = np.log(pd.to_numeric(df[\"at\"], errors=\"coerce\").where(pd.to_numeric(df[\"at\"], errors=\"coerce\") > 0))\n",
    "        df[\"size_decile\"] = pd.cut(log_at_all, bins=edges, include_lowest=True, labels=False)\n",
    "    else:\n",
    "        df[\"size_decile\"] = np.nan\n",
    "else:\n",
    "    df[\"size_decile\"] = np.nan\n",
    "\n",
    "group_cols = [\"fyear\", \"size_decile\"]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.5 Step 1: Construct / reconcile FIRST (no leakage: contemporaneous or lag only)\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.5.1 Mild lag-1 fill for prices, then mkvalt = prcc_f * csho if missing\n",
    "for px in [\"prcc_f\", \"prcc_c\"]:\n",
    "    if px in df.columns:\n",
    "        df[px] = df[px].where(df[px].notna(), df.groupby(\"firm_id\")[px].shift(1))\n",
    "\n",
    "if all(c in df.columns for c in [\"mkvalt\", \"prcc_f\", \"csho\"]):\n",
    "    mkvalt_miss = df[\"mkvalt\"].isna()\n",
    "    mkvalt_calc = pd.to_numeric(df[\"prcc_f\"], errors=\"coerce\") * pd.to_numeric(df[\"csho\"], errors=\"coerce\")\n",
    "    df.loc[mkvalt_miss & mkvalt_calc.notna(), \"mkvalt\"] = mkvalt_calc.loc[mkvalt_miss & mkvalt_calc.notna()]\n",
    "\n",
    "# 3.5.2 Reconstruct change variables from level differences (fill only if change var is missing)\n",
    "def _fill_change_from_levels(change_col, level_col):\n",
    "    if change_col in df.columns and level_col in df.columns:\n",
    "        miss = df[change_col].isna()\n",
    "        lvl = pd.to_numeric(df[level_col], errors=\"coerce\")\n",
    "        lag_lvl = pd.to_numeric(df.groupby(\"firm_id\")[level_col].shift(1), errors=\"coerce\")\n",
    "        recon = lvl - lag_lvl\n",
    "        df.loc[miss & recon.notna(), change_col] = recon.loc[miss & recon.notna()]\n",
    "\n",
    "_fill_change_from_levels(\"dlcch\", \"dlc\")\n",
    "_fill_change_from_levels(\"recch\", \"rect\")\n",
    "_fill_change_from_levels(\"invch\", \"invt\")\n",
    "_fill_change_from_levels(\"chech\", \"che\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.6 Sparse zero-fill for structurally zero-inflated items (TRAIN-validated)\n",
    "#   - Keeps miss_* as informative signal\n",
    "# ---------------------------------------------------------------------------\n",
    "SPARSE_CANDIDATES = [c for c in [\n",
    "    \"txach\",\"txdc\",\"txbcof\",\"spi\",\"siv\",\"sppiv\",\"ivstch\",\"sppe\",\"esubc\",\"dvp\",\"dv\",\"dvc\",\"dvt\"\n",
    "] if c in df.columns]\n",
    "\n",
    "def _zero_fill_if_sparse(col: str, zero_share_thresh: float = 0.70, min_obs: int = 1000) -> bool:\n",
    "    mflag = f\"miss_{col}\"\n",
    "    if mflag not in df.columns:\n",
    "        return False\n",
    "    obs_mask = train_mask_for_imputation & (df[mflag] == 0)\n",
    "    s = pd.to_numeric(df.loc[obs_mask, col], errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "    if s.notna().sum() < min_obs:\n",
    "        return False\n",
    "    zero_share = float((s == 0).mean())\n",
    "    med = float(s.median()) if s.notna().any() else np.nan\n",
    "    if np.isfinite(med) and (med == 0.0) and (zero_share >= zero_share_thresh):\n",
    "        df.loc[df[col].isna(), col] = 0.0\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "for c in SPARSE_CANDIDATES:\n",
    "    _zero_fill_if_sparse(c)\n",
    "\n",
    "# Explicit “tax components -> 0” rule (FFO proxy safety)\n",
    "for c in [\"txt\", \"txdc\", \"txach\"]:\n",
    "    if c in df.columns:\n",
    "        df.loc[df[c].isna(), c] = 0.0\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.7 Step 3: Stocks — lag-1 fill -> peer median of ratio (x/at) with ratio-space clipping\n",
    "# ---------------------------------------------------------------------------\n",
    "STOCKS = [c for c in [\n",
    "    \"aco\",\"act\",\"ao\",\"ap\",\"at\",\"caps\",\"ceq\",\"che\",\"csho\",\"cstk\",\"dlc\",\"dltt\",\"intan\",\"invt\",\"lco\",\"lct\",\"lt\",\n",
    "    \"mibt\",\"ppent\",\"pstk\",\"pstkn\",\"pstkr\",\"re\",\"rect\",\"seq\",\"tstk\",\"ivaeq\",\"mkvalt\"\n",
    "] if c in df.columns]\n",
    "\n",
    "# lag-1 fill (past-only)\n",
    "if STOCKS:\n",
    "    lag1 = df.groupby(\"firm_id\")[STOCKS].shift(1)\n",
    "    df[STOCKS] = df[STOCKS].where(df[STOCKS].notna(), lag1)\n",
    "\n",
    "# non-negativity only where economically necessary\n",
    "NONNEG_STOCKS = set([c for c in STOCKS if c in {\n",
    "    \"aco\",\"act\",\"ao\",\"ap\",\"at\",\"caps\",\"che\",\"csho\",\"cstk\",\"dlc\",\"dltt\",\"intan\",\"invt\",\"lco\",\"lct\",\"lt\",\n",
    "    \"mibt\",\"mkvalt\",\"ppent\",\"pstk\",\"pstkn\",\"pstkr\",\"rect\",\"tstk\",\"ivaeq\"\n",
    "}])\n",
    "\n",
    "def _fit_ratio_stats(train_df: pd.DataFrame, col: str, base: str = \"at\", qlo=0.01, qhi=0.99):\n",
    "    mcol = f\"miss_{col}\"\n",
    "    mbase = f\"miss_{base}\"\n",
    "    s = pd.to_numeric(train_df[col], errors=\"coerce\")\n",
    "    b = pd.to_numeric(train_df[base], errors=\"coerce\") if base in train_df.columns else None\n",
    "\n",
    "    obs_s = (train_df[mcol] == 0) if mcol in train_df.columns else s.notna()\n",
    "\n",
    "    if b is None:\n",
    "        tr = train_df.loc[obs_s, group_cols + [col]].copy()\n",
    "        tr[col] = pd.to_numeric(tr[col], errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "        grp = tr.groupby(group_cols)[col].median()\n",
    "        overall = float(tr[col].median()) if tr[col].notna().any() else 0.0\n",
    "        return (\"level\", overall, grp, np.nan, np.nan)\n",
    "\n",
    "    obs_b = (train_df[mbase] == 0) if mbase in train_df.columns else b.notna()\n",
    "    valid = obs_s & obs_b & s.notna() & b.notna() & (b > 0)\n",
    "\n",
    "    if valid.sum() < 200:\n",
    "        tr = train_df.loc[obs_s, group_cols + [col]].copy()\n",
    "        tr[col] = pd.to_numeric(tr[col], errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "        grp = tr.groupby(group_cols)[col].median()\n",
    "        overall = float(tr[col].median()) if tr[col].notna().any() else 0.0\n",
    "        return (\"level\", overall, grp, np.nan, np.nan)\n",
    "\n",
    "    ratio = (s[valid] / b[valid]).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    overall = float(ratio.median()) if ratio.notna().any() else 0.0\n",
    "    r_lo = float(ratio.quantile(qlo))\n",
    "    r_hi = float(ratio.quantile(qhi))\n",
    "\n",
    "    tmp = train_df.loc[valid, group_cols].copy()\n",
    "    tmp[\"_ratio_\"] = ratio.values\n",
    "    grp = tmp.groupby(group_cols)[\"_ratio_\"].median()\n",
    "    return (\"ratio\", overall, grp, r_lo, r_hi)\n",
    "\n",
    "def _apply_ratio_stats(df_all: pd.DataFrame, col: str, fit_obj, base: str = \"at\", nonneg: bool = False):\n",
    "    kind, overall, grp, r_lo, r_hi = fit_obj\n",
    "    miss = df_all[col].isna()\n",
    "    if not miss.any():\n",
    "        return\n",
    "\n",
    "    if kind == \"ratio\" and base in df_all.columns:\n",
    "        b = pd.to_numeric(df_all.loc[miss, base], errors=\"coerce\")\n",
    "        g = df_all.loc[miss, group_cols]\n",
    "\n",
    "        if len(group_cols) == 1:\n",
    "            mapped = g[group_cols[0]].map(grp)\n",
    "        else:\n",
    "            keys = list(map(tuple, g[group_cols].to_numpy()))\n",
    "            mapped = pd.Series(keys, index=g.index).map(grp)\n",
    "\n",
    "        r = pd.to_numeric(mapped, errors=\"coerce\").replace([np.inf, -np.inf], np.nan).fillna(overall)\n",
    "\n",
    "        # clip in ratio space (TRAIN-observed band)\n",
    "        if np.isfinite(r_lo) and np.isfinite(r_hi) and (r_lo < r_hi):\n",
    "            r = r.clip(r_lo, r_hi)\n",
    "\n",
    "        fill = r * b\n",
    "        fill = fill.where(b.notna() & (b > 0), np.nan)\n",
    "        df_all.loc[miss & fill.notna(), col] = fill.loc[miss & fill.notna()].to_numpy()\n",
    "\n",
    "    # fallback: group level medians (TRAIN-fit, observed-only)\n",
    "    miss2 = df_all[col].isna()\n",
    "    if miss2.any():\n",
    "        mflag = f\"miss_{col}\"\n",
    "        obs_train_mask = train_mask_for_imputation\n",
    "        if mflag in df_all.columns:\n",
    "            obs_train_mask = obs_train_mask & (df_all[mflag] == 0)\n",
    "\n",
    "        tr = df_all.loc[obs_train_mask, group_cols + [col]].copy()\n",
    "        tr[col] = pd.to_numeric(tr[col], errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "        lvl_overall = float(tr[col].median()) if tr[col].notna().any() else 0.0\n",
    "        lvl_grp = tr.groupby(group_cols)[col].median()\n",
    "\n",
    "        g2 = df_all.loc[miss2, group_cols]\n",
    "        if len(group_cols) == 1:\n",
    "            mapped2 = g2[group_cols[0]].map(lvl_grp)\n",
    "        else:\n",
    "            keys2 = list(map(tuple, g2[group_cols].to_numpy()))\n",
    "            mapped2 = pd.Series(keys2, index=g2.index).map(lvl_grp)\n",
    "\n",
    "        fill2 = pd.to_numeric(mapped2, errors=\"coerce\").replace([np.inf, -np.inf], np.nan).fillna(lvl_overall)\n",
    "        if nonneg:\n",
    "            fill2 = fill2.clip(lower=0.0)\n",
    "        df_all.loc[miss2, col] = fill2.to_numpy()\n",
    "\n",
    "# Fit on training\n",
    "tr_all = df.loc[train_mask_for_imputation].copy()\n",
    "stock_fits = {c: _fit_ratio_stats(tr_all, c, base=\"at\") for c in STOCKS}\n",
    "\n",
    "# Apply to full df\n",
    "for c in STOCKS:\n",
    "    _apply_ratio_stats(df, c, stock_fits[c], base=\"at\", nonneg=(c in NONNEG_STOCKS))\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.8 Step 4: Flows / income variables — ratio-median imputation (leakage-aware)\n",
    "# ---------------------------------------------------------------------------\n",
    "FLOWS = [c for c in [\"ib\",\"ibadj\",\"ibc\",\"niadj\",\"nopi\",\"oibdp\",\"dp\",\"oancf\",\"fincf\",\"ivncf\",\"xint\"] if c in df.columns]\n",
    "\n",
    "# Debt-aware base for xint\n",
    "if \"xint\" in df.columns and all(c in df.columns for c in [\"dlc\", \"dltt\"]):\n",
    "    df[\"_td_for_xint\"] = (\n",
    "        pd.to_numeric(df[\"dlc\"], errors=\"coerce\").fillna(0.0)\n",
    "        + pd.to_numeric(df[\"dltt\"], errors=\"coerce\").fillna(0.0)\n",
    "    )\n",
    "else:\n",
    "    df[\"_td_for_xint\"] = np.nan\n",
    "\n",
    "# If total debt <= 0 and xint missing -> 0\n",
    "if \"xint\" in df.columns and \"_td_for_xint\" in df.columns:\n",
    "    td = pd.to_numeric(df[\"_td_for_xint\"], errors=\"coerce\").fillna(0.0)\n",
    "    df.loc[df[\"xint\"].isna() & (td <= 0), \"xint\"] = 0.0\n",
    "\n",
    "flow_fits = {}\n",
    "for c in FLOWS:\n",
    "    base = \"_td_for_xint\" if (c == \"xint\" and \"_td_for_xint\" in df.columns) else \"at\"\n",
    "    flow_fits[c] = _fit_ratio_stats(tr_all, c, base=base)\n",
    "\n",
    "for c in FLOWS:\n",
    "    base = \"_td_for_xint\" if (c == \"xint\" and \"_td_for_xint\" in df.columns) else \"at\"\n",
    "    _apply_ratio_stats(df, c, flow_fits[c], base=base, nonneg=False)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.9 KNN imputation (TRAIN-fit) for selected core balance-sheet items\n",
    "#   - Applies ONLY to originally-missing rows (miss_* == 1)\n",
    "#   - Uses a robust signed-log transform for magnitudes\n",
    "# ---------------------------------------------------------------------------\n",
    "KNN_TARGETS = [c for c in [\n",
    "    \"at\",\"act\",\"lct\",\"lt\",\"seq\",\"dlc\",\"dltt\",\"che\",\"ppent\",\"rect\",\"invt\",\"re\",\"ceq\",\"caps\"\n",
    "] if c in df.columns]\n",
    "\n",
    "# Non-negativity for clearly non-negative magnitudes (do NOT force seq/ceq/re to be non-negative)\n",
    "KNN_NONNEG = set([c for c in KNN_TARGETS if c in {\"at\",\"act\",\"lct\",\"lt\",\"dlc\",\"dltt\",\"che\",\"ppent\",\"rect\",\"invt\",\"caps\"}])\n",
    "\n",
    "if KNN_TARGETS:\n",
    "    # KNN feature set (kept compact and available in your raw inputs)\n",
    "    knn_feats = []\n",
    "    for c in [\"fyear\", \"size_decile\", \"mkvalt\", \"csho\", \"prcc_f\", \"prcc_c\"]:\n",
    "        if c in df.columns:\n",
    "            knn_feats.append(c)\n",
    "    knn_feats = list(dict.fromkeys(knn_feats))  # preserve order, unique\n",
    "\n",
    "    knn_cols = list(dict.fromkeys(knn_feats + KNN_TARGETS))\n",
    "    X = df[knn_cols].apply(pd.to_numeric, errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # For KNN, re-instate NaN only for the targets that were originally missing\n",
    "    for t in KNN_TARGETS:\n",
    "        mflag = f\"miss_{t}\"\n",
    "        if mflag in df.columns:\n",
    "            X.loc[df[mflag] == 1, t] = np.nan\n",
    "\n",
    "    # Fill size_decile NaN with TRAIN median decile for distance stability\n",
    "    if \"size_decile\" in X.columns:\n",
    "        sd_train = X.loc[train_mask_for_imputation, \"size_decile\"]\n",
    "        sd_fill = float(sd_train.median()) if sd_train.notna().any() else 5.0\n",
    "        X[\"size_decile\"] = X[\"size_decile\"].fillna(sd_fill)\n",
    "\n",
    "    # Robust transform: signed log1p for magnitudes; keep fyear/size_decile in levels\n",
    "    def _signed_log1p(a: pd.Series) -> pd.Series:\n",
    "        v = pd.to_numeric(a, errors=\"coerce\")\n",
    "        return np.sign(v) * np.log1p(np.abs(v))\n",
    "\n",
    "    Z = X.copy()\n",
    "    for c in Z.columns:\n",
    "        if c not in {\"fyear\", \"size_decile\"}:\n",
    "            Z[c] = _signed_log1p(Z[c])\n",
    "\n",
    "    # Standardize using TRAIN-only stats (nan-safe)\n",
    "    mu = Z.loc[train_mask_for_imputation].mean(skipna=True)\n",
    "    sd = Z.loc[train_mask_for_imputation].std(skipna=True, ddof=0).replace(0.0, 1.0)\n",
    "    sd = sd.fillna(1.0)\n",
    "    mu = mu.fillna(0.0)\n",
    "\n",
    "    Zs = (Z - mu) / sd\n",
    "\n",
    "    # Fit KNN on TRAIN only; transform all rows\n",
    "    imputer = KNNImputer(n_neighbors=25, weights=\"distance\", metric=\"nan_euclidean\")\n",
    "    imputer.fit(Zs.loc[train_mask_for_imputation].to_numpy(dtype=float))\n",
    "    Zs_imp = imputer.transform(Zs.to_numpy(dtype=float))\n",
    "    Zs_imp = pd.DataFrame(Zs_imp, index=Zs.index, columns=Zs.columns)\n",
    "\n",
    "    # Unscale\n",
    "    Z_imp = Zs_imp * sd + mu\n",
    "\n",
    "    # Invert signed log for magnitudes\n",
    "    X_imp = X.copy()\n",
    "    for c in Z_imp.columns:\n",
    "        if c in {\"fyear\", \"size_decile\"}:\n",
    "            X_imp[c] = Z_imp[c]\n",
    "        else:\n",
    "            v = pd.to_numeric(Z_imp[c], errors=\"coerce\")\n",
    "            X_imp[c] = np.sign(v) * (np.expm1(np.abs(v)))\n",
    "\n",
    "    # Write back ONLY for originally-missing targets\n",
    "    for t in KNN_TARGETS:\n",
    "        mflag = f\"miss_{t}\"\n",
    "        if mflag in df.columns:\n",
    "            miss_mask = (df[mflag] == 1)\n",
    "            df.loc[miss_mask, t] = pd.to_numeric(X_imp.loc[miss_mask, t], errors=\"coerce\").to_numpy()\n",
    "\n",
    "            if t in KNN_NONNEG:\n",
    "                df.loc[miss_mask, t] = pd.to_numeric(df.loc[miss_mask, t], errors=\"coerce\").clip(lower=0.0)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.10 Change variables AFTER level imputations:\n",
    "#   - Fill only if change is missing\n",
    "#   - Reconstruct only when both levels were originally observed; otherwise impute change directly\n",
    "# ---------------------------------------------------------------------------\n",
    "def _fill_change_after_levels(change_col, level_col):\n",
    "    if change_col in df.columns and level_col in df.columns:\n",
    "        lvl = pd.to_numeric(df[level_col], errors=\"coerce\")\n",
    "        lag_lvl = pd.to_numeric(df.groupby(\"firm_id\")[level_col].shift(1), errors=\"coerce\")\n",
    "        recon = lvl - lag_lvl\n",
    "\n",
    "        miss_now = df[change_col].isna()\n",
    "\n",
    "        # only reconstruct when both levels were originally observed\n",
    "        obs_now = _obs(level_col)\n",
    "        obs_lag = obs_now.groupby(df[\"firm_id\"]).shift(1).fillna(False)\n",
    "        can_recon = miss_now & obs_now & obs_lag & recon.notna()\n",
    "        df.loc[can_recon, change_col] = recon.loc[can_recon]\n",
    "\n",
    "        # remaining missing: direct group-median impute of change (TRAIN-fit, observed-only if possible)\n",
    "        miss2 = df[change_col].isna()\n",
    "        if miss2.any():\n",
    "            mflag = f\"miss_{change_col}\"\n",
    "            obs_train = train_mask_for_imputation\n",
    "            if mflag in df.columns:\n",
    "                obs_train = obs_train & (df[mflag] == 0)\n",
    "\n",
    "            tr = df.loc[obs_train, group_cols + [change_col]].copy()\n",
    "            tr[change_col] = pd.to_numeric(tr[change_col], errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "            overall = float(tr[change_col].median()) if tr[change_col].notna().any() else 0.0\n",
    "            grp = tr.groupby(group_cols)[change_col].median()\n",
    "\n",
    "            g = df.loc[miss2, group_cols]\n",
    "            if len(group_cols) == 1:\n",
    "                mapped = g[group_cols[0]].map(grp)\n",
    "            else:\n",
    "                keys = list(map(tuple, g[group_cols].to_numpy()))\n",
    "                mapped = pd.Series(keys, index=g.index).map(grp)\n",
    "\n",
    "            fill = pd.to_numeric(mapped, errors=\"coerce\").replace([np.inf, -np.inf], np.nan).fillna(overall)\n",
    "            df.loc[miss2, change_col] = fill.to_numpy()\n",
    "\n",
    "_fill_change_after_levels(\"dlcch\", \"dlc\")\n",
    "_fill_change_after_levels(\"recch\", \"rect\")\n",
    "_fill_change_after_levels(\"invch\", \"invt\")\n",
    "_fill_change_after_levels(\"chech\", \"che\")\n",
    "if \"apalch\" in df.columns and \"ap\" in df.columns:\n",
    "    _fill_change_after_levels(\"apalch\", \"ap\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.11 Guardrail: cap ONLY-imputed values (miss_* == 1) to TRAIN-observed quantile band\n",
    "# ---------------------------------------------------------------------------\n",
    "REG_NONNEG = set([\"at\",\"act\",\"lct\",\"lt\",\"dlc\",\"dltt\",\"che\",\"ppent\",\"rect\",\"invt\",\"caps\"])  # for this script\n",
    "\n",
    "def _cap_imputed_to_train_quantiles(\n",
    "    df_all: pd.DataFrame,\n",
    "    col: str,\n",
    "    lower_q: float = 0.01,\n",
    "    upper_q: float = 0.99,\n",
    "    nonneg: bool = False,\n",
    ") -> None:\n",
    "    flag = f\"miss_{col}\"\n",
    "    if col not in df_all.columns or flag not in df_all.columns:\n",
    "        return\n",
    "\n",
    "    miss_mask = df_all[flag].astype(bool)\n",
    "    if not miss_mask.any():\n",
    "        return\n",
    "\n",
    "    # bounds from TRAIN & originally-observed values\n",
    "    obs = pd.to_numeric(df_all.loc[train_mask_for_imputation & (~miss_mask), col], errors=\"coerce\").replace(\n",
    "        [np.inf, -np.inf], np.nan\n",
    "    )\n",
    "    if obs.notna().sum() < 200:\n",
    "        obs = pd.to_numeric(df_all.loc[train_mask_for_imputation, col], errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if obs.notna().sum() == 0:\n",
    "        return\n",
    "\n",
    "    lo = float(obs.quantile(lower_q))\n",
    "    hi = float(obs.quantile(upper_q))\n",
    "\n",
    "    if nonneg:\n",
    "        lo = max(0.0, lo) if np.isfinite(lo) else 0.0\n",
    "\n",
    "    s = pd.to_numeric(df_all.loc[miss_mask, col], errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "    s = s.clip(lo, hi)\n",
    "    if nonneg:\n",
    "        s = s.clip(lower=0.0)\n",
    "    df_all.loc[miss_mask, col] = s.to_numpy()\n",
    "\n",
    "CAP_COLS = sorted(\n",
    "    (\n",
    "        set(STOCKS + FLOWS + [\"dlcch\", \"apalch\", \"recch\", \"invch\", \"chech\"])\n",
    "        | set(KNN_TARGETS)\n",
    "    ) & set(df.columns)\n",
    ")\n",
    "\n",
    "for c in CAP_COLS:\n",
    "    _cap_imputed_to_train_quantiles(\n",
    "        df,\n",
    "        c,\n",
    "        lower_q=0.01,\n",
    "        upper_q=0.99,\n",
    "        nonneg=(c in NONNEG_STOCKS) or (c in REG_NONNEG),\n",
    "    )\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.12 EDA AFTER imputation (missingness reduction + distribution deltas)\n",
    "# ---------------------------------------------------------------------------\n",
    "df_raw_post = df[raw].copy(deep=True)\n",
    "\n",
    "post_miss = pd.DataFrame(\n",
    "    {\n",
    "        \"col\": raw,\n",
    "        \"n_na_post\": [int(df_raw_post[c].isna().sum()) for c in raw],\n",
    "        \"pct_na_post\": [float(df_raw_post[c].isna().mean() * 100.0) for c in raw],\n",
    "        \"train_pct_na_post\": [\n",
    "            float(df_raw_post.loc[train_mask_for_imputation, c].isna().mean() * 100.0) for c in raw\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "changes = pre_miss.merge(post_miss, on=\"col\", how=\"left\")\n",
    "changes[\"n_imputed\"] = changes[\"n_na_pre\"] - changes[\"n_na_post\"]\n",
    "changes[\"pct_points_na_reduction\"] = changes[\"pct_na_pre\"] - changes[\"pct_na_post\"]\n",
    "\n",
    "x_post = df_raw_post[raw].apply(pd.to_numeric, errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "q_post = x_post.quantile([0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99]).T\n",
    "post_dist = pd.DataFrame(\n",
    "    {\n",
    "        \"n_nonmiss_post\": x_post.notna().sum(),\n",
    "        \"mean_post\": x_post.mean(),\n",
    "        \"std_post\": x_post.std(ddof=0),\n",
    "        \"min_post\": x_post.min(),\n",
    "        \"p01_post\": q_post[0.01],\n",
    "        \"p05_post\": q_post[0.05],\n",
    "        \"p25_post\": q_post[0.25],\n",
    "        \"p50_post\": q_post[0.50],\n",
    "        \"p75_post\": q_post[0.75],\n",
    "        \"p95_post\": q_post[0.95],\n",
    "        \"p99_post\": q_post[0.99],\n",
    "        \"max_post\": x_post.max(),\n",
    "    }\n",
    ")\n",
    "\n",
    "pre_dist_key = pre_dist[[\"n_nonmiss_pre\",\"mean_pre\",\"std_pre\",\"p01_pre\",\"p50_pre\",\"p99_pre\"]].copy() if raw else pd.DataFrame()\n",
    "post_dist_key = post_dist[[\"n_nonmiss_post\",\"mean_post\",\"std_post\",\"p01_post\",\"p50_post\",\"p99_post\"]].copy() if raw else pd.DataFrame()\n",
    "\n",
    "dist_delta = pre_dist_key.join(post_dist_key, how=\"outer\")\n",
    "dist_delta[\"delta_mean\"] = dist_delta[\"mean_post\"] - dist_delta[\"mean_pre\"]\n",
    "dist_delta[\"delta_std\"]  = dist_delta[\"std_post\"]  - dist_delta[\"std_pre\"]\n",
    "dist_delta[\"delta_p50\"]  = dist_delta[\"p50_post\"]  - dist_delta[\"p50_pre\"]\n",
    "\n",
    "# Imputed-only diagnostics (based on original missingness in df_raw_pre)\n",
    "rows = []\n",
    "for c in raw:\n",
    "    pre_na_mask = df_raw_pre[c].isna()\n",
    "    n_imp = int(pre_na_mask.sum())\n",
    "    if n_imp == 0:\n",
    "        rows.append((c, 0, np.nan, np.nan, np.nan, np.nan, np.nan))\n",
    "        continue\n",
    "\n",
    "    imp_vals = pd.to_numeric(df_raw_post.loc[pre_na_mask, c], errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "    obs_vals = pd.to_numeric(df_raw_pre.loc[~pre_na_mask, c], errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    rows.append(\n",
    "        (\n",
    "            c,\n",
    "            n_imp,\n",
    "            float(imp_vals.mean()) if imp_vals.notna().any() else np.nan,\n",
    "            float(imp_vals.median()) if imp_vals.notna().any() else np.nan,\n",
    "            float(imp_vals.std(ddof=0)) if imp_vals.notna().any() else np.nan,\n",
    "            float(obs_vals.mean()) if obs_vals.notna().any() else np.nan,\n",
    "            float(obs_vals.median()) if obs_vals.notna().any() else np.nan,\n",
    "        )\n",
    "    )\n",
    "\n",
    "imputed_only = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\"col\",\"n_imputed\",\"imputed_mean\",\"imputed_median\",\"imputed_std\",\"observed_mean_pre\",\"observed_median_pre\"],\n",
    ").set_index(\"col\")\n",
    "\n",
    "print(\"\\n=== EDA (AFTER imputation): Missingness on raw inputs + change ===\")\n",
    "cols_show = [\n",
    "    \"col\", \"n\", \"n_na_pre\", \"pct_na_pre\", \"n_na_post\", \"pct_na_post\",\n",
    "    \"n_imputed\", \"pct_points_na_reduction\", \"train_pct_na_pre\", \"train_pct_na_post\",\n",
    "]\n",
    "print(\n",
    "    changes[cols_show]\n",
    "    .sort_values([\"n_imputed\",\"pct_points_na_reduction\"], ascending=[False, False])\n",
    "    .round(4)\n",
    "    .head(50)\n",
    ")\n",
    "\n",
    "print(\"\\n=== Change analysis: Distribution deltas (post - pre) on raw inputs ===\")\n",
    "print(\n",
    "    dist_delta[[\"n_nonmiss_pre\",\"n_nonmiss_post\",\"delta_mean\",\"delta_std\",\"delta_p50\"]]\n",
    "    .sort_values(\"delta_mean\", key=lambda s: s.abs(), ascending=False)\n",
    "    .round(6)\n",
    "    .head(50)\n",
    ")\n",
    "\n",
    "print(\"\\n=== Change analysis: Imputed-only vs observed (pre) summary ===\")\n",
    "print(\n",
    "    imputed_only.assign(\n",
    "        mean_gap_imputed_minus_observed=lambda d: d[\"imputed_mean\"] - d[\"observed_mean_pre\"],\n",
    "        median_gap_imputed_minus_observed=lambda d: d[\"imputed_median\"] - d[\"observed_median_pre\"],\n",
    "    )\n",
    "    .sort_values(\"n_imputed\", ascending=False)\n",
    "    .round(6)\n",
    "    .head(50)\n",
    ")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EDA (BEFORE imputation): Missingness on raw inputs ===\n",
      "       col      n  n_na_pre  pct_na_pre  train_n  train_pct_na_pre\n",
      "18   dlcch  75005     33143     44.1877    48458           42.9630\n",
      "5   apalch  75005     30371     40.4920    48458           39.0214\n",
      "75   txach  75005     22791     30.3860    48458           29.2501\n",
      "48  ivstch  75005     19194     25.5903    48458           23.0282\n",
      "66   recch  75005     12589     16.7842    48458           16.5938\n",
      "53  mkvalt  75005     12350     16.4656    48458           17.0189\n",
      "71    sppe  75005     12239     16.3176    48458           16.4307\n",
      "1      act  75005     10721     14.2937    48458           14.6581\n",
      "50     lct  75005     10695     14.2590    48458           14.5982\n",
      "84    xint  75005     10536     14.0471    48458           13.9234\n",
      "78  txditc  75005      9069     12.0912    48458           12.2044\n",
      "79     txp  75005      7942     10.5886    48458           10.6917\n",
      "29   esubc  75005      6798      9.0634    48458            8.8964\n",
      "49     lco  75005      6779      9.0381    48458            9.3277\n",
      "0      aco  75005      6778      9.0367    48458            9.3277\n",
      "60  prcc_f  75005      5956      7.9408    48458            8.8902\n",
      "59  prcc_c  75005      5942      7.9221    48458            8.8819\n",
      "40   invch  75005      5315      7.0862    48458            7.3445\n",
      "72   sppiv  75005      3967      5.2890    48458            4.9631\n",
      "44   ivaeq  75005      3548      4.7304    48458            4.8826\n",
      "61  prstkc  75005      3410      4.5464    48458            4.9486\n",
      "8     caps  75005      3074      4.0984    48458            4.2655\n",
      "6      aqc  75005      2812      3.7491    48458            3.2647\n",
      "23      dp  75005      2722      3.6291    48458            3.7909\n",
      "65      re  75005      2596      3.4611    48458            3.6650\n",
      "46    ivch  75005      2540      3.3864    48458            3.3844\n",
      "45    ivao  75005      2529      3.3718    48458            3.1821\n",
      "77    txdc  75005      2472      3.2958    48458            3.4050\n",
      "69     siv  75005      2299      3.0651    48458            3.1285\n",
      "57   oibdp  75005      2142      2.8558    48458            2.9366\n",
      "14    cstk  75005      1925      2.5665    48458            3.0294\n",
      "24     dpc  75005      1757      2.3425    48458            2.4268\n",
      "19   dltis  75005      1756      2.3412    48458            2.4681\n",
      "58   ppent  75005      1723      2.2972    48458            2.3814\n",
      "73    sstk  75005      1436      1.9145    48458            1.8593\n",
      "20    dltr  75005      1388      1.8505    48458            2.0616\n",
      "25      dv  75005      1137      1.5159    48458            1.5911\n",
      "70     spi  75005      1126      1.5012    48458            1.3785\n",
      "67    rect  75005       880      1.1733    48458            1.1247\n",
      "52    mibt  75005       842      1.1226    48458            1.3022\n",
      "13    csho  75005       804      1.0719    48458            1.2939\n",
      "28     dvt  75005       706      0.9413    48458            1.1412\n",
      "26     dvc  75005       703      0.9373    48458            1.1350\n",
      "39   intan  75005       654      0.8719    48458            0.8977\n",
      "41    invt  75005       652      0.8693    48458            0.8255\n",
      "76  txbcof  75005       619      0.8253    48458            1.0071\n",
      "9     capx  75005       503      0.6706    48458            0.7429\n",
      "83   xidoc  75005       477      0.6360    48458            0.7202\n",
      "3   aoloch  75005       462      0.6160    48458            0.6893\n",
      "38     ibc  75005       454      0.6053    48458            0.6810\n",
      "\n",
      "=== EDA (BEFORE imputation): Distribution summary (raw inputs) ===\n",
      "         n_nonmiss_pre      mean_pre       std_pre       min_pre  \\\n",
      "consol               0           NaN           NaN           NaN   \n",
      "datafmt              0           NaN           NaN           NaN   \n",
      "conm                 0           NaN           NaN           NaN   \n",
      "indfmt               0           NaN           NaN           NaN   \n",
      "dlcch            41862  2.168121e+03  1.155250e+06 -8.555368e+07   \n",
      "apalch           44634  8.385212e+03  3.253932e+05 -4.849894e+07   \n",
      "txach            52214  3.214308e+02  3.140000e+04 -3.282293e+06   \n",
      "ivstch           55811  6.871673e+04  3.980372e+06 -1.752327e+07   \n",
      "recch            62416 -1.318528e+04  3.129434e+05 -2.297888e+07   \n",
      "mkvalt           62655  9.288907e+05  2.201298e+07  0.000000e+00   \n",
      "sppe             62766  6.939724e+03  1.245321e+05 -5.895100e+04   \n",
      "act              64284  6.440956e+05  4.645197e+06 -2.498000e+03   \n",
      "lct              64310  4.755088e+05  3.980895e+06  0.000000e+00   \n",
      "xint             64469  2.759126e+04  1.196406e+05 -7.284730e+05   \n",
      "txditc           65936  4.384545e+04  3.265589e+05 -1.616000e+03   \n",
      "txp              67063  1.062620e+04  1.001099e+05 -2.217000e+03   \n",
      "esubc            68207 -2.220041e+03  8.665624e+04 -8.775761e+06   \n",
      "lco              68226  2.039805e+05  1.784063e+06 -8.900000e+01   \n",
      "aco              68227  5.573244e+04  9.308496e+05 -2.500000e+01   \n",
      "prcc_f           69029  6.838158e+02  1.018087e+04  1.000000e-04   \n",
      "prcc_c           69039  7.067634e+02  1.003861e+04  1.000000e-04   \n",
      "invch            69690 -3.860008e+04  1.008997e+06 -8.472324e+07   \n",
      "sppiv            71038 -7.943608e+03  1.753101e+05 -1.598761e+07   \n",
      "ivaeq            71457  6.873078e+04  9.162568e+05 -1.551500e+04   \n",
      "prstkc           71595  2.902337e+04  2.408411e+05 -1.131100e+04   \n",
      "caps             71931  5.224117e+05  2.924028e+06 -2.333792e+07   \n",
      "aqc              72193  2.583727e+04  3.427732e+05 -2.002407e+07   \n",
      "dp               72283  7.989028e+04  7.010127e+05 -4.121000e+03   \n",
      "re               72409  4.063079e+05  4.773704e+06 -8.108442e+07   \n",
      "ivch             72465  4.336588e+05  1.267386e+07  0.000000e+00   \n",
      "ivao             72476  1.117387e+06  1.830844e+07  0.000000e+00   \n",
      "txdc             72533 -2.552908e+03  8.582746e+04 -8.127405e+06   \n",
      "siv              72706  3.204806e+05  9.796796e+06 -3.080000e+02   \n",
      "oibdp            72863  2.472644e+05  1.847387e+06 -6.234977e+06   \n",
      "cstk             73080  1.440901e+05  1.035999e+06 -1.466100e+04   \n",
      "dpc              73248  8.009984e+04  6.495898e+05 -1.551370e+05   \n",
      "dltis            73249  2.584506e+05  2.896443e+06 -4.257400e+04   \n",
      "ppent            73282  5.870571e+05  5.242606e+06  0.000000e+00   \n",
      "sstk             73569  2.949253e+04  2.324514e+05 -5.012830e+05   \n",
      "dltr             73617  2.679871e+05  2.856270e+06 -2.844300e+04   \n",
      "dv               73868  4.155940e+04  3.023818e+05 -1.631969e+06   \n",
      "spi              73879 -1.290533e+04  1.992509e+05 -1.354613e+07   \n",
      "rect             74125  2.121967e+06  3.060638e+07 -7.300000e+01   \n",
      "mibt             74163  5.164076e+04  6.023643e+05 -1.237174e+06   \n",
      "csho             74201  2.254364e+05  5.059555e+06  0.000000e+00   \n",
      "dvt              74299  4.154428e+04  3.087856e+05 -1.329470e+05   \n",
      "dvc              74302  4.081004e+04  3.071635e+05 -2.473000e+03   \n",
      "intan            74351  3.830857e+05  2.603599e+06  0.000000e+00   \n",
      "invt             74353  1.471455e+05  1.356403e+06  0.000000e+00   \n",
      "txbcof           74386  2.081901e+02  4.374733e+03 -9.510600e+04   \n",
      "\n",
      "              p01_pre      p05_pre     p25_pre     p50_pre      p75_pre  \\\n",
      "consol            NaN          NaN         NaN         NaN          NaN   \n",
      "datafmt           NaN          NaN         NaN         NaN          NaN   \n",
      "conm              NaN          NaN         NaN         NaN          NaN   \n",
      "indfmt            NaN          NaN         NaN         NaN          NaN   \n",
      "dlcch   -2.265422e+05   -8701.6500      0.0000      0.0000       1.0000   \n",
      "apalch  -1.053346e+05  -14648.8000   -119.0000     50.0000    1693.0000   \n",
      "txach   -9.192830e+03    -193.0000      0.0000      0.0000       0.0000   \n",
      "ivstch  -2.099582e+05  -10799.5000      0.0000      0.0000       0.0000   \n",
      "recch   -2.890472e+05  -52494.2500  -2252.2500    -17.0000      54.0000   \n",
      "mkvalt   5.154000e-01       3.3549     48.9974    467.1767    4760.4262   \n",
      "sppe     0.000000e+00       0.0000      0.0000      0.0000       6.8300   \n",
      "act      1.700000e-01      21.0000   1732.0000  23074.0000  214833.2500   \n",
      "lct      6.600000e-01      26.5090   1277.0000  11405.0000  105630.0000   \n",
      "xint     0.000000e+00       0.0000     15.0000    433.0000    8992.0000   \n",
      "txditc   0.000000e+00       0.0000      0.0000      0.0000    1883.5000   \n",
      "txp      0.000000e+00       0.0000      0.0000      0.0000     152.0000   \n",
      "esubc   -3.866582e+04    -484.7000      0.0000      0.0000       0.0000   \n",
      "lco      0.000000e+00       0.0000    301.0000   3986.5000   46136.5000   \n",
      "aco      0.000000e+00       0.0000     58.0000    946.0000   10293.0000   \n",
      "prcc_f   1.000000e-02       0.1016      2.6000     13.9200      42.8400   \n",
      "prcc_c   1.000000e-02       0.1000      2.6200     14.0100      43.0250   \n",
      "invch   -3.362948e+05  -33105.1500   -172.0000      0.0000       0.0000   \n",
      "sppiv   -1.356993e+05  -11708.4500    -13.0000      0.0000       0.0000   \n",
      "ivaeq    0.000000e+00       0.0000      0.0000      0.0000       0.0000   \n",
      "prstkc   0.000000e+00       0.0000      0.0000      0.0000     846.5000   \n",
      "caps     0.000000e+00       0.0000   1645.0000  26381.0000  268051.0000   \n",
      "aqc     -1.760872e+04       0.0000      0.0000      0.0000       0.0000   \n",
      "dp       0.000000e+00       0.0000     68.0000   1079.4700   15628.5000   \n",
      "re      -2.484404e+06 -701966.6000 -80817.0000  -1251.0000   29686.0000   \n",
      "ivch     0.000000e+00       0.0000      0.0000      0.0000     275.0000   \n",
      "ivao     0.000000e+00       0.0000      0.0000      0.0000    3979.7500   \n",
      "txdc    -1.337068e+05  -18598.4000   -105.0000      0.0000       5.0000   \n",
      "siv      0.000000e+00       0.0000      0.0000      0.0000      61.0000   \n",
      "oibdp   -2.060061e+05  -47857.8000  -1873.5000    800.4000   46909.0000   \n",
      "cstk     0.000000e+00       0.0000      8.0000    197.0000   13957.2500   \n",
      "dpc      0.000000e+00       0.0000    100.3000   1542.0000   21382.2500   \n",
      "dltis    0.000000e+00       0.0000      0.0000      0.1500    1740.0000   \n",
      "ppent    0.000000e+00       0.0000    514.0000   8837.5000   93080.2500   \n",
      "sstk     0.000000e+00       0.0000      0.0000     58.0000    4216.0000   \n",
      "dltr     0.000000e+00       0.0000      0.0000    140.7000    8902.0000   \n",
      "dv       0.000000e+00       0.0000      0.0000      0.0000     884.0000   \n",
      "spi     -3.215187e+05  -47237.4000  -1071.0000      0.0000       0.0000   \n",
      "rect     0.000000e+00       0.0000    225.0000   5841.0000   97807.0000   \n",
      "mibt    -1.887380e+03       0.0000      0.0000      0.0000       0.0000   \n",
      "csho     3.150000e+00      28.0000   7219.0000  36145.0000  107343.0000   \n",
      "dvt      0.000000e+00       0.0000      0.0000      0.0000    1178.4450   \n",
      "dvc      0.000000e+00       0.0000      0.0000      0.0000     621.0000   \n",
      "intan    0.000000e+00       0.0000      0.0000   1365.0000   45975.0000   \n",
      "invt     0.000000e+00       0.0000      0.0000    275.3000   12041.0000   \n",
      "txbcof   0.000000e+00       0.0000      0.0000      0.0000       0.0000   \n",
      "\n",
      "             p95_pre      p99_pre       max_pre  \n",
      "consol           NaN          NaN           NaN  \n",
      "datafmt          NaN          NaN           NaN  \n",
      "conm             NaN          NaN           NaN  \n",
      "indfmt           NaN          NaN           NaN  \n",
      "dlcch      12288.900    239569.72  1.674825e+08  \n",
      "apalch     40626.050    280635.99  2.085900e+07  \n",
      "txach        544.000     16622.89  3.282293e+06  \n",
      "ivstch     14654.500    206862.80  3.964813e+08  \n",
      "recch      14636.250    102245.15  3.777953e+07  \n",
      "mkvalt   1137140.389  16710068.76  3.522211e+09  \n",
      "sppe        7315.750     96130.00  7.819213e+06  \n",
      "act      1887389.100  11141665.87  2.648894e+08  \n",
      "lct      1150175.850   8749902.35  2.275619e+08  \n",
      "xint      126670.400    475515.16  3.895731e+06  \n",
      "txditc    138738.750    824518.10  1.309682e+07  \n",
      "txp        20801.100    229018.76  5.843978e+06  \n",
      "esubc        503.700     18641.34  3.314734e+06  \n",
      "lco       528718.500   3510482.25  1.976466e+08  \n",
      "aco       124386.000    926127.42  1.973027e+08  \n",
      "prcc_f       271.900     17260.04  1.369125e+06  \n",
      "prcc_c       275.000     17524.72  1.369125e+06  \n",
      "invch       9558.700     99906.23  1.919409e+07  \n",
      "sppiv        779.000     12781.32  7.378669e+06  \n",
      "ivaeq      69436.200   1094948.44  4.184002e+07  \n",
      "prstkc     87371.300    608747.74  1.551860e+07  \n",
      "caps     2034165.500   7334828.10  1.724641e+08  \n",
      "aqc        71387.800    578083.72  5.188218e+07  \n",
      "dp        220915.500   1361138.02  3.714574e+07  \n",
      "re       1402781.800  11353646.12  2.219454e+08  \n",
      "ivch      441795.800   3486163.72  1.169666e+09  \n",
      "ivao     1193829.500  10658327.50  1.099816e+09  \n",
      "txdc       11204.800     85023.32  4.507005e+06  \n",
      "siv       237122.750   2260226.70  9.068858e+08  \n",
      "oibdp     727543.600   4683708.38  7.350011e+07  \n",
      "cstk      420270.900   2924964.10  3.904769e+07  \n",
      "dpc       252347.000   1229121.52  3.648387e+07  \n",
      "dltis     692136.800   4321096.08  1.818726e+08  \n",
      "ppent    1817488.900  10065753.57  2.926841e+08  \n",
      "sstk      111376.400    525775.68  2.036114e+07  \n",
      "dltr      761854.400   3911400.36  1.796217e+08  \n",
      "dv        122377.700    899427.56  1.296791e+07  \n",
      "spi         2226.100     54763.82  1.148318e+07  \n",
      "rect     1917288.600  17121476.16  1.218880e+09  \n",
      "mibt       66241.300   1067080.28  3.075527e+07  \n",
      "csho      533438.000   2530459.00  6.064077e+08  \n",
      "dvt       124549.500    879155.68  1.485872e+07  \n",
      "dvc       120572.700    869661.95  1.437446e+07  \n",
      "intan    1345701.500   7366060.00  9.849622e+07  \n",
      "invt      385661.800   2910600.48  1.043358e+08  \n",
      "txbcof         0.000      3680.50  6.321680e+05  \n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "78ca33dd026fe0b1",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Feature Engineering & Target Construction\n",
    "#   - Build leverage / coverage / cash-flow-to-debt ratios commonly used in credit analysis\n",
    "#   - Define a 'highly leveraged' distress proxy from multiple ratio-based conditions\n",
    "#   - Create the supervised-learning target: next-year distress within the same firm\n",
    "# =============================================================================\n",
    "\n",
    "firm_col = \"firm_id\"\n",
    "\n",
    "dlc = pd.to_numeric(df.get(\"dlc\", np.nan), errors=\"coerce\")\n",
    "dltt = pd.to_numeric(df.get(\"dltt\", np.nan), errors=\"coerce\")\n",
    "df[\"total_debt\"] = pd.concat([dlc, dltt], axis=1).sum(axis=1, min_count=1)\n",
    "\n",
    "seq = pd.to_numeric(df.get(\"seq\", np.nan), errors=\"coerce\")\n",
    "mibt = pd.to_numeric(df.get(\"mibt\", 0.0), errors=\"coerce\")\n",
    "df[\"equity_plus_mi_sp\"] = seq + mibt\n",
    "df[\"total_capital_sp\"] = df[\"total_debt\"] + df[\"equity_plus_mi_sp\"]\n",
    "\n",
    "# --- CHANGED (minimal): handle total capital <= 0 as extreme leverage + flag ---\n",
    "cap_s = pd.to_numeric(df[\"total_capital_sp\"], errors=\"coerce\")\n",
    "df[\"cap_nonpos_flag\"] = (cap_s.notna() & (cap_s <= 0)).astype(\"int8\")\n",
    "df[\"sp_debt_to_capital\"] = safe_divide(df[\"total_debt\"], cap_s)\n",
    "df.loc[df[\"cap_nonpos_flag\"] == 1, \"sp_debt_to_capital\"] = np.inf\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "oibdp = pd.to_numeric(df.get(\"oibdp\", np.nan), errors=\"coerce\")\n",
    "xint = pd.to_numeric(df.get(\"xint\", np.nan), errors=\"coerce\")\n",
    "\n",
    "# --- CHANGED (minimal): handle EBITDA <= 0 as extreme leverage + flag ---\n",
    "df[\"ebitda_nonpos_flag\"] = (oibdp.notna() & (oibdp <= 0)).astype(\"int8\")\n",
    "df[\"sp_debt_to_ebitda\"] = safe_divide(df[\"total_debt\"], oibdp)\n",
    "df.loc[df[\"ebitda_nonpos_flag\"] == 1, \"sp_debt_to_ebitda\"] = np.inf\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# --- OPTIONAL transparency (simple): flag if tax proxy components missing (no complex proxying) ---\n",
    "txt_raw = pd.to_numeric(df.get(\"txt\", np.nan), errors=\"coerce\")\n",
    "txdc_raw = pd.to_numeric(df.get(\"txdc\", np.nan), errors=\"coerce\")\n",
    "txach_raw = pd.to_numeric(df.get(\"txach\", np.nan), errors=\"coerce\")\n",
    "df[\"tax_proxy_incomplete\"] = (txt_raw.isna() | txdc_raw.isna() | txach_raw.isna()).astype(\"int8\")\n",
    "\n",
    "txt = txt_raw.fillna(0.0)\n",
    "txdc = txdc_raw.fillna(0.0)\n",
    "txach = txach_raw.fillna(0.0)\n",
    "df[\"cash_tax_paid_proxy\"] = txt - txdc - txach\n",
    "\n",
    "df[\"ffo_proxy\"] = oibdp - xint - pd.to_numeric(df[\"cash_tax_paid_proxy\"], errors=\"coerce\")\n",
    "df[\"sp_ffo_to_debt\"] = safe_divide(df[\"ffo_proxy\"], df[\"total_debt\"])\n",
    "\n",
    "oancf = pd.to_numeric(df.get(\"oancf\", np.nan), errors=\"coerce\")\n",
    "capx = pd.to_numeric(df.get(\"capx\", np.nan), errors=\"coerce\")\n",
    "df[\"sp_cfo_to_debt\"] = safe_divide(oancf, df[\"total_debt\"])\n",
    "df[\"focf\"] = oancf - capx\n",
    "df[\"sp_focf_to_debt\"] = safe_divide(df[\"focf\"], df[\"total_debt\"])\n",
    "\n",
    "# --- CHANGED (minimal): make outflows robust to sign conventions ---\n",
    "dv = pd.to_numeric(df.get(\"dv\", 0.0), errors=\"coerce\").fillna(0.0).abs()\n",
    "prstkc = pd.to_numeric(df.get(\"prstkc\", 0.0), errors=\"coerce\").fillna(0.0).abs()\n",
    "df[\"dcf\"] = df[\"focf\"] - dv - prstkc\n",
    "df[\"sp_dcf_to_debt\"] = safe_divide(df[\"dcf\"], df[\"total_debt\"])\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# Log transforms (log1p handles 0 nicely). Negative → NaN.\n",
    "for c in [\"at\", \"mkvalt\"]:\n",
    "    if c in df.columns:\n",
    "        s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        df[f\"log_{c}\"] = np.where(s >= 0, np.log1p(s), np.nan)\n",
    "\n",
    "# Interest coverage: EBITDA / |interest expense|\n",
    "# Interest coverage can explode when interest expense is near zero.\n",
    "# Stabilize by flooring the denominator, capping extreme values, and log-transforming.\n",
    "INT_FLOOR = 1.0  # minimum |interest expense| to avoid blow-ups\n",
    "IC_CAP = 100.0  # cap extreme coverage magnitudes before log transform\n",
    "denom_ic = np.maximum(xint.abs(), INT_FLOOR)\n",
    "df[\"sp_interest_coverage_raw\"] = safe_divide(oibdp, denom_ic)\n",
    "df[\"sp_interest_coverage_is_capped\"] = (df[\"sp_interest_coverage_raw\"].abs() > IC_CAP).astype(\"int8\")\n",
    "df[\"sp_interest_coverage_denom_floored\"] = (xint.abs() < INT_FLOOR).astype(\"int8\")\n",
    "df[\"sp_interest_coverage\"] = np.sign(df[\"sp_interest_coverage_raw\"]) * np.log1p(\n",
    "    np.minimum(df[\"sp_interest_coverage_raw\"].abs(), IC_CAP)\n",
    ")\n",
    "\n",
    "# Distress proxy: 'highly leveraged' condition using multiple credit-ratio thresholds\n",
    "# NOTE (important): avoid defaulting missing label components to non-distress.\n",
    "# Pandas BooleanDtype preserves <NA>, so missingness becomes an *unknown* label rather than 0.\n",
    "\n",
    "td_s  = pd.to_numeric(df[\"total_debt\"], errors=\"coerce\")\n",
    "cap_s = pd.to_numeric(df[\"total_capital_sp\"], errors=\"coerce\")\n",
    "eb_s  = pd.to_numeric(oibdp, errors=\"coerce\")\n",
    "ffo_s = pd.to_numeric(df[\"ffo_proxy\"], errors=\"coerce\")\n",
    "\n",
    "# Ratios (keep as Series aligned to df.index)\n",
    "ffo_to_debt_pct = pd.Series(100.0 * safe_divide(ffo_s, td_s), index=df.index, dtype=\"float64\")\n",
    "\n",
    "debt_to_capital_pct = pd.Series(100.0 * safe_divide(td_s, cap_s), index=df.index, dtype=\"float64\")\n",
    "# cap <= 0 => extreme leverage (so hl_cap can still trigger)\n",
    "debt_to_capital_pct = debt_to_capital_pct.mask(cap_s.notna() & (cap_s <= 0), np.inf)\n",
    "\n",
    "debt_to_ebitda = pd.Series(safe_divide(td_s, eb_s), index=df.index, dtype=\"float64\")\n",
    "# EBITDA <= 0 => extreme leverage (so hl_deb can still trigger)\n",
    "debt_to_ebitda = debt_to_ebitda.mask(eb_s.notna() & (eb_s <= 0), np.inf)\n",
    "\n",
    "def _cmp_with_na(x: pd.Series, op: str, thr: float) -> pd.Series:\n",
    "    \"\"\"Comparison that returns <NA> when x is missing (prevents NaN -> False label drift).\"\"\"\n",
    "    if op == \"<\":\n",
    "        arr = np.where(x.isna(), pd.NA, x < thr)\n",
    "    elif op == \">\":\n",
    "        arr = np.where(x.isna(), pd.NA, x > thr)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported op: {op}\")\n",
    "    return pd.Series(pd.array(arr, dtype=\"boolean\"), index=x.index)\n",
    "\n",
    "# td > 0 should also be <NA> when total debt is missing\n",
    "td_pos = pd.Series(pd.array(np.where(td_s.isna(), pd.NA, td_s > 0), dtype=\"boolean\"), index=df.index)\n",
    "\n",
    "# Three “highly leveraged” conditions (S&P table), now missingness-aware\n",
    "hl_ffo = td_pos & _cmp_with_na(ffo_to_debt_pct, \"<\", 15.0)      # FFO/total debt < 15%\n",
    "hl_cap = td_pos & _cmp_with_na(debt_to_capital_pct, \">\", 55.0)  # TD/total capital > 55% (or cap<=0 => inf)\n",
    "hl_deb = td_pos & _cmp_with_na(debt_to_ebitda, \">\", 4.5)        # TD/EBITDA > 4.5 (or EBITDA<=0 => inf)\n",
    "\n",
    "is_highly_leveraged = hl_ffo & hl_cap & hl_deb  # BooleanDtype (can be <NA>)\n",
    "\n",
    "# Equity strictly negative and not missing (kept as a separate, deterministic distress trigger)\n",
    "is_equity_negative = (seq.notna() & (seq < 0))\n",
    "eq_bool = pd.Series(pd.array(is_equity_negative.to_numpy(dtype=bool), dtype=\"boolean\"), index=df.index)\n",
    "\n",
    "# Final distress rule:\n",
    "#   - 1 if evidence exists (high leverage or neg equity)\n",
    "#   - <NA> if high leverage is <NA> and no neg-equity trigger (unknown rather than 0)\n",
    "distress_bool = is_highly_leveraged | eq_bool\n",
    "df[\"distress_dummy\"] = distress_bool.astype(\"Int8\")  # nullable integer to preserve <NA>\n",
    "\n",
    "# Diagnostics (useful for auditability / reporting)\n",
    "df[\"distress_label_incomplete\"] = df[\"distress_dummy\"].isna().astype(\"int8\")\n",
    "n_incomplete = int(df[\"distress_label_incomplete\"].sum())\n",
    "print(\n",
    "    f\"Distress proxy: incomplete label components -> distress_dummy=<NA> for {n_incomplete:,} rows \"\n",
    "    f\"({(n_incomplete / max(len(df), 1)):.1%}).\"\n",
    ")\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Target: next year's distress (within firm)\n",
    "df[\"target_next_year_distress\"] = (\n",
    "    df.groupby(firm_col)[\"distress_dummy\"].shift(-1)\n",
    ")\n",
    "\n",
    "# Indicator for whether the next-year label is observed (attrition / survivorship considerations)\n",
    "df[\"has_next_year_obs\"] = df[\"target_next_year_distress\"].notna().astype(\"int8\")\n",
    "n_total = len(df)\n",
    "n_missing_next = int((df[\"has_next_year_obs\"] == 0).sum())\n",
    "print(\n",
    "    f\"Next-year label availability: {n_total - n_missing_next:,}/{n_total:,} observed \"\n",
    "    f\"({(1 - n_missing_next / max(n_total, 1)):.1%}); missing next-year={n_missing_next:,}.\"\n",
    ")\n",
    "\n",
    "# ---- Attrition diagnostics (next-year label missingness) ----\n",
    "# Missing next-year observations are rarely random in corporate panels (delistings, M&A, coverage breaks).\n",
    "# We report simple, publication-friendly diagnostics for selection risk.\n",
    "\n",
    "try:\n",
    "    _tmp = df[[firm_col, \"fyear\", \"has_next_year_obs\", \"distress_dummy\", \"at\"]].copy()\n",
    "    _tmp[\"at\"] = pd.to_numeric(_tmp[\"at\"], errors=\"coerce\")\n",
    "    # Size deciles within year to avoid secular size drift\n",
    "    _tmp[\"size_decile\"] = _tmp.groupby(\"fyear\")[\"at\"].transform(\n",
    "        lambda s: pd.qcut(s, 10, labels=False, duplicates=\"drop\") + 1\n",
    "        if s.notna().sum() >= 20 else pd.Series([np.nan] * len(s), index=s.index)\n",
    "    )\n",
    "    # 1) By year\n",
    "    year_attr = (_tmp.groupby(\"fyear\")[\"has_next_year_obs\"]\n",
    "                 .agg([\"count\", \"mean\"])\n",
    "                 .rename(columns={\"count\": \"n\", \"mean\": \"share_observed\"}))\n",
    "    year_attr[\"attrition_rate\"] = 1.0 - year_attr[\"share_observed\"]\n",
    "    print(\"\\nAttrition by year (target observability):\")\n",
    "    display(year_attr.reset_index().sort_values(\"fyear\"))\n",
    "\n",
    "    # 2) By current distress state\n",
    "    state_attr = (_tmp.groupby(\"distress_dummy\")[\"has_next_year_obs\"]\n",
    "                  .agg([\"count\", \"mean\"])\n",
    "                  .rename(columns={\"count\": \"n\", \"mean\": \"share_observed\"}))\n",
    "    state_attr[\"attrition_rate\"] = 1.0 - state_attr[\"share_observed\"]\n",
    "    print(\"\\nAttrition by current distress state (t):\")\n",
    "    display(state_attr.reset_index())\n",
    "\n",
    "    # 3) By size decile (pooled)\n",
    "    if \"size_decile\" in _tmp.columns:\n",
    "        size_attr = (_tmp.groupby(\"size_decile\")[\"has_next_year_obs\"]\n",
    "                     .agg([\"count\", \"mean\"])\n",
    "                     .rename(columns={\"count\": \"n\", \"mean\": \"share_observed\"}))\n",
    "        size_attr[\"attrition_rate\"] = 1.0 - size_attr[\"share_observed\"]\n",
    "        print(\"\\nAttrition by size decile (within-year deciles, pooled):\")\n",
    "        display(size_attr.reset_index().sort_values(\"size_decile\"))\n",
    "except Exception as e:\n",
    "    print(f\"Attrition diagnostics skipped due to error: {e}\")\n",
    "\n",
    "\n",
    "# Modeling sample: restrict to observed next-year labels (do NOT overwrite df)\n",
    "df_model = df[df[\"has_next_year_obs\"] == 1].copy().reset_index(drop=True)\n",
    "df_model[\"target_next_year_distress\"] = df_model[\"target_next_year_distress\"].astype(\"int8\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a7e623c933fe53ca",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 4. Distinct moments / event indicators (interpretable \"drivers & levers\")\n",
    "# Purpose:\n",
    "#   - Translate continuous accounting ratios into discrete, management-relevant \"events\"\n",
    "#   - Use TRAIN pool only for any distribution-based thresholds (no look-ahead)\n",
    "# =============================================================================\n",
    "\n",
    "df_events = df.copy()\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def _as_series(x, index) -> pd.Series:\n",
    "    \"\"\"Ensure x is a pandas Series aligned to index (handles scalars, arrays, Series).\"\"\"\n",
    "    if isinstance(x, pd.Series):\n",
    "        return x.reindex(index)\n",
    "    if isinstance(x, (pd.Index, list, tuple, np.ndarray)):\n",
    "        return pd.Series(x, index=index, dtype=\"float64\")\n",
    "    return pd.Series([x] * len(index), index=index, dtype=\"float64\")\n",
    "\n",
    "\n",
    "def lag(series: pd.Series, k: int = 1) -> pd.Series:\n",
    "    \"\"\"Firm-level lag (t-k) using firm_id.\"\"\"\n",
    "    s = _as_series(series, df_events.index)\n",
    "    return s.groupby(df_events[\"firm_id\"]).shift(k)\n",
    "\n",
    "\n",
    "def ratio(a, b) -> pd.Series:\n",
    "    \"\"\"Safe ratio a/b (handles zeros, inf, NaN).\"\"\"\n",
    "    a_s = _as_series(a, df_events.index)\n",
    "    b_s = _as_series(b, df_events.index)\n",
    "    return pd.Series(safe_divide(a_s, b_s), index=df_events.index, dtype=\"float64\")\n",
    "\n",
    "\n",
    "# Define training mask for distribution-aware thresholds (exclude val/test)\n",
    "pool_mask = (df_events[\"label_year\"] <= TRAIN_CUTOFF_LABEL_YEAR)\n",
    "train_mask = pool_mask.copy()\n",
    "if \"val_years\" in globals() and len(val_years) > 0:\n",
    "    train_mask = train_mask & (~df_events[\"label_year\"].isin(val_years))\n",
    "\n",
    "# =============================================================================\n",
    "# 4.1 Dividend \"moments\"\n",
    "# =============================================================================\n",
    "# Distress literature treats dividend cuts as a signal of cash-flow stress / financing constraints.\n",
    "# Given the mass at zero (many firms pay no dividends), define events conditional on prior payment.\n",
    "\n",
    "div = pd.to_numeric(\n",
    "    _as_series(df_events[\"dv\"] if \"dv\" in df_events.columns else 0.0, df_events.index),\n",
    "    errors=\"coerce\",\n",
    ").fillna(0.0)\n",
    "div_l1 = lag(div, 1).fillna(0.0)\n",
    "\n",
    "# Distribution-aware cut threshold among dividend payers (TRAIN pool only)\n",
    "payer_mask_train = train_mask & (div_l1 > 0)\n",
    "pct_change = (div - div_l1) / div_l1.replace(0, np.nan)\n",
    "df_events[\"div_pct_change\"] = pct_change\n",
    "\n",
    "cut_q = pct_change.loc[payer_mask_train].quantile(0.10)  # 10th percentile among payers\n",
    "cut_threshold = float(cut_q) if pd.notna(cut_q) else -0.25\n",
    "cut_threshold = max(min(cut_threshold, -0.10), -0.50)  # clamp to reasonable range\n",
    "\n",
    "df_events[\"evt_div_cut\"] = ((div_l1 > 0) & (pct_change <= cut_threshold)).astype(\"int8\")\n",
    "df_events[\"evt_div_suspend\"] = ((div_l1 > 0) & (div <= 0)).astype(\"int8\")\n",
    "df_events[\"evt_div_initiate\"] = ((div_l1 <= 0) & (div > 0)).astype(\"int8\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4.2 / liquidity / profitability \"moments\"\n",
    "# =============================================================================\n",
    "\n",
    "# --- Liquidity squeeze ---\n",
    "act = pd.to_numeric(_as_series(df_events.get(\"act\", np.nan), df_events.index), errors=\"coerce\")\n",
    "lct = pd.to_numeric(_as_series(df_events.get(\"lct\", np.nan), df_events.index), errors=\"coerce\")\n",
    "invt = pd.to_numeric(_as_series(df_events.get(\"invt\", 0.0), df_events.index), errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "cr = ratio(act, lct)\n",
    "qr = ratio(act - invt, lct)\n",
    "\n",
    "df_events[\"evt_liquidity_squeeze\"] = (cr.notna() & (cr < 1.0)).astype(\"int8\")\n",
    "df_events[\"evt_quick_squeeze\"] = (qr.notna() & (qr < 0.8)).astype(\"int8\")\n",
    "\n",
    "# --- EBITDA and CFO stress ---\n",
    "oibdp = pd.to_numeric(_as_series(df_events.get(\"oibdp\", np.nan), df_events.index), errors=\"coerce\")\n",
    "oibdp_l1 = lag(oibdp, 1)\n",
    "# Define \"drop\" as a multiplicative deterioration only where that ratio is meaningful.\n",
    "doibdp = pd.Series(np.nan, index=df_events.index, dtype=\"float64\")\n",
    "m_eb = (\n",
    "    oibdp.notna()\n",
    "    & oibdp_l1.notna()\n",
    "    & np.isfinite(oibdp)\n",
    "    & np.isfinite(oibdp_l1)\n",
    "    & (oibdp_l1 > 0)\n",
    "    & (oibdp >= 0)\n",
    ")\n",
    "doibdp.loc[m_eb] = (oibdp.loc[m_eb] / oibdp_l1.loc[m_eb]).astype(\"float64\")\n",
    "\n",
    "df_events[\"doibdp\"] = doibdp\n",
    "doibdp_cal = doibdp.loc[train_mask].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "doibdp_q = doibdp_cal.quantile(0.05)\n",
    "doibdp_thr = float(doibdp_q) if pd.notna(doibdp_q) else 0.5\n",
    "doibdp_thr = float(np.clip(doibdp_thr, 0.10, 0.90))  # guardrails\n",
    "\n",
    "df_events[\"evt_ebitda_drop\"] = (m_eb & (doibdp < doibdp_thr)).astype(\"int8\")\n",
    "df_events[\"evt_ebitda_neg\"] = (oibdp.notna() & (oibdp < 0)).astype(\"int8\")\n",
    "\n",
    "oancf = pd.to_numeric(_as_series(df_events.get(\"oancf\", np.nan), df_events.index), errors=\"coerce\")\n",
    "oancf_l1 = lag(oancf, 1)\n",
    "# Define \"drop\" as a multiplicative deterioration only where that ratio is meaningful.\n",
    "dcfo = pd.Series(np.nan, index=df_events.index, dtype=\"float64\")\n",
    "m_cfo = (\n",
    "    oancf.notna()\n",
    "    & oancf_l1.notna()\n",
    "    & np.isfinite(oancf)\n",
    "    & np.isfinite(oancf_l1)\n",
    "    & (oancf_l1 > 0)\n",
    "    & (oancf >= 0)\n",
    ")\n",
    "dcfo.loc[m_cfo] = (oancf.loc[m_cfo] / oancf_l1.loc[m_cfo]).astype(\"float64\")\n",
    "\n",
    "df_events[\"dcfo\"] = dcfo\n",
    "dcfo_cal = dcfo.loc[train_mask].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "cfo_drop_q = dcfo_cal.quantile(0.05)\n",
    "cfo_drop_thr = float(cfo_drop_q) if pd.notna(cfo_drop_q) else 0.5\n",
    "cfo_drop_thr = float(np.clip(cfo_drop_thr, 0.10, 0.90))  # guardrails\n",
    "\n",
    "df_events[\"evt_cfo_drop\"] = (m_cfo & (dcfo < cfo_drop_thr)).astype(\"int8\")\n",
    "df_events[\"evt_cfo_neg\"] = (oancf.notna() & (oancf < 0)).astype(\"int8\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4.3 Sanity summary (TRAIN pool only)\n",
    "# =============================================================================\n",
    "event_cols = [c for c in df_events.columns if c.startswith(\"evt_\")]\n",
    "summary_pool = (\n",
    "    df_events.loc[train_mask, event_cols]\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .to_frame(\"train_event_rate\")\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"event\"})\n",
    ")\n",
    "\n",
    "print(\"Dividend cut threshold (TRAIN 10th pct):\", round(cut_threshold, 3))\n",
    "print(\"EBITDA drop threshold (TRAIN 5th pct):\", round(doibdp_thr, 3))\n",
    "print(\"CFO drop threshold (TRAIN 5th pct):\", round(cfo_drop_thr, 3))\n",
    "\n",
    "display(summary_pool.head(15))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7a159c9c78920781",
   "metadata": {},
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Train / Validation / Test Split (out-of-time)\n",
    "#   - Split by label_year to respect the t → t+1 prediction structure\n",
    "#   - Keep the last label year(s) inside the training pool as validation\n",
    "# =============================================================================\n",
    "# Use the observed-label sample for modeling to avoid silently conditioning on future existence\n",
    "df_split = df_events[df_events[\"has_next_year_obs\"] == 1].copy().reset_index(drop=True)\n",
    "\n",
    "train_pool = df_split[df_split[\"label_year\"] <= TRAIN_CUTOFF_LABEL_YEAR].copy()\n",
    "test = df_split[df_split[\"label_year\"] > TRAIN_CUTOFF_LABEL_YEAR].copy()\n",
    "\n",
    "years = np.sort(train_pool[\"label_year\"].dropna().unique())\n",
    "val_years = years[-VAL_YEARS:] if len(years) else np.array([], dtype=int)\n",
    "\n",
    "val = train_pool[train_pool[\"label_year\"].isin(val_years)].copy()\n",
    "train = train_pool[~train_pool[\"label_year\"].isin(val_years)].copy()\n",
    "\n",
    "print(\n",
    "    \"Split:\",\n",
    "    f\"train={len(train):,}\",\n",
    "    f\"val={len(val):,}\",\n",
    "    f\"test={len(test):,}\",\n",
    "    \"| val_years:\",\n",
    "    list(val_years),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "234ae95e1f693074",
   "metadata": {},
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Modeling-Ready Preprocessing (fit on TRAIN only)\n",
    "#   - Handle infinities and remaining NaNs\n",
    "#   - Winsorize continuous features using TRAIN quantile bounds\n",
    "#   - Standardize continuous features to z-scores using TRAIN stats\n",
    "#   - Keep binary (event) features as-is\n",
    "# =============================================================================\n",
    "continuous_feats = [\n",
    "    # \"sp_debt_to_capital\",   # EXCLUDED: part of distress dummy definition\n",
    "    # \"sp_ffo_to_debt\",      # EXCLUDED: part of distress dummy definition\n",
    "    \"sp_cfo_to_debt\",\n",
    "    \"sp_focf_to_debt\",\n",
    "    \"sp_dcf_to_debt\",\n",
    "    # \"sp_debt_to_ebitda\",    # EXCLUDED: part of distress dummy definition\n",
    "    # \"sp_interest_coverage\", # EXCLUDED: coverage ratio proxy (potential circularity)\n",
    "    \"log_at\",\n",
    "    \"log_mkvalt\",\n",
    "]\n",
    "# Ensure they exist in all splits\n",
    "continuous_feats = [c for c in continuous_feats if c in train.columns and c in val.columns and c in test.columns]\n",
    "\n",
    "event_feats = []\n",
    "if \"event_cols\" in globals():\n",
    "    # EXCLUDE events derived from the distress-definition variables to avoid endogeneity\n",
    "    excluded_event_stems = [\"evt_cov_breach\", \"evt_cov_collapse\", \"evt_lev_spike\"]\n",
    "    event_feats = [\n",
    "        c for c in event_cols \n",
    "        if c in train.columns and c in val.columns and c in test.columns\n",
    "        and not any(s in c for s in excluded_event_stems)\n",
    "    ]\n",
    "\n",
    "# Total features to be used\n",
    "all_feats = continuous_feats + event_feats\n",
    "\n",
    "# Replace +/-inf with NaN\n",
    "for d in (train, val, test):\n",
    "    d[all_feats] = d[all_feats].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Impute NaNs (train-only medians)\n",
    "fill = train[all_feats].median(numeric_only=True)\n",
    "for d in (train, val, test):\n",
    "    d[all_feats] = d[all_feats].fillna(fill)\n",
    "\n",
    "# Winsorize ONLY continuous features\n",
    "bounds = {}\n",
    "for c in continuous_feats:\n",
    "    s = pd.to_numeric(train[c], errors=\"coerce\")\n",
    "    bounds[c] = (s.quantile(WINSOR_LOWER_Q), s.quantile(WINSOR_UPPER_Q))\n",
    "\n",
    "for d in (train, val, test):\n",
    "    for c, (lo, hi) in bounds.items():\n",
    "        s = pd.to_numeric(d[c], errors=\"coerce\")\n",
    "        d[c] = s.clip(lo, hi)\n",
    "\n",
    "# Standardize (z-scores) ONLY continuous features\n",
    "scaler = StandardScaler().fit(train[continuous_feats].to_numpy(dtype=float))\n",
    "\n",
    "# Map to new columns\n",
    "z_cols_cont = [f\"z_{c}\" for c in continuous_feats]\n",
    "train[z_cols_cont] = scaler.transform(train[continuous_feats].to_numpy(dtype=float))\n",
    "val[z_cols_cont] = scaler.transform(val[continuous_feats].to_numpy(dtype=float))\n",
    "test[z_cols_cont] = scaler.transform(test[continuous_feats].to_numpy(dtype=float))\n",
    "\n",
    "# Final model features: scaled continuous + raw binary events\n",
    "MODEL_FEATS = z_cols_cont + event_feats\n",
    "# Update z_cols for compatibility with later cells\n",
    "z_cols = MODEL_FEATS"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "822793c7947cae74",
   "metadata": {},
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Diagnostics & Monitoring Proxies\n",
    "#   - Correlation screen (TRAIN) for rough signal strength and sanity checks\n",
    "#   - Expanding-window time folds for temporal stability checks\n",
    "#   - Dataset overview (rows/firms/years/target rate) + target rate by year\n",
    "#   - Distribution summaries, collinearity scan, and simple drift proxy (SMD) Train→Test\n",
    "# =============================================================================\n",
    "t = \"target_next_year_distress\"\n",
    "\n",
    "feats = [c for c in (all_feats if \"all_feats\" in globals() else z_cols) if c in train.columns and c in test.columns]\n",
    "\n",
    "corr = (\n",
    "    train[[t] + feats]\n",
    "    .corr(numeric_only=True)[t]\n",
    "    .drop(t)\n",
    "    .sort_values(key=np.abs, ascending=False)\n",
    ")\n",
    "print(\"Correlation with target:\")\n",
    "print(corr)\n",
    "\n",
    "\n",
    "# Multicollinearity: Variance Inflation Factor (VIF)\n",
    "def calculate_vif(df, features):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = features\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(df[features].values, i) for i in range(len(features))]\n",
    "    return vif_data.sort_values(\"VIF\", ascending=False)\n",
    "\n",
    "\n",
    "vif_df = calculate_vif(train, z_cols)\n",
    "print(\"\\n=== Multicollinearity Diagnostic (VIF) ===\")\n",
    "print(vif_df)\n",
    "\n",
    "folds = rolling_year_folds(train_pool, n_splits=N_SPLITS_TIME_CV, min_train_years=3)\n",
    "for i, (tr_idx, va_idx, tr_years, va_year) in enumerate(folds, 1):\n",
    "    print(\n",
    "        f\"Fold {i}: train_years={tr_years[0]}..{tr_years[-1]} (n={len(tr_idx)}), \"\n",
    "        f\"val_year={va_year} (n={len(va_idx)})\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _overview(d: pd.DataFrame, name: str) -> None:\n",
    "    n_rows = len(d)\n",
    "    n_firms = d[\"firm_id\"].nunique() if \"firm_id\" in d.columns else np.nan\n",
    "    n_years = d[\"fyear\"].nunique() if \"fyear\" in d.columns else np.nan\n",
    "    target_rate = float(d[t].mean()) if t in d.columns else np.nan\n",
    "\n",
    "    print(f\"\\n=== {name} === rows={n_rows:,} | firms={n_firms:,} | years={n_years} | target_rate={target_rate:.4f}\")\n",
    "\n",
    "    if \"label_year\" in d.columns:\n",
    "        by_year = d.groupby(\"label_year\")[t].agg([\"mean\", \"count\"])\n",
    "        print(\"\\nTarget by label_year (tail):\")\n",
    "        print(by_year.tail(12))\n",
    "\n",
    "\n",
    "_overview(train, \"TRAIN\")\n",
    "_overview(val, \"VAL\")\n",
    "_overview(test, \"TEST\")\n",
    "\n",
    "post_miss = pd.DataFrame({\"col\": raw})\n",
    "post_miss[\"train_pct_na\"] = [train[c].isna().mean() * 100 if c in train.columns else np.nan for c in raw]\n",
    "post_miss[\"val_pct_na\"] = [val[c].isna().mean() * 100 if c in val.columns else np.nan for c in raw]\n",
    "post_miss[\"test_pct_na\"] = [test[c].isna().mean() * 100 if c in test.columns else np.nan for c in raw]\n",
    "if not post_miss.empty:\n",
    "    post_miss = post_miss.sort_values(\"train_pct_na\", ascending=False)\n",
    "    print(\"\\nPost-imputation missingness on raw inputs (pct):\")\n",
    "    print(post_miss.head(50).round(4))\n",
    "\n",
    "\n",
    "def _dist(d: pd.DataFrame, cols: list[str], name: str) -> pd.DataFrame:\n",
    "    x = d[cols].replace([np.inf, -np.inf], np.nan)\n",
    "    q = x.quantile([0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99]).T\n",
    "    out = pd.DataFrame(\n",
    "        {\n",
    "            \"n\": x.notna().sum(),\n",
    "            \"mean\": x.mean(),\n",
    "            \"std\": x.std(ddof=0),\n",
    "            \"min\": x.min(),\n",
    "            \"p01\": q[0.01],\n",
    "            \"p05\": q[0.05],\n",
    "            \"p25\": q[0.25],\n",
    "            \"p50\": q[0.50],\n",
    "            \"p75\": q[0.75],\n",
    "            \"p95\": q[0.95],\n",
    "            \"p99\": q[0.99],\n",
    "            \"max\": x.max(),\n",
    "            \"skew\": x.skew(numeric_only=True),\n",
    "            \"kurt\": x.kurtosis(numeric_only=True),\n",
    "        }\n",
    "    )\n",
    "    print(f\"\\nDistribution summary ({name})\")\n",
    "    print(out.round(4).sort_values(\"skew\", key=lambda s: s.abs(), ascending=False))\n",
    "    return out\n",
    "\n",
    "\n",
    "_ = _dist(train, feats, \"TRAIN | winsorized raw feats\")\n",
    "_ = _dist(train, z_cols, \"TRAIN | standardized feats\")\n",
    "\n",
    "\n",
    "def _hi_corr(d: pd.DataFrame, cols: list[str], thr: float = 0.80) -> list[tuple[str, str, float]]:\n",
    "    cm = d[cols].corr(numeric_only=True)\n",
    "    pairs = []\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            r = cm.iloc[i, j]\n",
    "            if np.isfinite(r) and abs(r) >= thr:\n",
    "                pairs.append((cols[i], cols[j], float(r)))\n",
    "    return sorted(pairs, key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "\n",
    "pairs = _hi_corr(train, feats, thr=0.80)\n",
    "print(\"\\nHigh collinearity pairs among feats (|corr|>=0.80) [top 25]:\")\n",
    "for a, b, r in pairs[:25]:\n",
    "    print(f\"{a} vs {b}: r={r:.3f}\")\n",
    "\n",
    "\n",
    "def _drift_smd(a_df: pd.DataFrame, b_df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for c in cols:\n",
    "        a = pd.to_numeric(a_df[c], errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "        b = pd.to_numeric(b_df[c], errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "        ma, mb = float(a.mean()), float(b.mean())\n",
    "        sa, sb = float(a.std(ddof=0)), float(b.std(ddof=0))\n",
    "        sp = np.sqrt(0.5 * (sa ** 2 + sb ** 2))\n",
    "        smd = (mb - ma) / sp if sp > 0 else np.nan\n",
    "\n",
    "        rows.append((c, ma, mb, smd, abs(smd) if np.isfinite(smd) else np.nan))\n",
    "\n",
    "        out = pd.DataFrame(rows, columns=[\"feature\", \"mean_train\", \"mean_test\", \"smd\", \"abs_smd\"])\n",
    "    return out.sort_values(\"abs_smd\", ascending=False)\n",
    "\n",
    "\n",
    "drift = _drift_smd(train, test, feats)\n",
    "print(\"\\nTrain→Test drift (SMD) [top 15]:\")\n",
    "print(drift.head(15).round(4))\n",
    "\n",
    "\n",
    "def _group_diff(d: pd.DataFrame, cols: list[str]) -> pd.Series:\n",
    "    g = d.groupby(t)[cols].mean(numeric_only=True)\n",
    "    if 0 in g.index and 1 in g.index:\n",
    "        return (g.loc[1] - g.loc[0]).sort_values(key=np.abs, ascending=False)\n",
    "    return pd.Series(dtype=\"float64\")\n",
    "\n",
    "\n",
    "diff = _group_diff(train, feats)\n",
    "if not diff.empty:\n",
    "    print(\"\\nMean difference (target=1 minus target=0) on TRAIN feats [top 15]:\")\n",
    "    print(diff.head(15).round(4))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2ec169034d267c32",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Visual EDA: Feature distributions by distress flag\n",
    "#   - Quick separation check: do distressed vs non-distressed firms differ in levels?\n",
    "#   - Uses a horizontal boxplot for comparability across features\n",
    "# =============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Objective: visualize feature distribution differences for distressed vs non-distressed observations.\n",
    "\n",
    "plot_df = train_pool.copy() if \"train_pool\" in globals() else df.copy()\n",
    "\n",
    "flag_col = \"distress_dummy\" if \"distress_dummy\" in plot_df.columns else (\n",
    "    \"target_next_year_distress\" if \"target_next_year_distress\" in plot_df.columns else None\n",
    ")\n",
    "if flag_col is None:\n",
    "    raise KeyError(\"No distress flag found. Expected 'distress_dummy' or 'target_next_year_distress' in the data.\")\n",
    "\n",
    "plot_feats = [c for c in (feats if \"feats\" in globals() else []) if c in plot_df.columns]\n",
    "if not plot_feats:\n",
    "    plot_feats = [c for c in (z_cols if \"z_cols\" in globals() else []) if c in plot_df.columns]\n",
    "if not plot_feats:\n",
    "    raise KeyError(\n",
    "        \"No feature columns found to plot. Expected 'feats' or 'z_cols' to exist and be present in the data.\")\n",
    "\n",
    "tmp = plot_df[[flag_col] + plot_feats].copy()\n",
    "tmp[flag_col] = pd.to_numeric(tmp[flag_col], errors=\"coerce\").astype(\"Int64\")\n",
    "tmp = tmp[tmp[flag_col].isin([0, 1])].copy()\n",
    "\n",
    "long = tmp.melt(id_vars=[flag_col], value_vars=plot_feats, var_name=\"feature\", value_name=\"value\")\n",
    "long[\"value\"] = pd.to_numeric(long[\"value\"], errors=\"coerce\")\n",
    "long = long.dropna(subset=[\"value\"])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, max(4.5, 0.45 * len(plot_feats))))\n",
    "sns.boxplot(\n",
    "    data=long,\n",
    "    x=\"value\",\n",
    "    y=\"feature\",\n",
    "    hue=flag_col,\n",
    "    orient=\"h\",\n",
    "    showfliers=False,\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title(f\"Feature distributions by {flag_col} (0=No distress, 1=Distress)\")\n",
    "ax.set_xlabel(\"Feature value\")\n",
    "ax.set_ylabel(\"\")\n",
    "ax.legend(title=flag_col, loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bb54961054fb0b07",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Persistence Benchmark (Economic Sanity Check)\n",
    "#   - Does the ML model add value over simply assuming distress persists?\n",
    "#   - We use 'distress_dummy' (current year) to predict 'target_next_year_distress'\n",
    "# NOTE: distress_dummy can be <NA> if label components are incomplete. Drop those rows here.\n",
    "# =============================================================================\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "for name, d in [(\"VAL\", val), (\"TEST\", test)]:\n",
    "    bench = d[[TARGET_COL, \"distress_dummy\"]].copy()\n",
    "    bench = bench.dropna(subset=[TARGET_COL, \"distress_dummy\"])\n",
    "\n",
    "    if bench.empty:\n",
    "        print(f\"--- Persistence Benchmark ({name}) ---\")\n",
    "        print(\"No rows available after dropping missing distress_dummy; skipping.\")\n",
    "        continue\n",
    "\n",
    "    y_true = bench[TARGET_COL].astype(int)\n",
    "    y_bench = bench[\"distress_dummy\"].astype(int)\n",
    "\n",
    "    # roc_auc_score requires both classes to be present\n",
    "    try:\n",
    "        auc_bench = roc_auc_score(y_true, y_bench)\n",
    "    except ValueError:\n",
    "        auc_bench = np.nan\n",
    "\n",
    "    f1_bench = f1_score(y_true, y_bench)\n",
    "    acc_bench = accuracy_score(y_true, y_bench)\n",
    "\n",
    "    print(f\"--- Persistence Benchmark ({name}) ---\")\n",
    "    print(f\"Rows used: {len(bench):,}/{len(d):,}\")\n",
    "    print(f\"AUC: {auc_bench:.4f} | F1: {f1_bench:.4f} | Accuracy: {acc_bench:.4f}\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1573707bdfc2f8c5",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 6.1 Setup: Features, target, and train/val/test matrices\n",
    "# =============================================================================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Prefer standardized feature columns created earlier (fit on TRAIN only)\n",
    "if \"z_cols\" in globals():\n",
    "    MODEL_FEATS = list(z_cols)\n",
    "else:\n",
    "    MODEL_FEATS = [f\"z_{c}\" for c in feats]  # fallback\n",
    "\n",
    "# Basic sanity checks\n",
    "for df_name, df_ in [(\"train\", train), (\"val\", val), (\"test\", test)]:\n",
    "    missing_feats = [c for c in MODEL_FEATS if c not in df_.columns]\n",
    "    if missing_feats:\n",
    "        raise KeyError(f\"{df_name}: missing feature columns: {missing_feats}\")\n",
    "\n",
    "X_train = train[MODEL_FEATS].to_numpy(dtype=float)\n",
    "y_train = train[TARGET_COL].astype(int).to_numpy()\n",
    "\n",
    "X_val = val[MODEL_FEATS].to_numpy(dtype=float)\n",
    "y_val = val[TARGET_COL].astype(int).to_numpy()\n",
    "\n",
    "X_test = test[MODEL_FEATS].to_numpy(dtype=float)\n",
    "y_test = test[TARGET_COL].astype(int).to_numpy()\n",
    "\n",
    "# Defensive: ensure model inputs are finite\n",
    "def _assert_finite(name, X):\n",
    "    bad = ~np.isfinite(X)\n",
    "    if bad.any():\n",
    "        rows, cols = np.where(bad)\n",
    "        raise ValueError(f\"{name}: found non-finite values at {len(rows)} cells (e.g., row={rows[0]}, col={MODEL_FEATS[cols[0]]}).\")\n",
    "\n",
    "_assert_finite(\"X_train\", X_train)\n",
    "_assert_finite(\"X_val\", X_val)\n",
    "_assert_finite(\"X_test\", X_test)\n",
    "\n",
    "print(\"Modeling matrix shapes:\")\n",
    "print(f\"  X_train: {X_train.shape} | y_train mean={y_train.mean():.4f}\")\n",
    "print(f\"  X_val:   {X_val.shape} | y_val mean={y_val.mean():.4f}\")\n",
    "print(f\"  X_test:  {X_test.shape} | y_test mean={y_test.mean():.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40ef8ef91a07ca7e",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 6.2 Baseline model: Logistic Regression (tuned on VAL)\n",
    "#   - Interpretable, strong baseline for tabular finance ratios\n",
    "#   - We tune C on VAL; you can expand the grid later if needed\n",
    "# =============================================================================\n",
    "C_GRID = [0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "best = {\"C\": None, \"val_auc\": -np.inf, \"model\": None}\n",
    "\n",
    "for C in C_GRID:\n",
    "    clf = LogisticRegression(\n",
    "        C=C,\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=500,\n",
    "        class_weight=None,  # keep likelihood-consistent probabilities (PD interpretation requires calibration discipline)\n",
    "        n_jobs=None,\n",
    "        random_state=42,\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    val_proba = clf.predict_proba(X_val)[:, 1]\n",
    "    val_auc = roc_auc_score(y_val, val_proba)\n",
    "\n",
    "    if val_auc > best[\"val_auc\"]:\n",
    "        best.update({\"C\": C, \"val_auc\": val_auc, \"model\": clf})\n",
    "\n",
    "print(f\"Best LogisticRegression on VAL: C={best['C']} | AUC={best['val_auc']:.4f}\")\n",
    "logit = best[\"model\"]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ec03d9887afd9e2a",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 6.2b Statistical Inference & Economic Interpretation (Statsmodels)\n",
    "#   - sklearn is great for prediction but lacks p-values and inference.\n",
    "#   - We re-estimate the logit using statsmodels to audit statistical significance.\n",
    "#   - We also report Marginal Effects to see economic significance.\n",
    "# =============================================================================\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Add constant for intercept\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "# Panel dependence: use cluster-robust SEs (firm-level) for inference in the training panel\n",
    "try:\n",
    "    _groups = train[\"firm_id\"].to_numpy()\n",
    "    sm_model = sm.Logit(y_train, X_train_sm).fit(\n",
    "        disp=0,\n",
    "        cov_type=\"cluster\",\n",
    "        cov_kwds={\"groups\": _groups},\n",
    "    )\n",
    "    _se_note = \"cluster-robust SEs (firm-level)\"\n",
    "except Exception as e:\n",
    "    # Fallback: still produce estimates, but avoid overstating inference quality\n",
    "    sm_model = sm.Logit(y_train, X_train_sm).fit(disp=0)\n",
    "    _se_note = f\"non-robust SEs (fallback; clustering failed: {e})\"\n",
    "\n",
    "print(f\"\\n=== Statsmodels Logistic Regression Summary ({_se_note}) ===\")\n",
    "print(sm_model.summary(xname=[\"const\"] + MODEL_FEATS))\n",
    "\n",
    "# Marginal Effects at the Mean (MEM)\n",
    "try:\n",
    "    mfx = sm_model.get_margeff(at=\"mean\")\n",
    "    print(\"\\n=== Economic Significance: Marginal Effects at the Mean ===\")\n",
    "    print(mfx.summary())\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not calculate marginal effects: {e}\")\n",
    "\n",
    "# AIC / BIC / Pseudo-R2\n",
    "print(f\"\\nModel Fit:\")\n",
    "print(f\"  Pseudo R2 (McFadden): {sm_model.prsquared:.4f}\")\n",
    "print(f\"  AIC: {sm_model.aic:.2f}\")\n",
    "print(f\"  BIC: {sm_model.bic:.2f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a124f1b5a5eb6d7b",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 6.2c Temporal Stability (Walk-Forward Validation)\n",
    "#   - We assess whether model performance is stable across different time regimes.\n",
    "#   - Using the expanding-window folds defined in diagnostics.\n",
    "# =============================================================================\n",
    "temporal_results = []\n",
    "\n",
    "FOLD_FEATS = [c for c in feats if c in train_pool.columns]\n",
    "if not FOLD_FEATS:\n",
    "    raise KeyError(\"No usable feature columns found in train_pool for temporal CV. Expected 'feats' columns to exist.\")\n",
    "\n",
    "for i, (tr_idx, va_idx, tr_years, va_year) in enumerate(folds, 1):\n",
    "    tr_df = train_pool.loc[tr_idx, FOLD_FEATS + [TARGET_COL]].copy()\n",
    "    va_df = train_pool.loc[va_idx, FOLD_FEATS + [TARGET_COL]].copy()\n",
    "\n",
    "    # Targets\n",
    "    y_tr = pd.to_numeric(tr_df[TARGET_COL], errors=\"coerce\").astype(int).to_numpy()\n",
    "    y_va = pd.to_numeric(va_df[TARGET_COL], errors=\"coerce\").astype(int).to_numpy()\n",
    "\n",
    "    # Features: fold-local preprocessing (no leakage)\n",
    "    X_tr_df = tr_df[FOLD_FEATS].replace([np.inf, -np.inf], np.nan).apply(pd.to_numeric, errors=\"coerce\")\n",
    "    X_va_df = va_df[FOLD_FEATS].replace([np.inf, -np.inf], np.nan).apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    # Median imputation fit on fold-train only\n",
    "    fold_fill = X_tr_df.median(numeric_only=True)\n",
    "    X_tr_df = X_tr_df.fillna(fold_fill)\n",
    "    X_va_df = X_va_df.fillna(fold_fill)\n",
    "\n",
    "    # Winsorize using fold-train only\n",
    "    fold_bounds = {}\n",
    "    for c in FOLD_FEATS:\n",
    "        s = pd.to_numeric(X_tr_df[c], errors=\"coerce\")\n",
    "        fold_bounds[c] = (s.quantile(WINSOR_LOWER_Q), s.quantile(WINSOR_UPPER_Q))\n",
    "\n",
    "    for c, (lo, hi) in fold_bounds.items():\n",
    "        X_tr_df[c] = pd.to_numeric(X_tr_df[c], errors=\"coerce\").clip(lo, hi)\n",
    "        X_va_df[c] = pd.to_numeric(X_va_df[c], errors=\"coerce\").clip(lo, hi)\n",
    "\n",
    "    # Standardize fit on fold-train only\n",
    "    fold_scaler = StandardScaler().fit(X_tr_df.to_numpy(dtype=float))\n",
    "    X_tr = fold_scaler.transform(X_tr_df.to_numpy(dtype=float))\n",
    "    X_va = fold_scaler.transform(X_va_df.to_numpy(dtype=float))\n",
    "\n",
    "    # Fit model (using best C from tuning)\n",
    "    fold_clf = LogisticRegression(C=best[\"C\"], solver=\"lbfgs\", max_iter=500, class_weight=\"balanced\", random_state=42)\n",
    "    fold_clf.fit(X_tr, y_tr)\n",
    "\n",
    "    # Evaluate\n",
    "    probs = fold_clf.predict_proba(X_va)[:, 1]\n",
    "    auc = roc_auc_score(y_va, probs)\n",
    "    ap = average_precision_score(y_va, probs)\n",
    "\n",
    "    temporal_results.append({\n",
    "        \"Fold\": i,\n",
    "        \"Val_Year\": va_year,\n",
    "        \"Train_End\": tr_years[-1],\n",
    "        \"AUC\": auc,\n",
    "        \"PR-AUC\": ap\n",
    "    })\n",
    "\n",
    "temporal_df = pd.DataFrame(temporal_results)\n",
    "print(\"\\n=== Temporal Stability: Walk-Forward Results ===\")\n",
    "print(temporal_df.round(4))\n",
    "print(f\"\\nAverage AUC across folds: {temporal_df['AUC'].mean():.4f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "56e2e08d3b243c6a",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 6.3 Evaluation: VAL and TEST (AUC, PR-AUC, Brier) + thresholding\n",
    "# =============================================================================\n",
    "def evaluate_split(name, y_true, y_proba, threshold=0.5):\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    ap = average_precision_score(y_true, y_proba)\n",
    "    brier = brier_score_loss(y_true, y_proba)\n",
    "\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"AUC (ROC): {auc:.4f}\")\n",
    "    print(f\"Average Precision (PR-AUC): {ap:.4f}\")\n",
    "    print(f\"Brier score (calibration): {brier:.4f}\")\n",
    "    print(f\"Threshold: {threshold:.3f}\")\n",
    "    print(\"Confusion matrix [ [TN FP] [FN TP] ]:\")\n",
    "    print(cm)\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    \n",
    "    # Explicitly pull out Sensitivity and Specificity\n",
    "    # TN FP\n",
    "    # FN TP\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    print(f\"Sensitivity (Recall of Distress): {sensitivity:.4f}\")\n",
    "    print(f\"Specificity (Recall of Non-Distress): {specificity:.4f}\")\n",
    "    \n",
    "    return {\"auc\": auc, \"ap\": ap, \"brier\": brier, \"cm\": cm, \"sens\": sensitivity, \"spec\": specificity}\n",
    "\n",
    "val_proba = logit.predict_proba(X_val)[:, 1]\n",
    "test_proba = logit.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Choose an operational threshold using VAL under an explicit screening objective.\n",
    "# Finance deployment typically follows either:\n",
    "#   (i) capacity constraints (review top X% highest-risk firms), and/or\n",
    "#   (ii) asymmetric costs (missed distress >> false alarms).\n",
    "#\n",
    "# We implement both and report results using the cost-minimizing threshold as the default.\n",
    "\n",
    "# Capacity rule: flag the top CAP_RATE proportion by predicted risk on VAL\n",
    "CAP_RATE = 0.10  # adjust to your review bandwidth (e.g., 5%–20%)\n",
    "cap_thr = float(np.nanquantile(val_proba, 1.0 - CAP_RATE))\n",
    "\n",
    "# Cost rule: minimize expected cost on VAL over a dense grid of thresholds\n",
    "# Define relative costs. A principled default is COST_FN proportional to class imbalance,\n",
    "# reflecting that positives are rarer/more consequential; COST_FP normalized to 1.\n",
    "pos = float(np.sum(y_val == 1))\n",
    "neg = float(np.sum(y_val == 0))\n",
    "imbalance_ratio = (neg / max(pos, 1.0))\n",
    "COST_FP = 1.0\n",
    "COST_FN = float(imbalance_ratio)\n",
    "\n",
    "def expected_cost(y_true, proba, thr, cost_fn, cost_fp):\n",
    "    y_pred = (proba >= thr).astype(int)\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return (cost_fp * fp + cost_fn * fn) / max(len(y_true), 1)\n",
    "\n",
    "thr_grid = np.linspace(0.01, 0.99, 199)\n",
    "costs = [expected_cost(y_val, val_proba, t, COST_FN, COST_FP) for t in thr_grid]\n",
    "cost_thr = float(thr_grid[int(np.argmin(costs))])\n",
    "\n",
    "print(f\"VAL base rate: {pos / max(pos + neg, 1.0):.3f} | CAP_RATE={CAP_RATE:.2%} -> cap_thr={cap_thr:.3f} | COST_FN={COST_FN:.2f}, COST_FP={COST_FP:.2f} -> cost_thr={cost_thr:.3f}\")\n",
    "\n",
    "# Report both; use cost_thr as default operating point\n",
    "_ = evaluate_split(\"VAL (threshold=min expected cost)\", y_val, val_proba, threshold=cost_thr)\n",
    "_ = evaluate_split(\"TEST (same threshold)\", y_test, test_proba, threshold=cost_thr)\n",
    "\n",
    "_ = evaluate_split(\"VAL (capacity threshold: top X% risk)\", y_val, val_proba, threshold=cap_thr)\n",
    "_ = evaluate_split(\"TEST (capacity threshold: top X% risk)\", y_test, test_proba, threshold=cap_thr)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b7bf16a46e72626d",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 6.4 Diagnostic plots: ROC and Precision-Recall (VAL vs TEST)\n",
    "# =============================================================================\n",
    "def plot_roc(y_true, y_proba, title):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_pr(y_true, y_proba, title):\n",
    "    p, r, _ = precision_recall_curve(y_true, y_proba)\n",
    "    plt.figure()\n",
    "    plt.plot(r, p)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_roc(y_val, val_proba, \"ROC Curve (VAL) - Logistic Regression\")\n",
    "plot_roc(y_test, test_proba, \"ROC Curve (TEST) - Logistic Regression\")\n",
    "\n",
    "plot_pr(y_val, val_proba, \"Precision-Recall Curve (VAL) - Logistic Regression\")\n",
    "plot_pr(y_test, test_proba, \"Precision-Recall Curve (TEST) - Logistic Regression\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b50a6bb7548c9db7",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 6.5 Interpretability: coefficients as (approx.) log-odds contributions\n",
    "# =============================================================================\n",
    "coef = pd.Series(logit.coef_.ravel(), index=MODEL_FEATS).sort_values(ascending=False)\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"feature\": coef.index,\n",
    "    \"coef_log_odds\": coef.values,\n",
    "    \"odds_ratio\": np.exp(coef.values),  # change from 0->1 for binary, per 1SD for scaled continuous\n",
    "}).sort_values(\"coef_log_odds\", ascending=False)\n",
    "\n",
    "print(\"Top positive (higher distress risk):\")\n",
    "display(summary.head(10))\n",
    "\n",
    "print(\"Top negative (lower distress risk):\")\n",
    "display(summary.tail(10).iloc[::-1])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f0229faf16179b5d",
   "metadata": {},
   "source": [
    "### 6.6  Cost-sensitive boosted trees (non-linear) + calibrated PDs\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, export_text\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 0) HELPER UTILITIES\n",
    "# =============================================================================\n",
    "def _safe_feature_names(n_features: int):\n",
    "    cand = globals().get(\"FEATURE_COLS\", globals().get(\"MODEL_FEATS\", None))\n",
    "    if isinstance(cand, (list, tuple)) and len(cand) == n_features:\n",
    "        return list(cand)\n",
    "    return [f\"f{i}\" for i in range(n_features)]\n",
    "\n",
    "\n",
    "def gmean_tpr_tnr(y_true, proba, thr):\n",
    "    y_hat = (proba >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_hat, labels=[0, 1]).ravel()\n",
    "    tpr = tp / max(tp + fn, 1)\n",
    "    tnr = tn / max(tn + fp, 1)\n",
    "    gmean = float(np.sqrt(tpr * tnr))\n",
    "    return gmean, tpr, tnr, np.array([[tn, fp], [fn, tp]])\n",
    "\n",
    "\n",
    "def expected_cost(y_true, proba, thr, cost_fn: float, cost_fp: float):\n",
    "    y_hat = (proba >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_hat, labels=[0, 1]).ravel()\n",
    "    return float(cost_fn * fn + cost_fp * fp), np.array([[tn, fp], [fn, tp]])\n",
    "\n",
    "\n",
    "def pd_decile_table(y_true, proba, n_bins=10):\n",
    "    df = pd.DataFrame({\"y\": np.asarray(y_true).astype(int), \"p\": np.asarray(proba).astype(float)})\n",
    "    # qcut can fail with many ties; use rank to stabilize\n",
    "    r = df[\"p\"].rank(method=\"first\")\n",
    "    df[\"bin\"] = pd.qcut(r, q=n_bins, labels=False) + 1  # 1..n_bins (1=lowest risk)\n",
    "    out = (\n",
    "        df.groupby(\"bin\", as_index=False)\n",
    "        .agg(n=(\"y\", \"size\"), realized_rate=(\"y\", \"mean\"), avg_pd=(\"p\", \"mean\"),\n",
    "             min_pd=(\"p\", \"min\"), max_pd=(\"p\", \"max\"))\n",
    "        .sort_values(\"bin\")\n",
    "    )\n",
    "    out[\"lift_vs_base\"] = out[\"realized_rate\"] / max(df[\"y\"].mean(), 1e-12)\n",
    "    return out\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) SPLIT VALIDATION INTO:\n",
    "#    - early-stopping set (VAL_ES)\n",
    "#    - calibration + threshold selection set (VAL_CAL)\n",
    "#    This prevents using the same data for early stopping AND calibration/thresholding.\n",
    "# =============================================================================\n",
    "CAL_SIZE = 0.50  # half of VAL held for calibration + threshold selection\n",
    "\n",
    "try:\n",
    "    X_val_es, X_val_cal, y_val_es, y_val_cal = train_test_split(\n",
    "        X_val, y_val,\n",
    "        test_size=CAL_SIZE,\n",
    "        random_state=42,\n",
    "        stratify=y_val\n",
    "    )\n",
    "except Exception:\n",
    "    # fallback if stratify fails (e.g., too few positives)\n",
    "    X_val_es, X_val_cal, y_val_es, y_val_cal = train_test_split(\n",
    "        X_val, y_val,\n",
    "        test_size=CAL_SIZE,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "# =============================================================================\n",
    "# 2) COST-SENSITIVE WEIGHTS (weighted cross-entropy spirit: α_FN > α_FP)\n",
    "#    We implement via sample_weight to avoid double-counting with scale_pos_weight.\n",
    "# =============================================================================\n",
    "pos = int(np.sum(y_train))\n",
    "neg = int(len(y_train) - pos)\n",
    "imbalance_ratio = neg / max(pos, 1)\n",
    "\n",
    "# Choose explicit costs (finance logic: FN typically more costly than FP).\n",
    "# Default: match imbalance ratio -> roughly \"balanced\" effective loss.\n",
    "COST_FP = 1.0\n",
    "COST_FN = float(imbalance_ratio)\n",
    "\n",
    "w_train = np.where(np.asarray(y_train).astype(int) == 1, COST_FN, COST_FP)\n",
    "w_val_es = np.where(np.asarray(y_val_es).astype(int) == 1, COST_FN, COST_FP)\n",
    "\n",
    "# =============================================================================\n",
    "# 3) XGBOOST SPECIFICATION (strong tabular baseline; modest regularization)\n",
    "#    Key change: early stopping explicitly on PR-AUC (aucpr), not logloss.\n",
    "# =============================================================================\n",
    "base_params = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    booster=\"gbtree\",\n",
    "    tree_method=\"hist\",\n",
    "    n_estimators=5000,\n",
    "    learning_rate=0.02,\n",
    "    max_depth=4,\n",
    "    min_child_weight=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=5.0,\n",
    "    reg_alpha=0.0,\n",
    "    gamma=0.0,\n",
    "    max_delta_step=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    # IMPORTANT: do NOT set scale_pos_weight if using sample_weight for costs\n",
    "    scale_pos_weight=1.0,\n",
    ")\n",
    "\n",
    "# Early stop on PR-AUC explicitly (your learning curve peaked in aucpr before logloss)\n",
    "try:\n",
    "    es = xgb.callback.EarlyStopping(\n",
    "        rounds=200,\n",
    "        metric_name=\"aucpr\",\n",
    "        data_name=\"validation_0\",\n",
    "        maximize=True,\n",
    "        save_best=True\n",
    "    )\n",
    "    xgb_clf = XGBClassifier(**base_params, eval_metric=[\"aucpr\", \"auc\", \"logloss\"], callbacks=[es])\n",
    "    xgb_clf.fit(\n",
    "        X_train, y_train,\n",
    "        sample_weight=w_train,\n",
    "        eval_set=[(X_val_es, y_val_es)],\n",
    "        sample_weight_eval_set=[w_val_es],\n",
    "        verbose=200\n",
    "    )\n",
    "except TypeError:\n",
    "    # fallback for older API variants\n",
    "    xgb_clf = XGBClassifier(**base_params, eval_metric=[\"aucpr\", \"auc\", \"logloss\"], early_stopping_rounds=200)\n",
    "    xgb_clf.fit(\n",
    "        X_train, y_train,\n",
    "        sample_weight=w_train,\n",
    "        eval_set=[(X_val_es, y_val_es)],\n",
    "        sample_weight_eval_set=[w_val_es],\n",
    "        verbose=200\n",
    "    )\n",
    "\n",
    "# =============================================================================\n",
    "# 4) CALIBRATION (manual isotonic; avoids sklearn cv='prefit' deprecation path)\n",
    "#    Calibrate ONLY on VAL_CAL (not used for early stopping).\n",
    "# =============================================================================\n",
    "raw_cal = xgb_clf.predict_proba(X_val_cal)[:, 1]\n",
    "raw_test = xgb_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "iso.fit(raw_cal, np.asarray(y_val_cal).astype(int))\n",
    "\n",
    "val_cal_proba = iso.predict(raw_cal)\n",
    "test_proba = iso.predict(raw_test)\n",
    "\n",
    "print(\"XGBoost (cost-sensitive + early stop on VAL_ES aucpr + isotonic calib on VAL_CAL):\")\n",
    "print(\n",
    "    f\"  VAL_CAL AUC={roc_auc_score(y_val_cal, val_cal_proba):.4f} | \"\n",
    "    f\"PR-AUC={average_precision_score(y_val_cal, val_cal_proba):.4f} | \"\n",
    "    f\"Brier={brier_score_loss(y_val_cal, val_cal_proba):.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"  TEST    AUC={roc_auc_score(y_test, test_proba):.4f} | \"\n",
    "    f\"PR-AUC={average_precision_score(y_test, test_proba):.4f} | \"\n",
    "    f\"Brier={brier_score_loss(y_test, test_proba):.4f}\"\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# 5) THRESHOLDS: (i) cost-based, (ii) G-mean, (iii) capacity (top X%)\n",
    "#    Thresholds chosen on VAL_CAL; then reported on TEST.\n",
    "# =============================================================================\n",
    "grid = np.linspace(0.01, 0.99, 99)\n",
    "\n",
    "# (i) Cost-based threshold: minimize expected misclassification cost\n",
    "best_cost = {\"thr\": 0.5, \"cost\": np.inf, \"cm\": None}\n",
    "for t in grid:\n",
    "    c, cm = expected_cost(y_val_cal, val_cal_proba, t, cost_fn=COST_FN, cost_fp=COST_FP)\n",
    "    if c < best_cost[\"cost\"]:\n",
    "        best_cost.update({\"thr\": float(t), \"cost\": float(c), \"cm\": cm})\n",
    "\n",
    "# (ii) G-mean threshold (imbalance-robust diagnostic)\n",
    "best_g = {\"thr\": 0.5, \"gmean\": -1, \"tpr\": None, \"tnr\": None, \"cm\": None}\n",
    "for t in grid:\n",
    "    g, tpr, tnr, cm = gmean_tpr_tnr(y_val_cal, val_cal_proba, t)\n",
    "    if g > best_g[\"gmean\"]:\n",
    "        best_g.update({\"thr\": float(t), \"gmean\": g, \"tpr\": tpr, \"tnr\": tnr, \"cm\": cm})\n",
    "\n",
    "# (iii) Capacity rule: flag top X% by PD (common in finance screening / surveillance)\n",
    "TOP_PCT = 0.25\n",
    "thr_top = float(np.quantile(val_cal_proba, 1 - TOP_PCT))\n",
    "\n",
    "\n",
    "def report_threshold(name, thr):\n",
    "    g_val, tpr_val, tnr_val, cm_val = gmean_tpr_tnr(y_val_cal, val_cal_proba, thr)\n",
    "    g_t, tpr_t, tnr_t, cm_t = gmean_tpr_tnr(y_test, test_proba, thr)\n",
    "\n",
    "    # precision / flag rate\n",
    "    tn, fp, fn, tp = cm_t.ravel()\n",
    "    prec = tp / max(tp + fp, 1)\n",
    "    flag = (tp + fp) / max((tp + fp + tn + fn), 1)\n",
    "\n",
    "    print(f\"\\n{name}: threshold t={thr:.3f}\")\n",
    "    print(f\"  VAL_CAL  G-mean={g_val:.3f} | TPR={tpr_val:.3f} | TNR={tnr_val:.3f} | CM:\\n{cm_val}\")\n",
    "    print(\n",
    "        f\"  TEST     G-mean={g_t:.3f} | TPR={tpr_t:.3f} | TNR={tnr_t:.3f} | Precision={prec:.3f} | FlagRate={flag:.3f} | CM:\\n{cm_t}\")\n",
    "\n",
    "\n",
    "report_threshold(\"Cost-based (min expected cost; COST_FN vs COST_FP)\", best_cost[\"thr\"])\n",
    "report_threshold(\"G-mean (diagnostic)\", best_g[\"thr\"])\n",
    "report_threshold(f\"Top-{int(TOP_PCT * 100)}% capacity rule\", thr_top)\n",
    "\n",
    "# =============================================================================\n",
    "# 6) FINANCE-STYLE PD SORTS (deciles): monotonicity check + lift\n",
    "# =============================================================================\n",
    "print(\"\\nPD Deciles on VAL_CAL (1=lowest risk, 10=highest risk):\")\n",
    "print(pd_decile_table(y_val_cal, val_cal_proba, n_bins=10).to_string(index=False))\n",
    "\n",
    "print(\"\\nPD Deciles on TEST (1=lowest risk, 10=highest risk):\")\n",
    "print(pd_decile_table(y_test, test_proba, n_bins=10).to_string(index=False))\n",
    "\n",
    "# =============================================================================\n",
    "# 7) INTERPRETABILITY: “single-tree approximation” of the calibrated PD surface\n",
    "#    (practical analogue to merging rules into an interpretable tree)\n",
    "# =============================================================================\n",
    "feature_names = _safe_feature_names(X_train.shape[1])\n",
    "\n",
    "sur = DecisionTreeRegressor(\n",
    "    max_depth=4,\n",
    "    min_samples_leaf=max(50, int(0.01 * len(y_train))),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit surrogate on TRAIN predictions (do not use TEST); calibrated PD mapping included\n",
    "raw_train = xgb_clf.predict_proba(X_train)[:, 1]\n",
    "train_pd = iso.predict(raw_train)\n",
    "sur.fit(X_train, train_pd)\n",
    "\n",
    "print(\"\\nSurrogate tree (depth<=4) approximating calibrated PDs:\")\n",
    "print(export_text(sur, feature_names=feature_names))\n",
    "\n",
    "# =============================================================================\n",
    "# 8) FEATURE IMPORTANCE: prefer SHAP; fallback to gain if SHAP unavailable\n",
    "# =============================================================================\n",
    "try:\n",
    "    import shap  # optional\n",
    "\n",
    "    explainer = shap.TreeExplainer(xgb_clf)\n",
    "    idx = np.random.RandomState(42).choice(len(X_train), size=min(2000, len(X_train)), replace=False)\n",
    "    shap_values = explainer.shap_values(X_train[idx])\n",
    "\n",
    "    imp = np.mean(np.abs(shap_values), axis=0)\n",
    "    top = np.argsort(-imp)[:20]\n",
    "    print(\"\\nTop-20 features by mean(|SHAP|):\")\n",
    "    for j in top:\n",
    "        print(f\"{feature_names[j]}: {imp[j]:.6f}\")\n",
    "except Exception:\n",
    "    booster = xgb_clf.get_booster()\n",
    "    score = booster.get_score(importance_type=\"gain\")  # {\"f0\": gain, ...}\n",
    "    imp = np.zeros(X_train.shape[1], dtype=float)\n",
    "    for k, v in score.items():\n",
    "        if isinstance(k, str) and k.startswith(\"f\") and k[1:].isdigit():\n",
    "            j = int(k[1:])\n",
    "            if 0 <= j < imp.size:\n",
    "                imp[j] = float(v)\n",
    "\n",
    "    top = np.argsort(-imp)[:20]\n",
    "    print(\"\\nTop-20 features by XGBoost gain (fallback; SHAP unavailable):\")\n",
    "    for j in top:\n",
    "        print(f\"{feature_names[j]}: {imp[j]:.6f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# 6.7 Persistence dominance: correct benchmarks for (A) Surveillance vs (B) Early warning\n",
    "#   - Task A (Surveillance/Triage): allow current state as predictor; compare vs persistence\n",
    "#   - Task B (Early Warning): restrict to distress(t)=0; predict entry into distress(t+1)\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss, log_loss\n",
    ")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def _prep_df(d: pd.DataFrame, feats: list[str], target: str, require_state: bool = False) -> pd.DataFrame:\n",
    "    cols = feats + [target]\n",
    "    if require_state and \"distress_dummy\" not in cols:\n",
    "        cols = cols + [\"distress_dummy\"]\n",
    "    out = d[cols].copy()\n",
    "\n",
    "    # Coerce distress_dummy to numeric if present\n",
    "    if \"distress_dummy\" in out.columns:\n",
    "        out[\"distress_dummy\"] = pd.to_numeric(out[\"distress_dummy\"], errors=\"coerce\")\n",
    "\n",
    "    # Drop missing\n",
    "    out = out.dropna(subset=[target] + feats)\n",
    "    if require_state:\n",
    "        out = out.dropna(subset=[\"distress_dummy\"])\n",
    "\n",
    "    # Ensure target is int 0/1\n",
    "    out[target] = out[target].astype(int)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _metrics(y_true: np.ndarray, p: np.ndarray) -> dict:\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    p = np.asarray(p, dtype=float)\n",
    "\n",
    "    # Clip probabilities for stability (also keeps log_loss happy)\n",
    "    p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "\n",
    "    # handle single-class edge cases for AUC/AP\n",
    "    out = {}\n",
    "    try:\n",
    "        out[\"AUC\"] = float(roc_auc_score(y_true, p))\n",
    "    except ValueError:\n",
    "        out[\"AUC\"] = np.nan\n",
    "    try:\n",
    "        out[\"AP\"] = float(average_precision_score(y_true, p))\n",
    "    except ValueError:\n",
    "        out[\"AP\"] = np.nan\n",
    "\n",
    "    out[\"Brier\"] = float(brier_score_loss(y_true, p))\n",
    "    out[\"LogLoss\"] = float(log_loss(y_true, p, labels=[0, 1]))\n",
    "    out[\"PosRate\"] = float(np.mean(y_true)) if len(y_true) else np.nan\n",
    "    out[\"N\"] = int(len(y_true))\n",
    "    return out\n",
    "\n",
    "\n",
    "def _tune_logit(X_tr, y_tr, X_va, y_va, C_grid=(0.01, 0.1, 1.0, 10.0)):\n",
    "    best = {\"C\": None, \"va_auc\": -np.inf, \"model\": None}\n",
    "    for C in C_grid:\n",
    "        clf = LogisticRegression(\n",
    "            C=C,\n",
    "            solver=\"lbfgs\",\n",
    "            max_iter=800,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=42,\n",
    "        )\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        p_va = clf.predict_proba(X_va)[:, 1]\n",
    "        try:\n",
    "            auc = roc_auc_score(y_va, p_va)\n",
    "        except ValueError:\n",
    "            auc = np.nan\n",
    "        if np.isfinite(auc) and auc > best[\"va_auc\"]:\n",
    "            best = {\"C\": C, \"va_auc\": auc, \"model\": clf}\n",
    "    return best\n",
    "\n",
    "\n",
    "def _print_transition_matrix(name: str, d: pd.DataFrame, target_col: str):\n",
    "    tmp = d[[\"distress_dummy\", target_col]].copy()\n",
    "    tmp[\"distress_dummy\"] = pd.to_numeric(tmp[\"distress_dummy\"], errors=\"coerce\")\n",
    "    tmp = tmp.dropna(subset=[\"distress_dummy\", target_col])\n",
    "    if tmp.empty:\n",
    "        print(f\"\\n{name}: (transition matrix unavailable — missing distress_dummy/target)\")\n",
    "        return\n",
    "    tmp[\"distress_dummy\"] = tmp[\"distress_dummy\"].astype(int)\n",
    "    tmp[target_col] = tmp[target_col].astype(int)\n",
    "\n",
    "    # Rows: current state (t), Cols: next-year state (t+1)\n",
    "    mat = pd.crosstab(tmp[\"distress_dummy\"], tmp[target_col], normalize=\"index\")\n",
    "    counts = pd.crosstab(tmp[\"distress_dummy\"], tmp[target_col])\n",
    "    print(f\"\\n{name}: Transition matrix P(state(t+1) | state(t))\")\n",
    "    display(mat)\n",
    "    print(f\"{name}: Counts\")\n",
    "    display(counts)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Preconditions\n",
    "# ----------------------------\n",
    "if \"TARGET_COL\" not in globals():\n",
    "    TARGET_COL = \"target_next_year_distress\"\n",
    "if \"MODEL_FEATS\" not in globals():\n",
    "    raise RuntimeError(\"MODEL_FEATS not found. Run the preprocessing / feature setup cell first.\")\n",
    "\n",
    "# Transition matrix (this frames persistence dominance properly)\n",
    "_print_transition_matrix(\"TRAIN\", train, TARGET_COL)\n",
    "_print_transition_matrix(\"VAL\", val, TARGET_COL)\n",
    "_print_transition_matrix(\"TEST\", test, TARGET_COL)\n",
    "\n",
    "# ----------------------------\n",
    "# TASK A: Surveillance / triage\n",
    "# ----------------------------\n",
    "# Baseline: persistence (probability = 1 if distressed now else 0)\n",
    "# Models: (i) features-only, (ii) features + distress_dummy (increment over persistence)\n",
    "\n",
    "SURV_FEATS = list(MODEL_FEATS) + [\"distress_dummy\"]\n",
    "\n",
    "trainA = _prep_df(train, MODEL_FEATS, TARGET_COL, require_state=True)\n",
    "valA = _prep_df(val, MODEL_FEATS, TARGET_COL, require_state=True)\n",
    "testA = _prep_df(test, MODEL_FEATS, TARGET_COL, require_state=True)\n",
    "\n",
    "trainA_s = _prep_df(train, SURV_FEATS, TARGET_COL, require_state=True)\n",
    "valA_s = _prep_df(val, SURV_FEATS, TARGET_COL, require_state=True)\n",
    "testA_s = _prep_df(test, SURV_FEATS, TARGET_COL, require_state=True)\n",
    "\n",
    "# Persistence baseline probs\n",
    "# IMPORTANT: persistence is a *hard* classifier; using {0,1} as probabilities makes LogLoss explode.\n",
    "# Use clipped probabilities for scoring-rule comparability (AUC/AP are unaffected).\n",
    "eps = 1e-3\n",
    "p_val_persist  = np.clip(valA[\"distress_dummy\"].to_numpy(dtype=float), 0.0, 1.0)\n",
    "p_test_persist = np.clip(testA[\"distress_dummy\"].to_numpy(dtype=float), 0.0, 1.0)\n",
    "p_val_persist  = np.clip(eps + (1.0 - 2.0 * eps) * p_val_persist, eps, 1.0 - eps)\n",
    "p_test_persist = np.clip(eps + (1.0 - 2.0 * eps) * p_test_persist, eps, 1.0 - eps)\n",
    "\n",
    "# Fit features-only logit (Task A)\n",
    "Xtr = trainA[MODEL_FEATS].to_numpy(dtype=float);\n",
    "ytr = trainA[TARGET_COL].to_numpy(dtype=int)\n",
    "Xva = valA[MODEL_FEATS].to_numpy(dtype=float);\n",
    "yva = valA[TARGET_COL].to_numpy(dtype=int)\n",
    "Xte = testA[MODEL_FEATS].to_numpy(dtype=float);\n",
    "yte = testA[TARGET_COL].to_numpy(dtype=int)\n",
    "\n",
    "bestA = _tune_logit(Xtr, ytr, Xva, yva)\n",
    "logitA = bestA[\"model\"]\n",
    "p_val_A = logitA.predict_proba(Xva)[:, 1]\n",
    "p_test_A = logitA.predict_proba(Xte)[:, 1]\n",
    "\n",
    "# Fit state-augmented logit (Task A incremental)\n",
    "Xtr_s = trainA_s[SURV_FEATS].to_numpy(dtype=float)\n",
    "Xva_s = valA_s[SURV_FEATS].to_numpy(dtype=float)\n",
    "Xte_s = testA_s[SURV_FEATS].to_numpy(dtype=float)\n",
    "ytr_s = trainA_s[TARGET_COL].to_numpy(dtype=int)\n",
    "yva_s = valA_s[TARGET_COL].to_numpy(dtype=int)\n",
    "yte_s = testA_s[TARGET_COL].to_numpy(dtype=int)\n",
    "\n",
    "bestA_s = _tune_logit(Xtr_s, ytr_s, Xva_s, yva_s)\n",
    "logitA_s = bestA_s[\"model\"]\n",
    "p_val_A_s = logitA_s.predict_proba(Xva_s)[:, 1]\n",
    "p_test_A_s = logitA_s.predict_proba(Xte_s)[:, 1]\n",
    "\n",
    "rows = []\n",
    "rows.append((\"Task A (Surveillance)\", \"VAL\", \"Persistence\", *_metrics(yva, p_val_persist).values()))\n",
    "rows.append((\"Task A (Surveillance)\", \"VAL\", f\"Logit (feats-only) C={bestA['C']}\", *_metrics(yva, p_val_A).values()))\n",
    "rows.append(\n",
    "    (\"Task A (Surveillance)\", \"VAL\", f\"Logit (feats+state) C={bestA_s['C']}\", *_metrics(yva_s, p_val_A_s).values()))\n",
    "\n",
    "rows.append((\"Task A (Surveillance)\", \"TEST\", \"Persistence\", *_metrics(yte, p_test_persist).values()))\n",
    "rows.append((\"Task A (Surveillance)\", \"TEST\", f\"Logit (feats-only) C={bestA['C']}\", *_metrics(yte, p_test_A).values()))\n",
    "rows.append(\n",
    "    (\"Task A (Surveillance)\", \"TEST\", f\"Logit (feats+state) C={bestA_s['C']}\", *_metrics(yte_s, p_test_A_s).values()))\n",
    "\n",
    "# ----------------------------\n",
    "# TASK B: Early warning (entry into distress)\n",
    "# ----------------------------\n",
    "# Restrict to firms not distressed at t; target remains distress(t+1)=1\n",
    "trainB = _prep_df(train, MODEL_FEATS + [\"distress_dummy\"], TARGET_COL, require_state=True)\n",
    "valB = _prep_df(val, MODEL_FEATS + [\"distress_dummy\"], TARGET_COL, require_state=True)\n",
    "testB = _prep_df(test, MODEL_FEATS + [\"distress_dummy\"], TARGET_COL, require_state=True)\n",
    "\n",
    "trainB = trainB[trainB[\"distress_dummy\"].astype(int) == 0].copy()\n",
    "valB = valB[valB[\"distress_dummy\"].astype(int) == 0].copy()\n",
    "testB = testB[testB[\"distress_dummy\"].astype(int) == 0].copy()\n",
    "\n",
    "# Baselines for Task B\n",
    "# - Always-0 (prob=0)\n",
    "# - Constant transition rate from TRAIN (prob = mean(y_train_B))\n",
    "ytrB = trainB[TARGET_COL].to_numpy(dtype=int)\n",
    "p_tr_rate = float(np.mean(ytrB)) if len(ytrB) else 0.0\n",
    "\n",
    "\n",
    "def _eval_taskB(split_name, dB):\n",
    "    y = dB[TARGET_COL].to_numpy(dtype=int)\n",
    "    p0 = np.zeros_like(y, dtype=float)\n",
    "    pconst = np.full_like(y, fill_value=p_tr_rate, dtype=float)\n",
    "\n",
    "    # Fit features-only logit on Task B\n",
    "    XtrB = trainB[MODEL_FEATS].to_numpy(dtype=float)\n",
    "    XvaB = valB[MODEL_FEATS].to_numpy(dtype=float)\n",
    "    XteB = testB[MODEL_FEATS].to_numpy(dtype=float)\n",
    "\n",
    "    yvaB = valB[TARGET_COL].to_numpy(dtype=int)\n",
    "    yteB = testB[TARGET_COL].to_numpy(dtype=int)\n",
    "\n",
    "    bestB = _tune_logit(XtrB, ytrB, XvaB, yvaB)\n",
    "    logitB = bestB[\"model\"]\n",
    "\n",
    "    if split_name == \"VAL\":\n",
    "        p = logitB.predict_proba(XvaB)[:, 1]\n",
    "        return bestB, [\n",
    "            (\"Task B (Early warning)\", \"VAL\", \"Always-0\", *_metrics(yvaB, p0).values()),\n",
    "            (\"Task B (Early warning)\", \"VAL\", f\"Const rate={p_tr_rate:.4f}\", *_metrics(yvaB, pconst).values()),\n",
    "            (\"Task B (Early warning)\", \"VAL\", f\"Logit (feats-only) C={bestB['C']}\", *_metrics(yvaB, p).values()),\n",
    "        ]\n",
    "    else:\n",
    "        p = logitB.predict_proba(XteB)[:, 1]\n",
    "        return bestB, [\n",
    "            (\"Task B (Early warning)\", \"TEST\", \"Always-0\", *_metrics(yteB, p0).values()),\n",
    "            (\"Task B (Early warning)\", \"TEST\", f\"Const rate={p_tr_rate:.4f}\", *_metrics(yteB, pconst).values()),\n",
    "            (\"Task B (Early warning)\", \"TEST\", f\"Logit (feats-only) C={bestB['C']}\", *_metrics(yteB, p).values()),\n",
    "        ]\n",
    "\n",
    "\n",
    "bestB, rowsB_val = _eval_taskB(\"VAL\", valB)\n",
    "_, rowsB_tst = _eval_taskB(\"TEST\", testB)\n",
    "\n",
    "rows.extend(rowsB_val)\n",
    "rows.extend(rowsB_tst)\n",
    "\n",
    "# ----------------------------\n",
    "# Present results\n",
    "# ----------------------------\n",
    "cols = [\"Task\", \"Split\", \"Model\", \"AUC\", \"AP\", \"Brier\", \"LogLoss\", \"PosRate\", \"N\"]\n",
    "results = pd.DataFrame(rows, columns=cols)\n",
    "\n",
    "# make numeric columns numeric (for sorting/formatting)\n",
    "for c in [\"AUC\", \"AP\", \"Brier\", \"LogLoss\", \"PosRate\"]:\n",
    "    results[c] = pd.to_numeric(results[c], errors=\"coerce\")\n",
    "\n",
    "display(results)\n"
   ],
   "id": "ff07c7b63bb8438a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b026ec8d",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 7.1 Baseline distress rate, event lift, and year clustering (distribution-aware)\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Robust references ---\n",
    "_df = df_split.copy() if \"df_split\" in globals() else (df_events.copy() if \"df_events\" in globals() else df.copy())\n",
    "_target = TARGET_COL if \"TARGET_COL\" in globals() else (\n",
    "    \"target_next_year_distress\" if \"target_next_year_distress\" in _df.columns else \"distress_dummy\"\n",
    ")\n",
    "if _target not in _df.columns:\n",
    "    raise KeyError(f\"Target not found. Expected TARGET_COL or 'target_next_year_distress'/'distress_dummy'. Got: {_target}\")\n",
    "\n",
    "_evt_cols = [c for c in _df.columns if c.startswith(\"evt_\")]\n",
    "if not _evt_cols:\n",
    "    raise ValueError(\"No event columns found (expected columns starting with 'evt_').\")\n",
    "\n",
    "# Year for reporting (feature-year)\n",
    "if \"fyear\" in _df.columns:\n",
    "    _year = _df[\"fyear\"]\n",
    "else:\n",
    "    # label_year = fyear + 1 (in this project); use feature-year = label_year - 1\n",
    "    _year = _df[\"label_year\"] - 1 if \"label_year\" in _df.columns else pd.Series(np.nan, index=_df.index)\n",
    "\n",
    "base_rate = float(pd.to_numeric(_df[_target], errors=\"coerce\").mean())\n",
    "print(f\"Baseline distress rate (overall): {base_rate:.4%}  |  N={len(_df):,}\")\n",
    "\n",
    "def event_lift_table(df_in: pd.DataFrame, target: str, event_cols: list[str]) -> pd.DataFrame:\n",
    "    y = pd.to_numeric(df_in[target], errors=\"coerce\")\n",
    "    base = float(y.mean())\n",
    "    rows = []\n",
    "    for e in event_cols:\n",
    "        s = pd.to_numeric(df_in[e], errors=\"coerce\").fillna(0).astype(int)\n",
    "        n = int(s.sum())\n",
    "        prev = float(s.mean())\n",
    "        cond = float(y[s == 1].mean()) if n > 0 else np.nan\n",
    "        lift = (cond / base) if (pd.notna(cond) and base > 0) else np.nan\n",
    "        pp = (cond - base) if pd.notna(cond) else np.nan\n",
    "        rows.append((e, n, prev, cond, lift, pp))\n",
    "    out = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\"event\", \"n_events\", \"event_rate\", \"distress_rate_given_event\", \"lift_vs_base\", \"pp_diff_vs_base\"],\n",
    "    )\n",
    "    return out.sort_values([\"lift_vs_base\", \"n_events\"], ascending=[False, False]).reset_index(drop=True)\n",
    "\n",
    "lift_all = event_lift_table(_df, _target, _evt_cols)\n",
    "display(lift_all.head(20))\n",
    "\n",
    "# --- Year clustering ---\n",
    "year_tbl = (\n",
    "    _df.assign(feature_year=_year)\n",
    "    .groupby(\"feature_year\", dropna=True)[[_target] + _evt_cols]\n",
    "    .mean(numeric_only=True)\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Plot: distress rate + a few key event rates (auto-select top by lift)\n",
    "top_events = list(lift_all[\"event\"].head(6).values)\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(year_tbl.index, year_tbl[_target], marker=\"o\")\n",
    "ax.set_title(\"Distress rate by feature-year\")\n",
    "ax.set_xlabel(\"Feature year\")\n",
    "ax.set_ylabel(\"Rate\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for e in top_events:\n",
    "    if e in year_tbl.columns:\n",
    "        ax.plot(year_tbl.index, year_tbl[e], marker=\"o\", label=e)\n",
    "ax.set_title(\"Top event rates by feature-year (top by lift)\")\n",
    "ax.set_xlabel(\"Feature year\")\n",
    "ax.set_ylabel(\"Event rate\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc=\"best\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "52e2db18",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 7.2 Event co-occurrence and transitions (anticipation)\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_df = df_split.copy() if \"df_split\" in globals() else df_events.copy()\n",
    "_target = TARGET_COL if \"TARGET_COL\" in globals() else \"target_next_year_distress\"\n",
    "_evt_cols = [c for c in _df.columns if c.startswith(\"evt_\")]\n",
    "\n",
    "# Feature-year for ordering\n",
    "if \"fyear\" in _df.columns:\n",
    "    _df[\"_feature_year\"] = _df[\"fyear\"]\n",
    "elif \"label_year\" in _df.columns:\n",
    "    _df[\"_feature_year\"] = _df[\"label_year\"] - 1\n",
    "else:\n",
    "    raise KeyError(\"Need 'fyear' or 'label_year' to compute event transitions.\")\n",
    "\n",
    "if \"firm_id\" not in _df.columns:\n",
    "    raise KeyError(\"Need 'firm_id' to compute firm-level event transitions.\")\n",
    "\n",
    "E = _df[_evt_cols].fillna(0).astype(int)\n",
    "\n",
    "# --- Co-occurrence: Jaccard similarity (intersection/union) for binaries ---\n",
    "A = E.to_numpy(dtype=int)\n",
    "inter = A.T @ A  # intersection counts\n",
    "marg = A.sum(axis=0)\n",
    "union = marg.reshape(-1, 1) + marg.reshape(1, -1) - inter\n",
    "jacc = np.divide(inter, np.maximum(union, 1), dtype=float)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "im = ax.imshow(jacc, aspect=\"auto\")\n",
    "ax.set_title(\"Event co-occurrence (Jaccard similarity)\")\n",
    "ax.set_xticks(range(len(_evt_cols)))\n",
    "ax.set_yticks(range(len(_evt_cols)))\n",
    "ax.set_xticklabels(_evt_cols, rotation=90, fontsize=7)\n",
    "ax.set_yticklabels(_evt_cols, fontsize=7)\n",
    "fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Transitions: activation (0→1) and persistence (1→1) ---\n",
    "tmp = _df[[\"firm_id\", \"_feature_year\"] + _evt_cols].sort_values([\"firm_id\", \"_feature_year\"]).copy()\n",
    "for e in _evt_cols:\n",
    "    tmp[e + \"_next\"] = tmp.groupby(\"firm_id\")[e].shift(-1)\n",
    "\n",
    "rows = []\n",
    "for e in _evt_cols:\n",
    "    cur = tmp[e].fillna(0).astype(int)\n",
    "    nxt = tmp[e + \"_next\"]\n",
    "    m = nxt.notna()\n",
    "    cur_m = cur[m]\n",
    "    nxt_m = nxt[m].fillna(0).astype(int)\n",
    "\n",
    "    # activation: P(next=1 | current=0)\n",
    "    denom01 = int((cur_m == 0).sum())\n",
    "    p01 = float((nxt_m[(cur_m == 0)] == 1).mean()) if denom01 > 0 else np.nan\n",
    "\n",
    "    # persistence: P(next=1 | current=1)\n",
    "    denom11 = int((cur_m == 1).sum())\n",
    "    p11 = float((nxt_m[(cur_m == 1)] == 1).mean()) if denom11 > 0 else np.nan\n",
    "\n",
    "    rows.append((e, denom01, p01, denom11, p11))\n",
    "\n",
    "trans_tbl = pd.DataFrame(rows, columns=[\"event\", \"n_cur0\", \"p_activate_01\", \"n_cur1\", \"p_persist_11\"])\n",
    "trans_tbl = trans_tbl.sort_values(\"p_activate_01\", ascending=False).reset_index(drop=True)\n",
    "display(trans_tbl.head(20))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "65ebee1b",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 7.3 Tree-based explanations: SHAP focus on event binaries + model-implied ΔPD toggles\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Preconditions: trained tree model and calibration (from section 6.6)\n",
    "if \"xgb_clf\" not in globals():\n",
    "    raise RuntimeError(\"xgb_clf not found. Run the tree-based benchmark cell (6.6) first.\")\n",
    "if \"iso\" not in globals():\n",
    "    print(\"Warning: isotonic calibrator 'iso' not found. Proceeding with raw XGB probabilities.\")\n",
    "if \"MODEL_FEATS\" not in globals():\n",
    "    raise RuntimeError(\"MODEL_FEATS not found. Run the preprocessing / feature setup cells first.\")\n",
    "if \"test\" not in globals():\n",
    "    raise RuntimeError(\"test DataFrame not found.\")\n",
    "\n",
    "_evt_feats = [c for c in MODEL_FEATS if c.startswith(\"evt_\")]\n",
    "if not _evt_feats:\n",
    "    raise ValueError(\"No event binaries found in MODEL_FEATS. Expected event features to be included as raw binaries.\")\n",
    "\n",
    "def _predict_pd(df_rows: pd.DataFrame) -> np.ndarray:\n",
    "    X = df_rows[MODEL_FEATS].to_numpy(dtype=float)\n",
    "    raw = xgb_clf.predict_proba(X)[:, 1]\n",
    "    if \"iso\" in globals():\n",
    "        return np.asarray(iso.predict(raw), dtype=float)\n",
    "    return np.asarray(raw, dtype=float)\n",
    "\n",
    "# Sample for explanation\n",
    "N = min(3000, len(test))\n",
    "sample = test.sample(N, random_state=42).copy()\n",
    "sample_pd = _predict_pd(sample)\n",
    "\n",
    "# --- 7.3a Observed conditional PD by event (descriptive) ---\n",
    "rows = []\n",
    "for e in _evt_feats:\n",
    "    s = sample[e].fillna(0).astype(int)\n",
    "    n1 = int(s.sum())\n",
    "    if n1 == 0 or n1 == len(sample):\n",
    "        continue\n",
    "    pd1 = float(sample_pd[s == 1].mean())\n",
    "    pd0 = float(sample_pd[s == 0].mean())\n",
    "    rows.append((e, n1, float(s.mean()), pd0, pd1, pd1 - pd0))\n",
    "\n",
    "obs_pd_tbl = pd.DataFrame(\n",
    "    rows, columns=[\"event\", \"n_event1\", \"event_rate_in_sample\", \"mean_PD_if_0\", \"mean_PD_if_1\", \"PD_diff_1_minus_0\"]\n",
    ").sort_values(\"PD_diff_1_minus_0\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "display(obs_pd_tbl.head(20))\n",
    "\n",
    "# --- 7.3b Model-implied ΔPD toggles (counterfactual within the model; not causal) ---\n",
    "def toggle_delta_pd(df_rows: pd.DataFrame, event_col: str, new_value: int) -> np.ndarray:\n",
    "    tmp = df_rows.copy()\n",
    "    tmp[event_col] = int(new_value)\n",
    "    return _predict_pd(tmp)\n",
    "\n",
    "toggle_rows = []\n",
    "for e in _evt_feats:\n",
    "    s = sample[e].fillna(0).astype(int)\n",
    "\n",
    "    # 0 -> 1 toggle among those currently 0\n",
    "    m0 = (s == 0)\n",
    "    if m0.any():\n",
    "        pd_base0 = sample_pd[m0]\n",
    "        pd_t1 = toggle_delta_pd(sample.loc[m0, MODEL_FEATS], e, 1)\n",
    "        d01 = float(np.mean(pd_t1 - pd_base0))\n",
    "    else:\n",
    "        d01 = np.nan\n",
    "\n",
    "    # 1 -> 0 toggle among those currently 1\n",
    "    m1 = (s == 1)\n",
    "    if m1.any():\n",
    "        pd_base1 = sample_pd[m1]\n",
    "        pd_t0 = toggle_delta_pd(sample.loc[m1, MODEL_FEATS], e, 0)\n",
    "        d10 = float(np.mean(pd_t0 - pd_base1))\n",
    "    else:\n",
    "        d10 = np.nan\n",
    "\n",
    "    toggle_rows.append((e, float(s.mean()), d01, d10))\n",
    "\n",
    "toggle_tbl = pd.DataFrame(\n",
    "    toggle_rows,\n",
    "    columns=[\"event\", \"event_rate_in_sample\", \"avg_ΔPD_if_toggle_0_to_1\", \"avg_ΔPD_if_toggle_1_to_0\"],\n",
    ").sort_values(\"avg_ΔPD_if_toggle_0_to_1\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "display(toggle_tbl.head(25))\n",
    "\n",
    "# --- Optional: SHAP values (can be slow; uses a smaller sample) ---\n",
    "try:\n",
    "    import shap\n",
    "\n",
    "    N_SHAP = min(1500, len(sample))\n",
    "    shap_sample = sample.iloc[:N_SHAP].copy()\n",
    "\n",
    "    explainer = shap.TreeExplainer(xgb_clf)\n",
    "    shap_values = explainer.shap_values(shap_sample[MODEL_FEATS].to_numpy(dtype=float))\n",
    "    # shap_values is in log-odds space for binary classifiers (by default)\n",
    "    shap_df = pd.DataFrame(shap_values, columns=MODEL_FEATS)\n",
    "\n",
    "    # Summarize absolute SHAP contribution for event features\n",
    "    evt_shap = (\n",
    "        shap_df[_evt_feats]\n",
    "        .abs()\n",
    "        .mean()\n",
    "        .sort_values(ascending=False)\n",
    "        .rename(\"mean_abs_shap_logit\")\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"event\"})\n",
    "    )\n",
    "    display(evt_shap.head(20))\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"SHAP computation skipped or failed:\", repr(e))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0ca6c0ba",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 7.4 Accounting-consistent pro forma scenarios (simulation for a single firm-year)\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if \"xgb_clf\" not in globals():\n",
    "    raise RuntimeError(\"xgb_clf not found. Run the tree-based benchmark cell (6.6) first.\")\n",
    "if \"MODEL_FEATS\" not in globals():\n",
    "    raise RuntimeError(\"MODEL_FEATS not found. Run preprocessing / feature setup cells first.\")\n",
    "if \"continuous_feats\" not in globals():\n",
    "    raise RuntimeError(\"continuous_feats not found. Expected it from preprocessing cell.\")\n",
    "if \"event_feats\" not in globals():\n",
    "    # fall back: infer from MODEL_FEATS\n",
    "    event_feats = [c for c in MODEL_FEATS if c.startswith(\"evt_\")]\n",
    "\n",
    "# Required objects from preprocessing (winsor bounds + scaler)\n",
    "if \"bounds\" not in globals():\n",
    "    raise RuntimeError(\"winsorization bounds not found (expected 'bounds' dict from preprocessing cell).\")\n",
    "if \"scaler\" not in globals():\n",
    "    raise RuntimeError(\"scaler not found (expected StandardScaler 'scaler' from preprocessing cell).\")\n",
    "\n",
    "_df = df_split.copy() if \"df_split\" in globals() else df_events.copy()\n",
    "\n",
    "\n",
    "# Utility: calibrated PD\n",
    "def _predict_pd_from_features(df_rows: pd.DataFrame) -> float | np.ndarray:\n",
    "    proba = xgb_clf.predict_proba(df_rows[MODEL_FEATS])[:, 1]\n",
    "    return proba\n",
    "\n",
    "\n",
    "def _recompute_events_one(raw_row: pd.Series, lag_row: pd.Series | None) -> dict:\n",
    "    \"\"\"Recompute event flags for a single row using the same logic as the event cell.\"\"\"\n",
    "    out = {}\n",
    "\n",
    "    # Pull thresholds from the earlier event cell if available; otherwise use conservative defaults\n",
    "    cut_thr = float(globals().get(\"cut_threshold\", -0.25))\n",
    "    cov_drop_thr = float(globals().get(\"cov_drop_thr\", 0.7))\n",
    "    dlev_thr = float(globals().get(\"dlev_thr\", 5.0))\n",
    "    doibdp_thr = globals().get(\"doibdp_q\", np.nan)\n",
    "    if isinstance(doibdp_thr, (pd.Series, pd.DataFrame)):\n",
    "        doibdp_thr = float(pd.to_numeric(doibdp_thr, errors=\"coerce\"))\n",
    "    cfo_drop_thr = float(globals().get(\"cfo_drop_thr\", 0.7))\n",
    "\n",
    "    def _num(x):\n",
    "        return pd.to_numeric(x, errors=\"coerce\")\n",
    "\n",
    "    def _finite(x) -> bool:\n",
    "        try:\n",
    "            return np.isfinite(float(x))\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def g(name, default=np.nan):\n",
    "        return _num(raw_row.get(name, default))\n",
    "\n",
    "    def glag(name, default=np.nan):\n",
    "        if lag_row is None:\n",
    "            return _num(default)\n",
    "        return _num(lag_row.get(name, default))\n",
    "\n",
    "    # Dividend moments\n",
    "    dv = g(\"dv\", 0.0)\n",
    "    dv_l1 = glag(\"dv\", 0.0)\n",
    "    pct = (float(dv - dv_l1) / float(dv_l1)) if (_finite(dv) and _finite(dv_l1) and float(dv_l1) != 0.0) else np.nan\n",
    "    out[\"evt_div_cut\"] = int((_finite(dv_l1) and float(dv_l1) > 0) and pd.notna(pct) and (pct <= cut_thr))\n",
    "    out[\"evt_div_suspend\"] = int((_finite(dv_l1) and float(dv_l1) > 0) and (_finite(dv) and float(dv) <= 0))\n",
    "    out[\"evt_div_initiate\"] = int((not _finite(dv_l1) or float(dv_l1) <= 0) and (_finite(dv) and float(dv) > 0))\n",
    "\n",
    "    # Coverage (RAW interest coverage ratio)\n",
    "    # NOTE: sp_interest_coverage is a signed log1p-transformed quantity (with caps),\n",
    "    # so thresholding it at 1.0 is economically incorrect. Events are therefore computed on the RAW ratio.\n",
    "\n",
    "    cov_raw = g(\"sp_interest_coverage_raw\", np.nan)\n",
    "    cov_raw_l1 = glag(\"sp_interest_coverage_raw\", np.nan)\n",
    "\n",
    "    # Fallback reconstruction (if raw coverage column is absent in the selected row(s))\n",
    "    if not _finite(cov_raw):\n",
    "        oibdp_now = g(\"oibdp\", np.nan)\n",
    "        xint_now = g(\"xint\", np.nan)\n",
    "        denom_now = max(abs(float(xint_now)), 1.0) if _finite(xint_now) else np.nan\n",
    "        cov_raw = (float(oibdp_now) / denom_now) if (_finite(oibdp_now) and np.isfinite(denom_now) and denom_now != 0.0) else np.nan\n",
    "\n",
    "    if not _finite(cov_raw_l1):\n",
    "        oibdp_prev = glag(\"oibdp\", np.nan)\n",
    "        xint_prev = glag(\"xint\", np.nan)\n",
    "        denom_prev = max(abs(float(xint_prev)), 1.0) if _finite(xint_prev) else np.nan\n",
    "        cov_raw_l1 = (float(oibdp_prev) / denom_prev) if (_finite(oibdp_prev) and np.isfinite(denom_prev) and denom_prev != 0.0) else np.nan\n",
    "\n",
    "    out[\"evt_cov_breach\"] = int(_finite(cov_raw) and (float(cov_raw) < 1.0))  # EBITDA/|interest| < 1 (raw)\n",
    "    cov_ratio = (cov_raw / cov_raw_l1) if (_finite(cov_raw) and _finite(cov_raw_l1) and float(cov_raw_l1) != 0.0) else np.nan\n",
    "    out[\"evt_cov_collapse\"] = int(pd.notna(cov_ratio) and (cov_ratio < cov_drop_thr))\n",
    "\n",
    "    # Leverage spike (debt-to-capital) — guard against inf - inf\n",
    "    lev = g(\"sp_debt_to_capital\", np.nan)\n",
    "    lev_l1 = glag(\"sp_debt_to_capital\", np.nan)\n",
    "    dlev = (float(lev) - float(lev_l1)) if (_finite(lev) and _finite(lev_l1)) else np.nan\n",
    "    out[\"evt_lev_spike\"] = int(pd.notna(dlev) and (dlev >= dlev_thr))\n",
    "\n",
    "    # Liquidity squeeze (act/lct and quick ratio)\n",
    "    act = g(\"act\", np.nan)\n",
    "    lct = g(\"lct\", np.nan)\n",
    "    invt = g(\"invt\", 0.0)\n",
    "    cr = (act / lct) if (_finite(act) and _finite(lct) and float(lct) != 0.0) else np.nan\n",
    "    qr = ((act - invt) / lct) if (_finite(act) and _finite(lct) and float(lct) != 0.0 and _finite(invt)) else np.nan\n",
    "    out[\"evt_liquidity_squeeze\"] = int(pd.notna(cr) and (cr < 1.0))\n",
    "    out[\"evt_quick_squeeze\"] = int(pd.notna(qr) and (qr < 0.8))\n",
    "\n",
    "    # EBITDA and CFO stress\n",
    "    oibdp = g(\"oibdp\", np.nan)\n",
    "    oibdp_l1 = glag(\"oibdp\", np.nan)\n",
    "    doibdp = (oibdp / oibdp_l1) if (_finite(oibdp) and _finite(oibdp_l1) and float(oibdp_l1) != 0.0) else np.nan\n",
    "    out[\"evt_ebitda_drop\"] = int(pd.notna(doibdp) and (doibdp < float(doibdp_thr) if np.isfinite(doibdp_thr) else doibdp < 0.7))\n",
    "    out[\"evt_ebitda_neg\"] = int(_finite(oibdp) and float(oibdp) < 0)\n",
    "\n",
    "    oancf = g(\"oancf\", np.nan)\n",
    "    oancf_l1 = glag(\"oancf\", np.nan)\n",
    "    dcfo = (oancf / oancf_l1) if (_finite(oancf) and _finite(oancf_l1) and float(oancf_l1) != 0.0) else np.nan\n",
    "    out[\"evt_cfo_drop\"] = int(pd.notna(dcfo) and (dcfo < cfo_drop_thr))\n",
    "    out[\"evt_cfo_neg\"] = int(_finite(oancf) and float(oancf) < 0)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _prepare_feature_row(raw_row: pd.Series, lag_row: pd.Series | None) -> pd.DataFrame:\n",
    "    \"\"\"Given a raw row, recompute events, winsorize continuous feats, scale to z, and return 1-row DF with MODEL_FEATS.\"\"\"\n",
    "    one = raw_row.copy()\n",
    "\n",
    "    # Recompute events (only overwrite those present)\n",
    "    ev = _recompute_events_one(one, lag_row)\n",
    "    for k, v in ev.items():\n",
    "        if k in _df.columns:\n",
    "            one[k] = v\n",
    "\n",
    "    # Winsorize continuous features using TRAIN bounds (stored in `bounds`)\n",
    "    for c, (lo, hi) in bounds.items():\n",
    "        if c in one.index:\n",
    "            v = pd.to_numeric(one[c], errors=\"coerce\")\n",
    "            v = np.nan if not np.isfinite(v) else float(v)\n",
    "            if pd.notna(v) and np.isfinite(lo) and np.isfinite(hi):\n",
    "                one[c] = float(np.clip(v, lo, hi))\n",
    "            else:\n",
    "                one[c] = v\n",
    "\n",
    "    # Build a 1-row df\n",
    "    one_df = pd.DataFrame([one])\n",
    "\n",
    "    # Standardize continuous feats to z_ columns\n",
    "    one_df[continuous_feats] = one_df[continuous_feats].replace([np.inf, -np.inf], np.nan)\n",
    "    one_df[continuous_feats] = one_df[continuous_feats].fillna(pd.DataFrame([_df[continuous_feats].median(numeric_only=True)]).iloc[0])\n",
    "\n",
    "    z_cols = [f\"z_{c}\" for c in continuous_feats]\n",
    "    one_df[z_cols] = scaler.transform(one_df[continuous_feats].to_numpy(dtype=float))\n",
    "\n",
    "    # Ensure event features exist\n",
    "    for e in event_feats:\n",
    "        if e not in one_df.columns:\n",
    "            one_df[e] = 0\n",
    "\n",
    "    # Final feature row\n",
    "    return one_df\n",
    "\n",
    "\n",
    "# Pick a representative firm-year for scenario (highest PD in TEST if available)\n",
    "if \"test\" in globals() and \"test_proba\" in globals():\n",
    "    idx = int(np.argmax(test_proba))\n",
    "    base_row = test.iloc[idx].copy()\n",
    "else:\n",
    "    base_row = _df.sample(1, random_state=42).iloc[0].copy()\n",
    "\n",
    "# Pull lagged row if available\n",
    "lag_row = None\n",
    "if \"firm_id\" in _df.columns and \"fyear\" in _df.columns and \"firm_id\" in base_row.index:\n",
    "    fid = base_row[\"firm_id\"]\n",
    "    y = int(base_row[\"fyear\"])\n",
    "    lag_match = _df[(_df[\"firm_id\"] == fid) & (_df[\"fyear\"] == (y - 1))]\n",
    "    if len(lag_match) > 0:\n",
    "        lag_row = lag_match.iloc[0].copy()\n",
    "\n",
    "# Base PD\n",
    "base_feat = _prepare_feature_row(base_row, lag_row)\n",
    "base_pd = float(_predict_pd_from_features(base_feat)[0])\n",
    "\n",
    "# Scenarios (simple, interpretable pro-forma adjustments)\n",
    "\n",
    "scenarios = {\n",
    "    \"Base\": {},\n",
    "    \"Dividend suspension (dv→0)\": {\"dv\": 0.0},\n",
    "    # Note: leverage/coverage scenarios intentionally dropped because leverage/coverage channels are excluded\n",
    "    # from MODEL_FEATS in this publication specification (to avoid circularity with the label definition).\n",
    "    \"Liquidity buffer (CR>=1.2)\": {},  # handled below\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, adj in scenarios.items():\n",
    "    row_s = base_row.copy()\n",
    "\n",
    "    # Apply direct adjustments\n",
    "    for k, v in adj.items():\n",
    "        if pd.notna(v) and np.isfinite(v):\n",
    "            row_s[k] = v\n",
    "\n",
    "    # Liquidity buffer: minimally raise ACT to reach current ratio >= 1.2 (if inputs exist)\n",
    "    if name == \"Liquidity buffer (CR>=1.2)\":\n",
    "        act = pd.to_numeric(row_s.get(\"act\", np.nan), errors=\"coerce\")\n",
    "        lct = pd.to_numeric(row_s.get(\"lct\", np.nan), errors=\"coerce\")\n",
    "        if np.isfinite(act) and np.isfinite(lct) and lct > 0:\n",
    "            target_act = 1.2 * lct\n",
    "            row_s[\"act\"] = float(max(act, target_act))\n",
    "\n",
    "    feat = _prepare_feature_row(row_s, lag_row)\n",
    "\n",
    "    # Audit: ensure the scenario actually moves at least one MODEL_FEATS column.\n",
    "    def _neq(a, b):\n",
    "        # NaN-safe inequality\n",
    "        if pd.isna(a) and pd.isna(b):\n",
    "            return False\n",
    "        try:\n",
    "            return float(a) != float(b)\n",
    "        except Exception:\n",
    "            return a != b\n",
    "\n",
    "    changed = [c for c in MODEL_FEATS if _neq(feat[c].iloc[0], base_feat[c].iloc[0])]\n",
    "    pd_s = float(_predict_pd_from_features(feat)[0])\n",
    "\n",
    "    results.append((name, pd_s, pd_s - base_pd, \", \".join(changed) if changed else \"(no MODEL_FEATS change)\"))\n",
    "\n",
    "scen_tbl = (\n",
    "    pd.DataFrame(results, columns=[\"Scenario\", \"PD\", \"ΔPD vs Base\", \"MODEL_FEATS changed\"])\n",
    "      .sort_values(\"PD\", ascending=False)\n",
    ")\n",
    "print(f\"Base PD: {base_pd:.4f}\")\n",
    "display(scen_tbl)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "48cece52",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 7.5 Decision curves (net benefit) + cost curves (screening policy)\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use calibrated tree PDs if available; otherwise fall back to logistic\n",
    "if \"y_test\" in globals() and \"test_proba\" in globals():\n",
    "    y = np.asarray(y_test).astype(int)\n",
    "    p = np.asarray(test_proba).astype(float)\n",
    "    model_name = \"XGBoost (calibrated)\"\n",
    "elif \"test\" in globals() and \"pred_test\" in globals() and \"p_distress_logit\" in pred_test.columns:\n",
    "    # If you loaded predictions table\n",
    "    y = np.asarray(pred_test[TARGET_COL]).astype(int)\n",
    "    p = np.asarray(pred_test[\"p_distress_logit\"]).astype(float)\n",
    "    model_name = \"Logit (probability)\"\n",
    "elif \"test\" in globals() and \"TARGET_COL\" in globals() and \"test_proba\" in globals():\n",
    "    y = np.asarray(test[TARGET_COL]).astype(int)\n",
    "    p = np.asarray(test_proba).astype(float)\n",
    "    model_name = \"Model\"\n",
    "else:\n",
    "    raise RuntimeError(\"No TEST probabilities found. Run model cells first.\")\n",
    "\n",
    "def net_benefit(y_true, proba, thr):\n",
    "    y_hat = (proba >= thr).astype(int)\n",
    "    tp = int(((y_hat == 1) & (y_true == 1)).sum())\n",
    "    fp = int(((y_hat == 1) & (y_true == 0)).sum())\n",
    "    n = len(y_true)\n",
    "    w = thr / max(1 - thr, 1e-12)  # harm/benefit trade-off implied by threshold\n",
    "    return (tp / n) - (fp / n) * w\n",
    "\n",
    "ths = np.linspace(0.01, 0.60, 60)\n",
    "nb_model = np.array([net_benefit(y, p, t) for t in ths])\n",
    "\n",
    "# Treat-all and treat-none baselines for decision curve analysis\n",
    "prev = y.mean()\n",
    "nb_all = prev - (1 - prev) * (ths / (1 - ths))\n",
    "nb_none = np.zeros_like(ths)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "ax.plot(ths, nb_model, label=model_name)\n",
    "ax.plot(ths, nb_all, linestyle=\"--\", label=\"Treat all\")\n",
    "ax.plot(ths, nb_none, linestyle=\"--\", label=\"Treat none\")\n",
    "ax.set_title(\"Decision curve (Net Benefit)\")\n",
    "ax.set_xlabel(\"Threshold probability\")\n",
    "ax.set_ylabel(\"Net benefit\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "# Cost curve: expected cost under FP/FN costs\n",
    "C_FN = float(globals().get(\"C_FN\", 5.0))  # default: FN 5x as costly as FP\n",
    "C_FP = float(globals().get(\"C_FP\", 1.0))\n",
    "\n",
    "def expected_cost(y_true, proba, thr, c_fn=C_FN, c_fp=C_FP):\n",
    "    y_hat = (proba >= thr).astype(int)\n",
    "    fn = int(((y_hat == 0) & (y_true == 1)).sum())\n",
    "    fp = int(((y_hat == 1) & (y_true == 0)).sum())\n",
    "    return (c_fn * fn + c_fp * fp) / max(len(y_true), 1)\n",
    "\n",
    "costs = np.array([expected_cost(y, p, t) for t in ths])\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "ax.plot(ths, costs)\n",
    "ax.set_title(f\"Expected cost vs threshold (C_FN={C_FN:g}, C_FP={C_FP:g})\")\n",
    "ax.set_xlabel(\"Threshold probability\")\n",
    "ax.set_ylabel(\"Expected cost per observation\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
