{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:57:49.769401Z",
     "start_time": "2026-01-05T13:57:48.521658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "############################################################\n",
    "# 0. Problem Definition & Setup\n",
    "# ----------------------------------------------------------\n",
    "# Task: Predict/understand firms' dividend behavior (dv, div_dummy)\n",
    "#       using Compustat balance sheet, income statement and\n",
    "#       cash flow variables for panel data (gvkey, fyear).\n",
    "#\n",
    "# This script implements:\n",
    "#   - Data cleaning (formats, duplicates, missing values, outliers)\n",
    "#   - Transformations (standardization, log)\n",
    "#   - Feature engineering (ratios, cash-flow features, lags)\n",
    "#   - Filter-based feature selection (correlation, VIF, chi-square)\n",
    "############################################################\n",
    "\n",
    "############################################################\n",
    "# 0. Setup and Initial Checks + dtype fixes\n",
    "############################################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "TRAIN_CUTOFF_LABEL_YEAR = 2022     # Train/Val labels up to and incl. 2022, Test labels after 2022\n",
    "VAL_YEARS = 1                      # Hold out the last 1 label-year from the training pool as validation\n",
    "\n",
    "# Rolling year-based CV folds (within train_pool)\n",
    "N_SPLITS_TIME_CV = 5\n",
    "\n",
    "WINSOR_LOWER_Q = 0.01\n",
    "WINSOR_UPPER_Q = 0.99\n",
    "# Read CSV, avoid chunked type guessing\n",
    "df = pd.read_csv(\"data.csv\", low_memory=False)\n",
    "\n",
    "# 0.1 Datetime conversion\n",
    "# -----------------------\n",
    "# datadate is currently 'object' → convert to datetime\n",
    "df['datadate'] = pd.to_datetime(df['datadate'], errors='coerce')\n",
    "\n",
    "# 0.2 Numeric conversion for price columns (prcc_c, prcc_f)\n",
    "# ---------------------------------------------------------\n",
    "# In your extract these had mixed types; we force them to float\n",
    "for col in ['prcc_c', 'prcc_f']:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# 0.3 Panel key types\n",
    "# -------------------\n",
    "# gvkey, fyear, ismod are true integers → make them explicit\n",
    "df['gvkey'] = df['gvkey'].astype('Int64')\n",
    "df['fyear'] = df['fyear'].astype('Int64')\n",
    "df['ismod'] = df['ismod'].astype('Int64')\n",
    "\n",
    "# 0.4 Categorical / string columns (optional but nice)\n",
    "# ----------------------------------------------------\n",
    "for col in ['indfmt', 'datafmt', 'consol']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "# Company name stays as plain string\n",
    "# df['conm'] is fine as object (string)\n",
    "\n",
    "print(df.dtypes)\n",
    "print(\"Numeric columns:\", len(df.select_dtypes(include=[np.number]).columns))\n",
    "print(\"Categorical columns:\", len(df.select_dtypes(include=['category']).columns))\n",
    "print(\"Datetime columns:\", len(df.select_dtypes(include=['datetime']).columns))"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 31\u001B[39m\n\u001B[32m     29\u001B[39m WINSOR_UPPER_Q = \u001B[32m0.99\u001B[39m\n\u001B[32m     30\u001B[39m \u001B[38;5;66;03m# Read CSV, avoid chunked type guessing\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m31\u001B[39m df = pd.read_csv(\u001B[33m\"\u001B[39m\u001B[33mdata.csv\u001B[39m\u001B[33m\"\u001B[39m, low_memory=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m     33\u001B[39m \u001B[38;5;66;03m# 0.1 Datetime conversion\u001B[39;00m\n\u001B[32m     34\u001B[39m \u001B[38;5;66;03m# -----------------------\u001B[39;00m\n\u001B[32m     35\u001B[39m \u001B[38;5;66;03m# datadate is currently 'object' → convert to datetime\u001B[39;00m\n\u001B[32m     36\u001B[39m df[\u001B[33m'\u001B[39m\u001B[33mdatadate\u001B[39m\u001B[33m'\u001B[39m] = pd.to_datetime(df[\u001B[33m'\u001B[39m\u001B[33mdatadate\u001B[39m\u001B[33m'\u001B[39m], errors=\u001B[33m'\u001B[39m\u001B[33mcoerce\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/py313/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001B[39m, in \u001B[36mread_csv\u001B[39m\u001B[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[39m\n\u001B[32m   1013\u001B[39m kwds_defaults = _refine_defaults_read(\n\u001B[32m   1014\u001B[39m     dialect,\n\u001B[32m   1015\u001B[39m     delimiter,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1022\u001B[39m     dtype_backend=dtype_backend,\n\u001B[32m   1023\u001B[39m )\n\u001B[32m   1024\u001B[39m kwds.update(kwds_defaults)\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m _read(filepath_or_buffer, kwds)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/py313/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001B[39m, in \u001B[36m_read\u001B[39m\u001B[34m(filepath_or_buffer, kwds)\u001B[39m\n\u001B[32m    617\u001B[39m _validate_names(kwds.get(\u001B[33m\"\u001B[39m\u001B[33mnames\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[32m    619\u001B[39m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m620\u001B[39m parser = TextFileReader(filepath_or_buffer, **kwds)\n\u001B[32m    622\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[32m    623\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/py313/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001B[39m, in \u001B[36mTextFileReader.__init__\u001B[39m\u001B[34m(self, f, engine, **kwds)\u001B[39m\n\u001B[32m   1617\u001B[39m     \u001B[38;5;28mself\u001B[39m.options[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m] = kwds[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m   1619\u001B[39m \u001B[38;5;28mself\u001B[39m.handles: IOHandles | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1620\u001B[39m \u001B[38;5;28mself\u001B[39m._engine = \u001B[38;5;28mself\u001B[39m._make_engine(f, \u001B[38;5;28mself\u001B[39m.engine)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/py313/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001B[39m, in \u001B[36mTextFileReader._make_engine\u001B[39m\u001B[34m(self, f, engine)\u001B[39m\n\u001B[32m   1878\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[32m   1879\u001B[39m         mode += \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1880\u001B[39m \u001B[38;5;28mself\u001B[39m.handles = get_handle(\n\u001B[32m   1881\u001B[39m     f,\n\u001B[32m   1882\u001B[39m     mode,\n\u001B[32m   1883\u001B[39m     encoding=\u001B[38;5;28mself\u001B[39m.options.get(\u001B[33m\"\u001B[39m\u001B[33mencoding\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[32m   1884\u001B[39m     compression=\u001B[38;5;28mself\u001B[39m.options.get(\u001B[33m\"\u001B[39m\u001B[33mcompression\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[32m   1885\u001B[39m     memory_map=\u001B[38;5;28mself\u001B[39m.options.get(\u001B[33m\"\u001B[39m\u001B[33mmemory_map\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[32m   1886\u001B[39m     is_text=is_text,\n\u001B[32m   1887\u001B[39m     errors=\u001B[38;5;28mself\u001B[39m.options.get(\u001B[33m\"\u001B[39m\u001B[33mencoding_errors\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mstrict\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m   1888\u001B[39m     storage_options=\u001B[38;5;28mself\u001B[39m.options.get(\u001B[33m\"\u001B[39m\u001B[33mstorage_options\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[32m   1889\u001B[39m )\n\u001B[32m   1890\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.handles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1891\u001B[39m f = \u001B[38;5;28mself\u001B[39m.handles.handle\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/py313/lib/python3.13/site-packages/pandas/io/common.py:873\u001B[39m, in \u001B[36mget_handle\u001B[39m\u001B[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[39m\n\u001B[32m    868\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    869\u001B[39m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[32m    870\u001B[39m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[32m    871\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m ioargs.encoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs.mode:\n\u001B[32m    872\u001B[39m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m873\u001B[39m         handle = \u001B[38;5;28mopen\u001B[39m(\n\u001B[32m    874\u001B[39m             handle,\n\u001B[32m    875\u001B[39m             ioargs.mode,\n\u001B[32m    876\u001B[39m             encoding=ioargs.encoding,\n\u001B[32m    877\u001B[39m             errors=errors,\n\u001B[32m    878\u001B[39m             newline=\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    879\u001B[39m         )\n\u001B[32m    880\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    881\u001B[39m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[32m    882\u001B[39m         handle = \u001B[38;5;28mopen\u001B[39m(handle, ioargs.mode)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'data.csv'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "df.select_dtypes(include=['datetime']).describe()"
   ],
   "id": "ec1f291aa829b07a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "############################################################\n",
    "# 1. Data Cleaning (Chapter 2: Data Cleaning)\n",
    "# ----------------------------------------------------------\n",
    "# - Standardize formats (numeric, categorical, date)\n",
    "# - Remove duplicates\n",
    "# - Diagnose missing values\n",
    "# - Basic missing-value imputation\n",
    "# - Outlier detection & handling (winsorization)\n",
    "# - Transformations (standardization, log)\n",
    "############################################################\n",
    "\n",
    "# 1.1 Standardize formats\n",
    "# -----------------------\n",
    "\n",
    "# Date variable -> datetime\n",
    "if 'datadate' in df.columns:\n",
    "    df['datadate'] = pd.to_datetime(df['datadate'], errors='coerce')\n",
    "\n",
    "\n",
    "# Create numeric firm_id from gvkey\n",
    "df['firm_id'], _ = pd.factorize(df['gvkey'])\n",
    "\n",
    "# Sort panel\n",
    "df = df.sort_values(['firm_id', 'fyear']).reset_index(drop=True)\n",
    "\n",
    "# 1.2 Remove duplicates\n",
    "# ---------------------\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# 1.3 Missing value diagnostics\n",
    "# -----------------------------\n",
    "total_rows = len(df)\n",
    "missing_report = []\n",
    "\n",
    "for col in df.columns:\n",
    "    n_miss = df[col].isna().sum()\n",
    "    if n_miss > 0:\n",
    "        missing_report.append({\n",
    "            \"variable\": col,\n",
    "            \"dtype\": str(df[col].dtype),\n",
    "            \"n_missing\": n_miss,\n",
    "            \"pct_missing\": n_miss / total_rows * 100\n",
    "        })\n",
    "\n",
    "missing_df = pd.DataFrame(missing_report).sort_values(\"pct_missing\", ascending=False)\n",
    "print(\"******** Missing Values Report (top 20) ********\")\n",
    "print(missing_df.head(20))\n"
   ],
   "id": "2d5487bf6bce4667",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T19:09:08.645230Z",
     "start_time": "2025-12-19T19:09:08.487163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# 2. TRAIN / VALIDATION / TEST SPLIT (Temporal, based on label_year)\n",
    "# =============================================================================\n",
    "# 2.4 label_year and distress definition at time t\n",
    "df['label_year'] = df['fyear'] + 1\n",
    "\n",
    "train_pool = df[df['label_year'] <= TRAIN_CUTOFF_LABEL_YEAR].copy()\n",
    "test = df[df['label_year'] > TRAIN_CUTOFF_LABEL_YEAR].copy()\n",
    "\n",
    "if train_pool.empty:\n",
    "    raise ValueError(\"Training pool is empty after applying label_year cutoff. Check TRAIN_CUTOFF_LABEL_YEAR.\")\n",
    "\n",
    "unique_label_years = np.sort(train_pool['label_year'].dropna().unique())\n",
    "val_years = unique_label_years[-VAL_YEARS:] if len(unique_label_years) >= VAL_YEARS else unique_label_years\n",
    "\n",
    "val = train_pool[train_pool['label_year'].isin(val_years)].copy()\n",
    "train = train_pool[~train_pool['label_year'].isin(val_years)].copy()\n",
    "\n",
    "print(\"----- Split Summary (based on label_year = fyear+1) -----\")\n",
    "print(f\"Train label_year max: {train['label_year'].max()} | n={len(train)}\")\n",
    "print(f\"Val   label_years: {list(val_years)} | n={len(val)}\")\n",
    "print(f\"Test  label_year min: {test['label_year'].min()} | n={len(test)}\")"
   ],
   "id": "a848957fd3ae1187",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Split Summary (based on label_year = fyear+1) -----\n",
      "Train label_year max: 2021 | n=48458\n",
      "Val   label_years: [np.int64(2022)] | n=6851\n",
      "Test  label_year min: 2023 | n=19696\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "############################################################\n",
    "# 1.6 Log Transformations\n",
    "###########################################################\n",
    "# Log-transform size-related, strictly positive variables to reduce skewness\n",
    "for col in ['at', 'mkvalt']:\n",
    "    if col in df.columns:\n",
    "        s = pd.to_numeric(df[col], errors='coerce')\n",
    "        m = s > 0\n",
    "        log_s = pd.Series(np.nan, index=s.index, dtype='float64')\n",
    "        log_s.loc[m] = np.log(s.loc[m])\n",
    "        df[f\"log_{col}\"] = log_s"
   ],
   "id": "ce74115bc2dad29a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# 1.6 Transformations (standardization & log)\n",
    "# -------------------------------------------\n",
    "z_vars = ['ib', 'at', 'dltt', 'che', 're', 'seq', 'xrd', 'dv', 'sale', 'ni',\n",
    "          'oancf', 'ivncf', 'fincf']\n",
    "\n",
    "for col in z_vars:\n",
    "    if col in df.columns:\n",
    "        mean_val = df[col].mean()\n",
    "        std_val = df[col].std(ddof=0)\n",
    "        if std_val != 0:\n",
    "            df[f\"z_{col}\"] = (df[col] - mean_val) / std_val\n",
    "        else:\n",
    "            df[f\"z_{col}\"] = np.nan\n"
   ],
   "id": "ddbabf5fc812df7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "############################################################\n",
    "# 2. Feature Engineering (Chapter 2: Feature Engineering)\n",
    "# ----------------------------------------------------------\n",
    "# - Financial ratios (profitability, leverage, liquidity, payout, innovation)\n",
    "# - Cash-flow based features\n",
    "# - Lagged features for time-series structure\n",
    "############################################################\n",
    "\n",
    "# Profitability\n",
    "df['roa'] = np.where(df['at'] > 0, df['ib'] / df['at'], np.nan)          # Income before extraord. / assets\n",
    "df['profit_margin'] = np.where(df['sale'] > 0, df['ni'] / df['sale'], np.nan)\n",
    "\n",
    "# Leverage\n",
    "df['leverage'] = np.where(df['at'] > 0, df['dltt'] / df['at'], np.nan)\n",
    "\n",
    "# Liquidity\n",
    "df['cash_ratio'] = np.where(df['at'] > 0, df['che'] / df['at'], np.nan)\n",
    "df['current_ratio'] = np.where(df['lct'] > 0, df['act'] / df['lct'], np.nan)\n",
    "\n",
    "# Retained earnings capacity\n",
    "df['re_ratio'] = np.where(df['seq'] > 0, df['re'] / df['seq'], np.nan)\n",
    "\n",
    "# Innovation / investment\n",
    "df['rd_ratio'] = np.where(df['at'] > 0, df['xrd'] / df['at'], np.nan)\n",
    "df['capx_ratio'] = np.where(df['at'] > 0, df['capx'] / df['at'], np.nan)\n",
    "\n",
    "# Cash-flow structure (operating / investing / financing CF scaled by assets)\n",
    "df['cf_oancf_at'] = np.where(df['at'] > 0, df['oancf'] / df['at'], np.nan)\n",
    "df['cf_ivncf_at'] = np.where(df['at'] > 0, df['ivncf'] / df['at'], np.nan)\n",
    "df['cf_fincf_at'] = np.where(df['at'] > 0, df['fincf'] / df['at'], np.nan)\n",
    "\n",
    "# Dividend behavior\n",
    "df['div_dummy'] = np.where(df['dv'].notna(), (df['dv'] > 0).astype(int), np.nan)\n",
    "df['payout_ratio_ni'] = np.where(df['ni'] != 0, df['dv'] / df['ni'], np.nan)\n",
    "df['payout_ratio_at'] = np.where(df['at'] > 0, df['dv'] / df['at'], np.nan)\n",
    "\n",
    "# 2.1 Lagged Features (time-series / panel aspect)\n",
    "# ------------------------------------------------\n",
    "lag_base_vars = ['roa', 'leverage', 'cash_ratio',\n",
    "                 'cf_oancf_at', 'cf_ivncf_at', 'cf_fincf_at', 'payout_ratio_ni']\n",
    "\n",
    "for var in lag_base_vars:\n",
    "    if var in df.columns:\n",
    "        df[f\"lag_{var}\"] = df.groupby('firm_id')[var].shift(1)\n",
    "\n",
    "# Keep lags only for fyear >= 2023, if desired (align with Stata logic)\n",
    "df.loc[df['fyear'] < 2023,\n",
    "       [c for c in df.columns if c.startswith('lag_')]] = np.nan"
   ],
   "id": "923e4b641cf0939f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "############################################################\n",
    "# 3. Feature Selection Prep (Chapter 2: Feature Selection)\n",
    "# ----------------------------------------------------------\n",
    "# Filter methods:\n",
    "#   - Correlation analysis for numeric features vs target dv\n",
    "#   - VIF for multicollinearity diagnostics\n",
    "#   - Chi-square for categorical vs target (sic_clean vs div_dummy)\n",
    "############################################################\n",
    "\n",
    "feature_vars = [\n",
    "    'roa', 'leverage', 'cash_ratio', 'current_ratio', 're_ratio',\n",
    "    'rd_ratio', 'capx_ratio', 'profit_margin', 'log_at', 'log_sale',\n",
    "    'cf_oancf_at', 'cf_ivncf_at', 'cf_fincf_at', 'payout_ratio_ni',\n",
    "    'payout_ratio_at'\n",
    "]\n",
    "# Retain only existing ones\n",
    "feature_vars = [v for v in feature_vars if v in df.columns]\n",
    "\n",
    "# 3.1 Correlation with target dv (filter method)\n",
    "# ---------------------------------------------\n",
    "print(\"----- Correlation of Features with Target Variable (dv) -----\")\n",
    "corr_matrix = df[['dv'] + feature_vars].corr()\n",
    "\n",
    "for var in feature_vars:\n",
    "    r = corr_matrix.loc['dv', var]\n",
    "    print(f\"{var:<20} r = {r: .4f}\")\n",
    "\n",
    "# 3.2 VIF (Variance Inflation Factor) – pure NumPy\n",
    "# -----------------------------------------------\n",
    "vif_vars = feature_vars\n",
    "X_df = df[vif_vars].dropna()\n",
    "X = X_df.values\n",
    "var_names = list(vif_vars)\n",
    "\n",
    "print(\"\\n----- VIF for selected features -----\")\n",
    "\n",
    "def compute_vif(X, j):\n",
    "    y = X[:, j]\n",
    "    X_other = np.delete(X, j, axis=1)\n",
    "    X_other_const = np.column_stack([np.ones(X_other.shape[0]), X_other])\n",
    "    beta, _, _, _ = np.linalg.lstsq(X_other_const, y, rcond=None)\n",
    "    y_pred = X_other_const @ beta\n",
    "    ss_res = np.sum((y - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y - y.mean()) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot if ss_tot > 0 else 0.0\n",
    "    if r2 >= 1:\n",
    "        return np.inf\n",
    "    return 1.0 / (1.0 - r2)\n",
    "\n",
    "for j, name in enumerate(var_names):\n",
    "    vif_val = compute_vif(X, j)\n",
    "    print(f\"{name:<20} VIF = {vif_val: .4f}\")\n",
    "\n",
    "# 3.3 Variability checks (IQR & Std. Dev.)\n",
    "# ----------------------------------------\n",
    "print(\"\\n----- Variability Checks (IQR & Std. Dev.) -----\")\n",
    "for var in feature_vars:\n",
    "    series = df[var].dropna()\n",
    "    if series.empty:\n",
    "        print(f\"{var:<20} IQR: NA, Std. Dev.: NA\")\n",
    "        continue\n",
    "    q25 = series.quantile(0.25)\n",
    "    q75 = series.quantile(0.75)\n",
    "    iqr = q75 - q25\n",
    "    std = series.std(ddof=1)\n",
    "    print(\"----------------------------------------\")\n",
    "    print(f\"{var} IQR: {iqr:.6f}\")\n",
    "    print(f\"{var} Std. Dev.: {std:.6f}\")\n",
    "\n",
    "# 3.4 Chi-Square: sic_clean vs div_dummy (categorical vs target)\n",
    "# --------------------------------------------------------------\n",
    "chi_df = df[['sic_clean', 'div_dummy']].dropna()\n",
    "contingency_table = pd.crosstab(chi_df['sic_clean'], chi_df['div_dummy'])\n",
    "\n",
    "observed = contingency_table.values\n",
    "row_sums = observed.sum(axis=1, keepdims=True)\n",
    "col_sums = observed.sum(axis=0, keepdims=True)\n",
    "total = observed.sum()\n",
    "\n",
    "expected = row_sums @ col_sums / total\n",
    "\n",
    "chi2_stat = ((observed - expected) ** 2 / expected).sum()\n",
    "dof = (observed.shape[0] - 1) * (observed.shape[1] - 1)\n",
    "\n",
    "print(\"\\n----- Chi-Square Test: sic_clean vs div_dummy -----\")\n",
    "print(\"Chi2 statistic:\", chi2_stat)\n",
    "print(\"Degrees of freedom:\", dof)\n",
    "print(\"\\nContingency table:\")\n",
    "print(contingency_table)\n",
    "\n"
   ],
   "id": "9e9d7be4145e50",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
