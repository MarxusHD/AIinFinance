```python
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, average_precision_score, confusion_matrix,
    classification_report
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.impute import KNNImputer
from IPython.display import display

# =============================================================================
# Project: Next-year financial distress prediction (firm-year panel)
# Data Science Lifecycle focus in this cell:
#   - Setup & configuration (temporal split conventions, leakage control parameters)
#   - Panel integrity (types, deduplication, stable firm identifier)
#   - Define label_year used consistently for splitting and target alignment
# =============================================================================

# ----------------------------
# Configuration (split & preprocessing parameters)
# ----------------------------
FILE_NAME = "../data.csv"

TRAIN_CUTOFF_LABEL_YEAR = 2022  # label_year <= cutoff → train/val pool; after cutoff → test
VAL_YEARS = 1  # last N years within the pool are validation
N_SPLITS_TIME_CV = 4  # rolling time-based folds for sanity checks

WINSOR_LOWER_Q = 0.01  # winsorization lower quantile (train-only)
WINSOR_UPPER_Q = 0.99  # winsorization upper quantile (train-only)

REQUIRED_KEYS = ["gvkey", "fyear"]
TARGET_COL = "target_next_year_distress"


# ----------------------------
# Utilities (robust numeric ops for ratios)
# ----------------------------

def to_float_numpy(x) -> np.ndarray:
    """Convert series/array-like to float numpy array, coercing non-numeric to NaN."""
    s = pd.to_numeric(x, errors="coerce")
    return s.to_numpy(dtype=float) if hasattr(s, "to_numpy") else np.asarray(s, dtype=float)


def safe_divide(a, b) -> np.ndarray:
    """Elementwise divide a/b with NaN when division is invalid (0 or non-finite)."""
    a = to_float_numpy(a)
    b = to_float_numpy(b)
    out = np.full_like(a, np.nan, dtype=float)
    np.divide(a, b, out=out, where=(b != 0) & np.isfinite(a) & np.isfinite(b))
    return out


def rolling_year_folds(
        df_in: pd.DataFrame, year_col: str = "label_year", n_splits: int = 4, min_train_years: int = 3
) -> list[tuple[np.ndarray, np.ndarray, np.ndarray, int]]:
    """
    Create expanding-window time folds:
      train years: first (min_train_years + k) years
      val year:    next year
    Returns: list of (train_idx, val_idx, train_years, val_year)
    """
    years_sorted = np.sort(df_in[year_col].dropna().unique())
    if len(years_sorted) <= min_train_years:
        return []
    n_splits = min(n_splits, len(years_sorted) - min_train_years)

    folds_out = []
    for k in range(n_splits):
        train_years = years_sorted[: min_train_years + k]
        val_year = int(years_sorted[min_train_years + k])

        train_idx = df_in.index[df_in[year_col].isin(train_years)].to_numpy()
        val_idx = df_in.index[df_in[year_col] == val_year].to_numpy()
        folds_out.append((train_idx, val_idx, train_years, val_year))

    return folds_out


# =============================================================================
# 1) Data acquisition & panel hygiene
# =============================================================================
df = pd.read_csv(FILE_NAME, low_memory=False)

# Convert datadate if present
if "datadate" in df.columns:
    df["datadate"] = pd.to_datetime(df["datadate"], errors="coerce")

# Create stable firm id + de-duplicate firm-year (keep last record)
df["firm_id"] = df["gvkey"]
df = (
    df.sort_values(["firm_id", "fyear"])
    .drop_duplicates(subset=["firm_id", "fyear"], keep="last")
    .reset_index(drop=True)
)

# Label year: predict distress in the next fiscal year
df["label_year"] = df["fyear"] + 1

# =============================================================================
# 2) Split scaffolding (define train/val pool years via label_year)
# =============================================================================
pool_mask = df["label_year"] <= TRAIN_CUTOFF_LABEL_YEAR
pool_years = np.sort(df.loc[pool_mask, "label_year"].dropna().unique())
val_years = pool_years[-VAL_YEARS:] if len(pool_years) else np.array([], dtype=int)

# This mask is ONLY used for imputations (train-only information)
train_mask_for_imputation = pool_mask & (~df["label_year"].isin(val_years))

```


```python
# =============================================================================
# Data Cleaning & Missing-Data Handling (leakage-aware)
# Purpose:
#   - Quantify missingness and distribution properties before intervention
#   - Preserve informative missingness via miss_* indicators
#   - Impute financial statement inputs using TRAIN-only information:
#       (1) within-firm past values (lag-1) where economically sensible
#       (2) peer medians by year×size_decile on size-scaled ratios (ratio space)
#       (3) KNN imputation (TRAIN-fit) for selected core balance-sheet items
#   - Re-run EDA after imputation to audit how strongly imputations alter the data
# =============================================================================

RAW_INPUTS_FOR_FE = [
    "aco","act","ao","aoloch","ap","apalch","aqc","at","caps","capx","ceq","che","chech","csho","cstk","cstke",
    "datadate","dlc","dlcch","dltis","dltr","dltt","do","dp","dpc","dv","dvc","dvp","dvt","esubc","exre",
    "fiao","fincf","fopo","fyear","gvkey","ib","ibadj","ibc","intan","invch","invt","ismod","ivaco","ivaeq",
    "ivao","ivch","ivncf","ivstch","lco","lct","lt","mibt","mkvalt","niadj","nopi","oancf","oibdp","ppent",
    "prcc_c","prcc_f","prstkc","pstk","pstkn","pstkr","re","recch","rect","seq","siv","spi","sppe","sppiv",
    "sstk","tstk","txach","txbcof","txdc","txditc","txp","txt","xi","xido","xidoc","xint",
    # optional identifiers present in many extracts:
    "conm","consol","datafmt","indfmt",
]
raw = [c for c in RAW_INPUTS_FOR_FE if c in df.columns]

# ---------------------------------------------------------------------------
# 3.0 Ensure keys exist + types
# ---------------------------------------------------------------------------
if "firm_id" not in df.columns:
    if "gvkey" in df.columns:
        df["firm_id"] = df["gvkey"]
    else:
        raise ValueError("Need either firm_id or gvkey in df to run panel imputations.")

if "fyear" in df.columns:
    df["fyear"] = pd.to_numeric(df["fyear"], errors="coerce")

# ---------------------------------------------------------------------------
# 3.1 Drop rows with missing critical identifiers (do not impute these)
# ---------------------------------------------------------------------------
NON_IMPUTE_DROP = [c for c in ["gvkey", "datadate", "fyear", "conm", "datafmt", "indfmt", "consol"] if c in df.columns]
if NON_IMPUTE_DROP:
    before_n = df.shape[0]
    df = df.dropna(subset=NON_IMPUTE_DROP).copy()
    after_n = df.shape[0]
    if after_n < before_n:
        print(f"[INFO] Dropped {before_n - after_n:,} rows due to missing non-imputable ID/meta fields: {NON_IMPUTE_DROP}")

# Ensure train mask aligns after drops
if isinstance(train_mask_for_imputation, pd.Series):
    train_mask_for_imputation = train_mask_for_imputation.reindex(df.index).fillna(False).astype(bool)

# Rebuild raw after potential drop
raw = [c for c in RAW_INPUTS_FOR_FE if c in df.columns]

# ---------------------------------------------------------------------------
# 3.2 EDA BEFORE imputation (missingness + distribution snapshot)
# ---------------------------------------------------------------------------
df_raw_pre = df[raw].copy(deep=True)

pre_miss = pd.DataFrame(
    {
        "col": raw,
        "n": [int(df_raw_pre[c].shape[0]) for c in raw],
        "n_na_pre": [int(df_raw_pre[c].isna().sum()) for c in raw],
        "pct_na_pre": [float(df_raw_pre[c].isna().mean() * 100.0) for c in raw],
        "train_n": [int(train_mask_for_imputation.sum()) for _ in raw],
        "train_pct_na_pre": [
            float(df_raw_pre.loc[train_mask_for_imputation, c].isna().mean() * 100.0) for c in raw
        ],
    }
).sort_values("pct_na_pre", ascending=False)

print("\n=== EDA (BEFORE imputation): Missingness on raw inputs ===")
print(pre_miss.round(4).head(50))

# Numeric distribution summary (coerce non-numeric to NaN)
if raw:
    x_pre = df_raw_pre[raw].apply(pd.to_numeric, errors="coerce").replace([np.inf, -np.inf], np.nan)
    q_pre = x_pre.quantile([0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99]).T
    pre_dist = pd.DataFrame(
        {
            "n_nonmiss_pre": x_pre.notna().sum(),
            "mean_pre": x_pre.mean(),
            "std_pre": x_pre.std(ddof=0),
            "min_pre": x_pre.min(),
            "p01_pre": q_pre[0.01],
            "p05_pre": q_pre[0.05],
            "p25_pre": q_pre[0.25],
            "p50_pre": q_pre[0.50],
            "p75_pre": q_pre[0.75],
            "p95_pre": q_pre[0.95],
            "p99_pre": q_pre[0.99],
            "max_pre": x_pre.max(),
        }
    )
    print("\n=== EDA (BEFORE imputation): Distribution summary (raw inputs) ===")
    print(pre_dist.round(4).sort_values("n_nonmiss_pre", ascending=True).head(50))

# ---------------------------------------------------------------------------
# 3.3 Missingness flags (ALWAYS create before imputations)
# ---------------------------------------------------------------------------
for c in raw:
    df[f"miss_{c}"] = df[c].isna().astype("int8")

# Helper: originally-observed indicator
def _obs(col: str) -> pd.Series:
    m = f"miss_{col}"
    if m in df.columns:
        return (df[m] == 0)
    return df[col].notna()

# ---------------------------------------------------------------------------
# 3.4 Grouping key for peer-based imputations (no industry used)
#   - Use TRAIN-derived size_decile to avoid mixing microcaps with mega-caps
# ---------------------------------------------------------------------------
df = df.sort_values(["firm_id", "fyear"]).copy()

# TRAIN-derived size deciles on log(at) using observed TRAIN values only
if "at" in df.columns:
    at_train = pd.to_numeric(df.loc[train_mask_for_imputation, "at"], errors="coerce")
    log_at_train = np.log(at_train.where(at_train > 0))
    qs = np.linspace(0, 1, 11)
    edges = np.unique(np.nanquantile(log_at_train.dropna(), qs))
    if edges.size >= 3:
        log_at_all = np.log(pd.to_numeric(df["at"], errors="coerce").where(pd.to_numeric(df["at"], errors="coerce") > 0))
        df["size_decile"] = pd.cut(log_at_all, bins=edges, include_lowest=True, labels=False)
    else:
        df["size_decile"] = np.nan
else:
    df["size_decile"] = np.nan

group_cols = ["fyear", "size_decile"]

# ---------------------------------------------------------------------------
# 3.5 Step 1: Construct / reconcile FIRST (no leakage: contemporaneous or lag only)
# ---------------------------------------------------------------------------
# 3.5.1 Mild lag-1 fill for prices, then mkvalt = prcc_f * csho if missing
for px in ["prcc_f", "prcc_c"]:
    if px in df.columns:
        df[px] = df[px].where(df[px].notna(), df.groupby("firm_id")[px].shift(1))

if all(c in df.columns for c in ["mkvalt", "prcc_f", "csho"]):
    mkvalt_miss = df["mkvalt"].isna()
    mkvalt_calc = pd.to_numeric(df["prcc_f"], errors="coerce") * pd.to_numeric(df["csho"], errors="coerce")
    df.loc[mkvalt_miss & mkvalt_calc.notna(), "mkvalt"] = mkvalt_calc.loc[mkvalt_miss & mkvalt_calc.notna()]

# 3.5.2 Reconstruct change variables from level differences (fill only if change var is missing)
def _fill_change_from_levels(change_col, level_col):
    if change_col in df.columns and level_col in df.columns:
        miss = df[change_col].isna()
        lvl = pd.to_numeric(df[level_col], errors="coerce")
        lag_lvl = pd.to_numeric(df.groupby("firm_id")[level_col].shift(1), errors="coerce")
        recon = lvl - lag_lvl
        df.loc[miss & recon.notna(), change_col] = recon.loc[miss & recon.notna()]

_fill_change_from_levels("dlcch", "dlc")
_fill_change_from_levels("recch", "rect")
_fill_change_from_levels("invch", "invt")
_fill_change_from_levels("chech", "che")

# ---------------------------------------------------------------------------
# 3.6 Sparse zero-fill for structurally zero-inflated items (TRAIN-validated)
#   - Keeps miss_* as informative signal
# ---------------------------------------------------------------------------
SPARSE_CANDIDATES = [c for c in [
    "txach","txdc","txbcof","spi","siv","sppiv","ivstch","sppe","esubc","dvp","dv","dvc","dvt"
] if c in df.columns]

def _zero_fill_if_sparse(col: str, zero_share_thresh: float = 0.70, min_obs: int = 1000) -> bool:
    mflag = f"miss_{col}"
    if mflag not in df.columns:
        return False
    obs_mask = train_mask_for_imputation & (df[mflag] == 0)
    s = pd.to_numeric(df.loc[obs_mask, col], errors="coerce").replace([np.inf, -np.inf], np.nan)
    if s.notna().sum() < min_obs:
        return False
    zero_share = float((s == 0).mean())
    med = float(s.median()) if s.notna().any() else np.nan
    if np.isfinite(med) and (med == 0.0) and (zero_share >= zero_share_thresh):
        df.loc[df[col].isna(), col] = 0.0
        return True
    return False

for c in SPARSE_CANDIDATES:
    _zero_fill_if_sparse(c)

# Explicit “tax components -> 0” rule (FFO proxy safety)
for c in ["txt", "txdc", "txach"]:
    if c in df.columns:
        df.loc[df[c].isna(), c] = 0.0

# ---------------------------------------------------------------------------
# 3.7 Step 3: Stocks — lag-1 fill -> peer median of ratio (x/at) with ratio-space clipping
# ---------------------------------------------------------------------------
STOCKS = [c for c in [
    "aco","act","ao","ap","at","caps","ceq","che","csho","cstk","dlc","dltt","intan","invt","lco","lct","lt",
    "mibt","ppent","pstk","pstkn","pstkr","re","rect","seq","tstk","ivaeq","mkvalt"
] if c in df.columns]

# lag-1 fill (past-only)
if STOCKS:
    lag1 = df.groupby("firm_id")[STOCKS].shift(1)
    df[STOCKS] = df[STOCKS].where(df[STOCKS].notna(), lag1)

# non-negativity only where economically necessary
NONNEG_STOCKS = set([c for c in STOCKS if c in {
    "aco","act","ao","ap","at","caps","che","csho","cstk","dlc","dltt","intan","invt","lco","lct","lt",
    "mibt","mkvalt","ppent","pstk","pstkn","pstkr","rect","tstk","ivaeq"
}])

def _fit_ratio_stats(train_df: pd.DataFrame, col: str, base: str = "at", qlo=0.01, qhi=0.99):
    mcol = f"miss_{col}"
    mbase = f"miss_{base}"
    s = pd.to_numeric(train_df[col], errors="coerce")
    b = pd.to_numeric(train_df[base], errors="coerce") if base in train_df.columns else None

    obs_s = (train_df[mcol] == 0) if mcol in train_df.columns else s.notna()

    if b is None:
        tr = train_df.loc[obs_s, group_cols + [col]].copy()
        tr[col] = pd.to_numeric(tr[col], errors="coerce").replace([np.inf, -np.inf], np.nan)
        grp = tr.groupby(group_cols)[col].median()
        overall = float(tr[col].median()) if tr[col].notna().any() else 0.0
        return ("level", overall, grp, np.nan, np.nan)

    obs_b = (train_df[mbase] == 0) if mbase in train_df.columns else b.notna()
    valid = obs_s & obs_b & s.notna() & b.notna() & (b > 0)

    if valid.sum() < 200:
        tr = train_df.loc[obs_s, group_cols + [col]].copy()
        tr[col] = pd.to_numeric(tr[col], errors="coerce").replace([np.inf, -np.inf], np.nan)
        grp = tr.groupby(group_cols)[col].median()
        overall = float(tr[col].median()) if tr[col].notna().any() else 0.0
        return ("level", overall, grp, np.nan, np.nan)

    ratio = (s[valid] / b[valid]).replace([np.inf, -np.inf], np.nan).dropna()
    overall = float(ratio.median()) if ratio.notna().any() else 0.0
    r_lo = float(ratio.quantile(qlo))
    r_hi = float(ratio.quantile(qhi))

    tmp = train_df.loc[valid, group_cols].copy()
    tmp["_ratio_"] = ratio.values
    grp = tmp.groupby(group_cols)["_ratio_"].median()
    return ("ratio", overall, grp, r_lo, r_hi)

def _apply_ratio_stats(df_all: pd.DataFrame, col: str, fit_obj, base: str = "at", nonneg: bool = False):
    kind, overall, grp, r_lo, r_hi = fit_obj
    miss = df_all[col].isna()
    if not miss.any():
        return

    if kind == "ratio" and base in df_all.columns:
        b = pd.to_numeric(df_all.loc[miss, base], errors="coerce")
        g = df_all.loc[miss, group_cols]

        if len(group_cols) == 1:
            mapped = g[group_cols[0]].map(grp)
        else:
            keys = list(map(tuple, g[group_cols].to_numpy()))
            mapped = pd.Series(keys, index=g.index).map(grp)

        r = pd.to_numeric(mapped, errors="coerce").replace([np.inf, -np.inf], np.nan).fillna(overall)

        # clip in ratio space (TRAIN-observed band)
        if np.isfinite(r_lo) and np.isfinite(r_hi) and (r_lo < r_hi):
            r = r.clip(r_lo, r_hi)

        fill = r * b
        fill = fill.where(b.notna() & (b > 0), np.nan)
        df_all.loc[miss & fill.notna(), col] = fill.loc[miss & fill.notna()].to_numpy()

    # fallback: group level medians (TRAIN-fit, observed-only)
    miss2 = df_all[col].isna()
    if miss2.any():
        mflag = f"miss_{col}"
        obs_train_mask = train_mask_for_imputation
        if mflag in df_all.columns:
            obs_train_mask = obs_train_mask & (df_all[mflag] == 0)

        tr = df_all.loc[obs_train_mask, group_cols + [col]].copy()
        tr[col] = pd.to_numeric(tr[col], errors="coerce").replace([np.inf, -np.inf], np.nan)

        lvl_overall = float(tr[col].median()) if tr[col].notna().any() else 0.0
        lvl_grp = tr.groupby(group_cols)[col].median()

        g2 = df_all.loc[miss2, group_cols]
        if len(group_cols) == 1:
            mapped2 = g2[group_cols[0]].map(lvl_grp)
        else:
            keys2 = list(map(tuple, g2[group_cols].to_numpy()))
            mapped2 = pd.Series(keys2, index=g2.index).map(lvl_grp)

        fill2 = pd.to_numeric(mapped2, errors="coerce").replace([np.inf, -np.inf], np.nan).fillna(lvl_overall)
        if nonneg:
            fill2 = fill2.clip(lower=0.0)
        df_all.loc[miss2, col] = fill2.to_numpy()

# Fit on training
tr_all = df.loc[train_mask_for_imputation].copy()
stock_fits = {c: _fit_ratio_stats(tr_all, c, base="at") for c in STOCKS}

# Apply to full df
for c in STOCKS:
    _apply_ratio_stats(df, c, stock_fits[c], base="at", nonneg=(c in NONNEG_STOCKS))

# ---------------------------------------------------------------------------
# 3.8 Step 4: Flows / income variables — ratio-median imputation (leakage-aware)
# ---------------------------------------------------------------------------
FLOWS = [c for c in ["ib","ibadj","ibc","niadj","nopi","oibdp","dp","oancf","fincf","ivncf","xint"] if c in df.columns]

# Debt-aware base for xint
if "xint" in df.columns and all(c in df.columns for c in ["dlc", "dltt"]):
    df["_td_for_xint"] = (
        pd.to_numeric(df["dlc"], errors="coerce").fillna(0.0)
        + pd.to_numeric(df["dltt"], errors="coerce").fillna(0.0)
    )
else:
    df["_td_for_xint"] = np.nan

# If total debt <= 0 and xint missing -> 0
if "xint" in df.columns and "_td_for_xint" in df.columns:
    td = pd.to_numeric(df["_td_for_xint"], errors="coerce").fillna(0.0)
    df.loc[df["xint"].isna() & (td <= 0), "xint"] = 0.0

flow_fits = {}
for c in FLOWS:
    base = "_td_for_xint" if (c == "xint" and "_td_for_xint" in df.columns) else "at"
    flow_fits[c] = _fit_ratio_stats(tr_all, c, base=base)

for c in FLOWS:
    base = "_td_for_xint" if (c == "xint" and "_td_for_xint" in df.columns) else "at"
    _apply_ratio_stats(df, c, flow_fits[c], base=base, nonneg=False)

# ---------------------------------------------------------------------------
# 3.9 KNN imputation (TRAIN-fit) for selected core balance-sheet items
#   - Applies ONLY to originally-missing rows (miss_* == 1)
#   - Uses a robust signed-log transform for magnitudes
# ---------------------------------------------------------------------------
KNN_TARGETS = [c for c in [
    "at","act","lct","lt","seq","dlc","dltt","che","ppent","rect","invt","re","ceq","caps"
] if c in df.columns]

# Non-negativity for clearly non-negative magnitudes (do NOT force seq/ceq/re to be non-negative)
KNN_NONNEG = set([c for c in KNN_TARGETS if c in {"at","act","lct","lt","dlc","dltt","che","ppent","rect","invt","caps"}])

if KNN_TARGETS:
    # KNN feature set (kept compact and available in your raw inputs)
    knn_feats = []
    for c in ["fyear", "size_decile", "mkvalt", "csho", "prcc_f", "prcc_c"]:
        if c in df.columns:
            knn_feats.append(c)
    knn_feats = list(dict.fromkeys(knn_feats))  # preserve order, unique

    knn_cols = list(dict.fromkeys(knn_feats + KNN_TARGETS))
    X = df[knn_cols].apply(pd.to_numeric, errors="coerce").replace([np.inf, -np.inf], np.nan)

    # For KNN, re-instate NaN only for the targets that were originally missing
    for t in KNN_TARGETS:
        mflag = f"miss_{t}"
        if mflag in df.columns:
            X.loc[df[mflag] == 1, t] = np.nan

    # Fill size_decile NaN with TRAIN median decile for distance stability
    if "size_decile" in X.columns:
        sd_train = X.loc[train_mask_for_imputation, "size_decile"]
        sd_fill = float(sd_train.median()) if sd_train.notna().any() else 5.0
        X["size_decile"] = X["size_decile"].fillna(sd_fill)

    # Robust transform: signed log1p for magnitudes; keep fyear/size_decile in levels
    def _signed_log1p(a: pd.Series) -> pd.Series:
        v = pd.to_numeric(a, errors="coerce")
        return np.sign(v) * np.log1p(np.abs(v))

    Z = X.copy()
    for c in Z.columns:
        if c not in {"fyear", "size_decile"}:
            Z[c] = _signed_log1p(Z[c])

    # Standardize using TRAIN-only stats (nan-safe)
    mu = Z.loc[train_mask_for_imputation].mean(skipna=True)
    sd = Z.loc[train_mask_for_imputation].std(skipna=True, ddof=0).replace(0.0, 1.0)
    sd = sd.fillna(1.0)
    mu = mu.fillna(0.0)

    Zs = (Z - mu) / sd

    # Fit KNN on TRAIN only; transform all rows
    imputer = KNNImputer(n_neighbors=25, weights="distance", metric="nan_euclidean")
    imputer.fit(Zs.loc[train_mask_for_imputation].to_numpy(dtype=float))
    Zs_imp = imputer.transform(Zs.to_numpy(dtype=float))
    Zs_imp = pd.DataFrame(Zs_imp, index=Zs.index, columns=Zs.columns)

    # Unscale
    Z_imp = Zs_imp * sd + mu

    # Invert signed log for magnitudes
    X_imp = X.copy()
    for c in Z_imp.columns:
        if c in {"fyear", "size_decile"}:
            X_imp[c] = Z_imp[c]
        else:
            v = pd.to_numeric(Z_imp[c], errors="coerce")
            X_imp[c] = np.sign(v) * (np.expm1(np.abs(v)))

    # Write back ONLY for originally-missing targets
    for t in KNN_TARGETS:
        mflag = f"miss_{t}"
        if mflag in df.columns:
            miss_mask = (df[mflag] == 1)
            df.loc[miss_mask, t] = pd.to_numeric(X_imp.loc[miss_mask, t], errors="coerce").to_numpy()

            if t in KNN_NONNEG:
                df.loc[miss_mask, t] = pd.to_numeric(df.loc[miss_mask, t], errors="coerce").clip(lower=0.0)

# ---------------------------------------------------------------------------
# 3.10 Change variables AFTER level imputations:
#   - Fill only if change is missing
#   - Reconstruct only when both levels were originally observed; otherwise impute change directly
# ---------------------------------------------------------------------------
def _fill_change_after_levels(change_col, level_col):
    if change_col in df.columns and level_col in df.columns:
        lvl = pd.to_numeric(df[level_col], errors="coerce")
        lag_lvl = pd.to_numeric(df.groupby("firm_id")[level_col].shift(1), errors="coerce")
        recon = lvl - lag_lvl

        miss_now = df[change_col].isna()

        # only reconstruct when both levels were originally observed
        obs_now = _obs(level_col)
        obs_lag = obs_now.groupby(df["firm_id"]).shift(1).fillna(False)
        can_recon = miss_now & obs_now & obs_lag & recon.notna()
        df.loc[can_recon, change_col] = recon.loc[can_recon]

        # remaining missing: direct group-median impute of change (TRAIN-fit, observed-only if possible)
        miss2 = df[change_col].isna()
        if miss2.any():
            mflag = f"miss_{change_col}"
            obs_train = train_mask_for_imputation
            if mflag in df.columns:
                obs_train = obs_train & (df[mflag] == 0)

            tr = df.loc[obs_train, group_cols + [change_col]].copy()
            tr[change_col] = pd.to_numeric(tr[change_col], errors="coerce").replace([np.inf, -np.inf], np.nan)
            overall = float(tr[change_col].median()) if tr[change_col].notna().any() else 0.0
            grp = tr.groupby(group_cols)[change_col].median()

            g = df.loc[miss2, group_cols]
            if len(group_cols) == 1:
                mapped = g[group_cols[0]].map(grp)
            else:
                keys = list(map(tuple, g[group_cols].to_numpy()))
                mapped = pd.Series(keys, index=g.index).map(grp)

            fill = pd.to_numeric(mapped, errors="coerce").replace([np.inf, -np.inf], np.nan).fillna(overall)
            df.loc[miss2, change_col] = fill.to_numpy()

_fill_change_after_levels("dlcch", "dlc")
_fill_change_after_levels("recch", "rect")
_fill_change_after_levels("invch", "invt")
_fill_change_after_levels("chech", "che")
if "apalch" in df.columns and "ap" in df.columns:
    _fill_change_after_levels("apalch", "ap")

# ---------------------------------------------------------------------------
# 3.11 Guardrail: cap ONLY-imputed values (miss_* == 1) to TRAIN-observed quantile band
# ---------------------------------------------------------------------------
REG_NONNEG = set(["at","act","lct","lt","dlc","dltt","che","ppent","rect","invt","caps"])  # for this script

def _cap_imputed_to_train_quantiles(
    df_all: pd.DataFrame,
    col: str,
    lower_q: float = 0.01,
    upper_q: float = 0.99,
    nonneg: bool = False,
) -> None:
    flag = f"miss_{col}"
    if col not in df_all.columns or flag not in df_all.columns:
        return

    miss_mask = df_all[flag].astype(bool)
    if not miss_mask.any():
        return

    # bounds from TRAIN & originally-observed values
    obs = pd.to_numeric(df_all.loc[train_mask_for_imputation & (~miss_mask), col], errors="coerce").replace(
        [np.inf, -np.inf], np.nan
    )
    if obs.notna().sum() < 200:
        obs = pd.to_numeric(df_all.loc[train_mask_for_imputation, col], errors="coerce").replace([np.inf, -np.inf], np.nan)

    if obs.notna().sum() == 0:
        return

    lo = float(obs.quantile(lower_q))
    hi = float(obs.quantile(upper_q))

    if nonneg:
        lo = max(0.0, lo) if np.isfinite(lo) else 0.0

    s = pd.to_numeric(df_all.loc[miss_mask, col], errors="coerce").replace([np.inf, -np.inf], np.nan)
    s = s.clip(lo, hi)
    if nonneg:
        s = s.clip(lower=0.0)
    df_all.loc[miss_mask, col] = s.to_numpy()

CAP_COLS = sorted(
    (
        set(STOCKS + FLOWS + ["dlcch", "apalch", "recch", "invch", "chech"])
        | set(KNN_TARGETS)
    ) & set(df.columns)
)

for c in CAP_COLS:
    _cap_imputed_to_train_quantiles(
        df,
        c,
        lower_q=0.01,
        upper_q=0.99,
        nonneg=(c in NONNEG_STOCKS) or (c in REG_NONNEG),
    )

# ---------------------------------------------------------------------------
# 3.12 EDA AFTER imputation (missingness reduction + distribution deltas)
# ---------------------------------------------------------------------------
df_raw_post = df[raw].copy(deep=True)

post_miss = pd.DataFrame(
    {
        "col": raw,
        "n_na_post": [int(df_raw_post[c].isna().sum()) for c in raw],
        "pct_na_post": [float(df_raw_post[c].isna().mean() * 100.0) for c in raw],
        "train_pct_na_post": [
            float(df_raw_post.loc[train_mask_for_imputation, c].isna().mean() * 100.0) for c in raw
        ],
    }
)

changes = pre_miss.merge(post_miss, on="col", how="left")
changes["n_imputed"] = changes["n_na_pre"] - changes["n_na_post"]
changes["pct_points_na_reduction"] = changes["pct_na_pre"] - changes["pct_na_post"]

x_post = df_raw_post[raw].apply(pd.to_numeric, errors="coerce").replace([np.inf, -np.inf], np.nan)
q_post = x_post.quantile([0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99]).T
post_dist = pd.DataFrame(
    {
        "n_nonmiss_post": x_post.notna().sum(),
        "mean_post": x_post.mean(),
        "std_post": x_post.std(ddof=0),
        "min_post": x_post.min(),
        "p01_post": q_post[0.01],
        "p05_post": q_post[0.05],
        "p25_post": q_post[0.25],
        "p50_post": q_post[0.50],
        "p75_post": q_post[0.75],
        "p95_post": q_post[0.95],
        "p99_post": q_post[0.99],
        "max_post": x_post.max(),
    }
)

pre_dist_key = pre_dist[["n_nonmiss_pre","mean_pre","std_pre","p01_pre","p50_pre","p99_pre"]].copy() if raw else pd.DataFrame()
post_dist_key = post_dist[["n_nonmiss_post","mean_post","std_post","p01_post","p50_post","p99_post"]].copy() if raw else pd.DataFrame()

dist_delta = pre_dist_key.join(post_dist_key, how="outer")
dist_delta["delta_mean"] = dist_delta["mean_post"] - dist_delta["mean_pre"]
dist_delta["delta_std"]  = dist_delta["std_post"]  - dist_delta["std_pre"]
dist_delta["delta_p50"]  = dist_delta["p50_post"]  - dist_delta["p50_pre"]

# Imputed-only diagnostics (based on original missingness in df_raw_pre)
rows = []
for c in raw:
    pre_na_mask = df_raw_pre[c].isna()
    n_imp = int(pre_na_mask.sum())
    if n_imp == 0:
        rows.append((c, 0, np.nan, np.nan, np.nan, np.nan, np.nan))
        continue

    imp_vals = pd.to_numeric(df_raw_post.loc[pre_na_mask, c], errors="coerce").replace([np.inf, -np.inf], np.nan)
    obs_vals = pd.to_numeric(df_raw_pre.loc[~pre_na_mask, c], errors="coerce").replace([np.inf, -np.inf], np.nan)

    rows.append(
        (
            c,
            n_imp,
            float(imp_vals.mean()) if imp_vals.notna().any() else np.nan,
            float(imp_vals.median()) if imp_vals.notna().any() else np.nan,
            float(imp_vals.std(ddof=0)) if imp_vals.notna().any() else np.nan,
            float(obs_vals.mean()) if obs_vals.notna().any() else np.nan,
            float(obs_vals.median()) if obs_vals.notna().any() else np.nan,
        )
    )

imputed_only = pd.DataFrame(
    rows,
    columns=["col","n_imputed","imputed_mean","imputed_median","imputed_std","observed_mean_pre","observed_median_pre"],
).set_index("col")

print("\n=== EDA (AFTER imputation): Missingness on raw inputs + change ===")
cols_show = [
    "col", "n", "n_na_pre", "pct_na_pre", "n_na_post", "pct_na_post",
    "n_imputed", "pct_points_na_reduction", "train_pct_na_pre", "train_pct_na_post",
]
print(
    changes[cols_show]
    .sort_values(["n_imputed","pct_points_na_reduction"], ascending=[False, False])
    .round(4)
    .head(50)
)

print("\n=== Change analysis: Distribution deltas (post - pre) on raw inputs ===")
print(
    dist_delta[["n_nonmiss_pre","n_nonmiss_post","delta_mean","delta_std","delta_p50"]]
    .sort_values("delta_mean", key=lambda s: s.abs(), ascending=False)
    .round(6)
    .head(50)
)

print("\n=== Change analysis: Imputed-only vs observed (pre) summary ===")
print(
    imputed_only.assign(
        mean_gap_imputed_minus_observed=lambda d: d["imputed_mean"] - d["observed_mean_pre"],
        median_gap_imputed_minus_observed=lambda d: d["imputed_median"] - d["observed_median_pre"],
    )
    .sort_values("n_imputed", ascending=False)
    .round(6)
    .head(50)
)

```

    
    === EDA (BEFORE imputation): Missingness on raw inputs ===
           col      n  n_na_pre  pct_na_pre  train_n  train_pct_na_pre
    18   dlcch  75005     33143     44.1877    48458           42.9630
    5   apalch  75005     30371     40.4920    48458           39.0214
    75   txach  75005     22791     30.3860    48458           29.2501
    48  ivstch  75005     19194     25.5903    48458           23.0282
    66   recch  75005     12589     16.7842    48458           16.5938
    53  mkvalt  75005     12350     16.4656    48458           17.0189
    71    sppe  75005     12239     16.3176    48458           16.4307
    1      act  75005     10721     14.2937    48458           14.6581
    50     lct  75005     10695     14.2590    48458           14.5982
    84    xint  75005     10536     14.0471    48458           13.9234
    78  txditc  75005      9069     12.0912    48458           12.2044
    79     txp  75005      7942     10.5886    48458           10.6917
    29   esubc  75005      6798      9.0634    48458            8.8964
    49     lco  75005      6779      9.0381    48458            9.3277
    0      aco  75005      6778      9.0367    48458            9.3277
    60  prcc_f  75005      5956      7.9408    48458            8.8902
    59  prcc_c  75005      5942      7.9221    48458            8.8819
    40   invch  75005      5315      7.0862    48458            7.3445
    72   sppiv  75005      3967      5.2890    48458            4.9631
    44   ivaeq  75005      3548      4.7304    48458            4.8826
    61  prstkc  75005      3410      4.5464    48458            4.9486
    8     caps  75005      3074      4.0984    48458            4.2655
    6      aqc  75005      2812      3.7491    48458            3.2647
    23      dp  75005      2722      3.6291    48458            3.7909
    65      re  75005      2596      3.4611    48458            3.6650
    46    ivch  75005      2540      3.3864    48458            3.3844
    45    ivao  75005      2529      3.3718    48458            3.1821
    77    txdc  75005      2472      3.2958    48458            3.4050
    69     siv  75005      2299      3.0651    48458            3.1285
    57   oibdp  75005      2142      2.8558    48458            2.9366
    14    cstk  75005      1925      2.5665    48458            3.0294
    24     dpc  75005      1757      2.3425    48458            2.4268
    19   dltis  75005      1756      2.3412    48458            2.4681
    58   ppent  75005      1723      2.2972    48458            2.3814
    73    sstk  75005      1436      1.9145    48458            1.8593
    20    dltr  75005      1388      1.8505    48458            2.0616
    25      dv  75005      1137      1.5159    48458            1.5911
    70     spi  75005      1126      1.5012    48458            1.3785
    67    rect  75005       880      1.1733    48458            1.1247
    52    mibt  75005       842      1.1226    48458            1.3022
    13    csho  75005       804      1.0719    48458            1.2939
    28     dvt  75005       706      0.9413    48458            1.1412
    26     dvc  75005       703      0.9373    48458            1.1350
    39   intan  75005       654      0.8719    48458            0.8977
    41    invt  75005       652      0.8693    48458            0.8255
    76  txbcof  75005       619      0.8253    48458            1.0071
    9     capx  75005       503      0.6706    48458            0.7429
    83   xidoc  75005       477      0.6360    48458            0.7202
    3   aoloch  75005       462      0.6160    48458            0.6893
    38     ibc  75005       454      0.6053    48458            0.6810
    
    === EDA (BEFORE imputation): Distribution summary (raw inputs) ===
             n_nonmiss_pre      mean_pre       std_pre       min_pre  \
    consol               0           NaN           NaN           NaN   
    datafmt              0           NaN           NaN           NaN   
    conm                 0           NaN           NaN           NaN   
    indfmt               0           NaN           NaN           NaN   
    dlcch            41862  2.168121e+03  1.155250e+06 -8.555368e+07   
    apalch           44634  8.385212e+03  3.253932e+05 -4.849894e+07   
    txach            52214  3.214308e+02  3.140000e+04 -3.282293e+06   
    ivstch           55811  6.871673e+04  3.980372e+06 -1.752327e+07   
    recch            62416 -1.318528e+04  3.129434e+05 -2.297888e+07   
    mkvalt           62655  9.288907e+05  2.201298e+07  0.000000e+00   
    sppe             62766  6.939724e+03  1.245321e+05 -5.895100e+04   
    act              64284  6.440956e+05  4.645197e+06 -2.498000e+03   
    lct              64310  4.755088e+05  3.980895e+06  0.000000e+00   
    xint             64469  2.759126e+04  1.196406e+05 -7.284730e+05   
    txditc           65936  4.384545e+04  3.265589e+05 -1.616000e+03   
    txp              67063  1.062620e+04  1.001099e+05 -2.217000e+03   
    esubc            68207 -2.220041e+03  8.665624e+04 -8.775761e+06   
    lco              68226  2.039805e+05  1.784063e+06 -8.900000e+01   
    aco              68227  5.573244e+04  9.308496e+05 -2.500000e+01   
    prcc_f           69029  6.838158e+02  1.018087e+04  1.000000e-04   
    prcc_c           69039  7.067634e+02  1.003861e+04  1.000000e-04   
    invch            69690 -3.860008e+04  1.008997e+06 -8.472324e+07   
    sppiv            71038 -7.943608e+03  1.753101e+05 -1.598761e+07   
    ivaeq            71457  6.873078e+04  9.162568e+05 -1.551500e+04   
    prstkc           71595  2.902337e+04  2.408411e+05 -1.131100e+04   
    caps             71931  5.224117e+05  2.924028e+06 -2.333792e+07   
    aqc              72193  2.583727e+04  3.427732e+05 -2.002407e+07   
    dp               72283  7.989028e+04  7.010127e+05 -4.121000e+03   
    re               72409  4.063079e+05  4.773704e+06 -8.108442e+07   
    ivch             72465  4.336588e+05  1.267386e+07  0.000000e+00   
    ivao             72476  1.117387e+06  1.830844e+07  0.000000e+00   
    txdc             72533 -2.552908e+03  8.582746e+04 -8.127405e+06   
    siv              72706  3.204806e+05  9.796796e+06 -3.080000e+02   
    oibdp            72863  2.472644e+05  1.847387e+06 -6.234977e+06   
    cstk             73080  1.440901e+05  1.035999e+06 -1.466100e+04   
    dpc              73248  8.009984e+04  6.495898e+05 -1.551370e+05   
    dltis            73249  2.584506e+05  2.896443e+06 -4.257400e+04   
    ppent            73282  5.870571e+05  5.242606e+06  0.000000e+00   
    sstk             73569  2.949253e+04  2.324514e+05 -5.012830e+05   
    dltr             73617  2.679871e+05  2.856270e+06 -2.844300e+04   
    dv               73868  4.155940e+04  3.023818e+05 -1.631969e+06   
    spi              73879 -1.290533e+04  1.992509e+05 -1.354613e+07   
    rect             74125  2.121967e+06  3.060638e+07 -7.300000e+01   
    mibt             74163  5.164076e+04  6.023643e+05 -1.237174e+06   
    csho             74201  2.254364e+05  5.059555e+06  0.000000e+00   
    dvt              74299  4.154428e+04  3.087856e+05 -1.329470e+05   
    dvc              74302  4.081004e+04  3.071635e+05 -2.473000e+03   
    intan            74351  3.830857e+05  2.603599e+06  0.000000e+00   
    invt             74353  1.471455e+05  1.356403e+06  0.000000e+00   
    txbcof           74386  2.081901e+02  4.374733e+03 -9.510600e+04   
    
                  p01_pre      p05_pre     p25_pre     p50_pre      p75_pre  \
    consol            NaN          NaN         NaN         NaN          NaN   
    datafmt           NaN          NaN         NaN         NaN          NaN   
    conm              NaN          NaN         NaN         NaN          NaN   
    indfmt            NaN          NaN         NaN         NaN          NaN   
    dlcch   -2.265422e+05   -8701.6500      0.0000      0.0000       1.0000   
    apalch  -1.053346e+05  -14648.8000   -119.0000     50.0000    1693.0000   
    txach   -9.192830e+03    -193.0000      0.0000      0.0000       0.0000   
    ivstch  -2.099582e+05  -10799.5000      0.0000      0.0000       0.0000   
    recch   -2.890472e+05  -52494.2500  -2252.2500    -17.0000      54.0000   
    mkvalt   5.154000e-01       3.3549     48.9974    467.1767    4760.4262   
    sppe     0.000000e+00       0.0000      0.0000      0.0000       6.8300   
    act      1.700000e-01      21.0000   1732.0000  23074.0000  214833.2500   
    lct      6.600000e-01      26.5090   1277.0000  11405.0000  105630.0000   
    xint     0.000000e+00       0.0000     15.0000    433.0000    8992.0000   
    txditc   0.000000e+00       0.0000      0.0000      0.0000    1883.5000   
    txp      0.000000e+00       0.0000      0.0000      0.0000     152.0000   
    esubc   -3.866582e+04    -484.7000      0.0000      0.0000       0.0000   
    lco      0.000000e+00       0.0000    301.0000   3986.5000   46136.5000   
    aco      0.000000e+00       0.0000     58.0000    946.0000   10293.0000   
    prcc_f   1.000000e-02       0.1016      2.6000     13.9200      42.8400   
    prcc_c   1.000000e-02       0.1000      2.6200     14.0100      43.0250   
    invch   -3.362948e+05  -33105.1500   -172.0000      0.0000       0.0000   
    sppiv   -1.356993e+05  -11708.4500    -13.0000      0.0000       0.0000   
    ivaeq    0.000000e+00       0.0000      0.0000      0.0000       0.0000   
    prstkc   0.000000e+00       0.0000      0.0000      0.0000     846.5000   
    caps     0.000000e+00       0.0000   1645.0000  26381.0000  268051.0000   
    aqc     -1.760872e+04       0.0000      0.0000      0.0000       0.0000   
    dp       0.000000e+00       0.0000     68.0000   1079.4700   15628.5000   
    re      -2.484404e+06 -701966.6000 -80817.0000  -1251.0000   29686.0000   
    ivch     0.000000e+00       0.0000      0.0000      0.0000     275.0000   
    ivao     0.000000e+00       0.0000      0.0000      0.0000    3979.7500   
    txdc    -1.337068e+05  -18598.4000   -105.0000      0.0000       5.0000   
    siv      0.000000e+00       0.0000      0.0000      0.0000      61.0000   
    oibdp   -2.060061e+05  -47857.8000  -1873.5000    800.4000   46909.0000   
    cstk     0.000000e+00       0.0000      8.0000    197.0000   13957.2500   
    dpc      0.000000e+00       0.0000    100.3000   1542.0000   21382.2500   
    dltis    0.000000e+00       0.0000      0.0000      0.1500    1740.0000   
    ppent    0.000000e+00       0.0000    514.0000   8837.5000   93080.2500   
    sstk     0.000000e+00       0.0000      0.0000     58.0000    4216.0000   
    dltr     0.000000e+00       0.0000      0.0000    140.7000    8902.0000   
    dv       0.000000e+00       0.0000      0.0000      0.0000     884.0000   
    spi     -3.215187e+05  -47237.4000  -1071.0000      0.0000       0.0000   
    rect     0.000000e+00       0.0000    225.0000   5841.0000   97807.0000   
    mibt    -1.887380e+03       0.0000      0.0000      0.0000       0.0000   
    csho     3.150000e+00      28.0000   7219.0000  36145.0000  107343.0000   
    dvt      0.000000e+00       0.0000      0.0000      0.0000    1178.4450   
    dvc      0.000000e+00       0.0000      0.0000      0.0000     621.0000   
    intan    0.000000e+00       0.0000      0.0000   1365.0000   45975.0000   
    invt     0.000000e+00       0.0000      0.0000    275.3000   12041.0000   
    txbcof   0.000000e+00       0.0000      0.0000      0.0000       0.0000   
    
                 p95_pre      p99_pre       max_pre  
    consol           NaN          NaN           NaN  
    datafmt          NaN          NaN           NaN  
    conm             NaN          NaN           NaN  
    indfmt           NaN          NaN           NaN  
    dlcch      12288.900    239569.72  1.674825e+08  
    apalch     40626.050    280635.99  2.085900e+07  
    txach        544.000     16622.89  3.282293e+06  
    ivstch     14654.500    206862.80  3.964813e+08  
    recch      14636.250    102245.15  3.777953e+07  
    mkvalt   1137140.389  16710068.76  3.522211e+09  
    sppe        7315.750     96130.00  7.819213e+06  
    act      1887389.100  11141665.87  2.648894e+08  
    lct      1150175.850   8749902.35  2.275619e+08  
    xint      126670.400    475515.16  3.895731e+06  
    txditc    138738.750    824518.10  1.309682e+07  
    txp        20801.100    229018.76  5.843978e+06  
    esubc        503.700     18641.34  3.314734e+06  
    lco       528718.500   3510482.25  1.976466e+08  
    aco       124386.000    926127.42  1.973027e+08  
    prcc_f       271.900     17260.04  1.369125e+06  
    prcc_c       275.000     17524.72  1.369125e+06  
    invch       9558.700     99906.23  1.919409e+07  
    sppiv        779.000     12781.32  7.378669e+06  
    ivaeq      69436.200   1094948.44  4.184002e+07  
    prstkc     87371.300    608747.74  1.551860e+07  
    caps     2034165.500   7334828.10  1.724641e+08  
    aqc        71387.800    578083.72  5.188218e+07  
    dp        220915.500   1361138.02  3.714574e+07  
    re       1402781.800  11353646.12  2.219454e+08  
    ivch      441795.800   3486163.72  1.169666e+09  
    ivao     1193829.500  10658327.50  1.099816e+09  
    txdc       11204.800     85023.32  4.507005e+06  
    siv       237122.750   2260226.70  9.068858e+08  
    oibdp     727543.600   4683708.38  7.350011e+07  
    cstk      420270.900   2924964.10  3.904769e+07  
    dpc       252347.000   1229121.52  3.648387e+07  
    dltis     692136.800   4321096.08  1.818726e+08  
    ppent    1817488.900  10065753.57  2.926841e+08  
    sstk      111376.400    525775.68  2.036114e+07  
    dltr      761854.400   3911400.36  1.796217e+08  
    dv        122377.700    899427.56  1.296791e+07  
    spi         2226.100     54763.82  1.148318e+07  
    rect     1917288.600  17121476.16  1.218880e+09  
    mibt       66241.300   1067080.28  3.075527e+07  
    csho      533438.000   2530459.00  6.064077e+08  
    dvt       124549.500    879155.68  1.485872e+07  
    dvc       120572.700    869661.95  1.437446e+07  
    intan    1345701.500   7366060.00  9.849622e+07  
    invt      385661.800   2910600.48  1.043358e+08  
    txbcof         0.000      3680.50  6.321680e+05  


    /var/folders/p1/_cwwbdbj51q1lwpynfnzdxpm0000gn/T/ipykernel_2546/2262392012.py:430: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
      obs_lag = obs_now.groupby(df["firm_id"]).shift(1).fillna(False)
    /var/folders/p1/_cwwbdbj51q1lwpynfnzdxpm0000gn/T/ipykernel_2546/2262392012.py:430: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
      obs_lag = obs_now.groupby(df["firm_id"]).shift(1).fillna(False)
    /var/folders/p1/_cwwbdbj51q1lwpynfnzdxpm0000gn/T/ipykernel_2546/2262392012.py:430: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
      obs_lag = obs_now.groupby(df["firm_id"]).shift(1).fillna(False)
    /var/folders/p1/_cwwbdbj51q1lwpynfnzdxpm0000gn/T/ipykernel_2546/2262392012.py:430: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
      obs_lag = obs_now.groupby(df["firm_id"]).shift(1).fillna(False)
    /var/folders/p1/_cwwbdbj51q1lwpynfnzdxpm0000gn/T/ipykernel_2546/2262392012.py:430: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
      obs_lag = obs_now.groupby(df["firm_id"]).shift(1).fillna(False)


    
    === EDA (AFTER imputation): Missingness on raw inputs + change ===
           col      n  n_na_pre  pct_na_pre  n_na_post  pct_na_post  n_imputed  \
    0    dlcch  75005     33143     44.1877          0       0.0000      33143   
    1   apalch  75005     30371     40.4920          0       0.0000      30371   
    2    txach  75005     22791     30.3860          0       0.0000      22791   
    4    recch  75005     12589     16.7842          0       0.0000      12589   
    5   mkvalt  75005     12350     16.4656          0       0.0000      12350   
    7      act  75005     10721     14.2937          0       0.0000      10721   
    8      lct  75005     10695     14.2590          0       0.0000      10695   
    9     xint  75005     10536     14.0471          0       0.0000      10536   
    12   esubc  75005      6798      9.0634          0       0.0000       6798   
    13     lco  75005      6779      9.0381          0       0.0000       6779   
    14     aco  75005      6778      9.0367          0       0.0000       6778   
    17   invch  75005      5315      7.0862          0       0.0000       5315   
    19   ivaeq  75005      3548      4.7304          0       0.0000       3548   
    21    caps  75005      3074      4.0984          0       0.0000       3074   
    23      dp  75005      2722      3.6291          0       0.0000       2722   
    24      re  75005      2596      3.4611          0       0.0000       2596   
    27    txdc  75005      2472      3.2958          0       0.0000       2472   
    28     siv  75005      2299      3.0651          0       0.0000       2299   
    29   oibdp  75005      2142      2.8558          0       0.0000       2142   
    30    cstk  75005      1925      2.5665          0       0.0000       1925   
    33   ppent  75005      1723      2.2972          0       0.0000       1723   
    38    rect  75005       880      1.1733          0       0.0000        880   
    39    mibt  75005       842      1.1226          0       0.0000        842   
    40    csho  75005       804      1.0719          0       0.0000        804   
    43   intan  75005       654      0.8719          0       0.0000        654   
    44    invt  75005       652      0.8693          0       0.0000        652   
    45  txbcof  75005       619      0.8253          0       0.0000        619   
    49     ibc  75005       454      0.6053          0       0.0000        454   
    54   ivncf  75005       316      0.4213          0       0.0000        316   
    55   fincf  75005       315      0.4200          0       0.0000        315   
    56   chech  75005       304      0.4053          0       0.0000        304   
    57   oancf  75005       303      0.4040          0       0.0000        303   
    58    pstk  75005       219      0.2920          0       0.0000        219   
    59   pstkn  75005       211      0.2813          0       0.0000        211   
    60    dltt  75005       188      0.2506          0       0.0000        188   
    61    tstk  75005       180      0.2400          0       0.0000        180   
    62     ceq  75005       149      0.1987          0       0.0000        149   
    15  prcc_f  75005      5956      7.9408       5826       7.7675        130   
    16  prcc_c  75005      5942      7.9221       5813       7.7501        129   
    63      lt  75005       101      0.1347          0       0.0000        101   
    64      ap  75005        88      0.1173          0       0.0000         88   
    65   pstkr  75005        75      0.1000          0       0.0000         75   
    66     dvp  75005        36      0.0480          0       0.0000         36   
    67     dlc  75005        31      0.0413          0       0.0000         31   
    68     txt  75005        15      0.0200          0       0.0000         15   
    69    nopi  75005        12      0.0160          0       0.0000         12   
    70     che  75005         6      0.0080          0       0.0000          6   
    71      ao  75005         5      0.0067          0       0.0000          5   
    74     seq  75005         3      0.0040          0       0.0000          3   
    3   ivstch  75005     19194     25.5903      19194      25.5903          0   
    
        pct_points_na_reduction  train_pct_na_pre  train_pct_na_post  
    0                   44.1877           42.9630             0.0000  
    1                   40.4920           39.0214             0.0000  
    2                   30.3860           29.2501             0.0000  
    4                   16.7842           16.5938             0.0000  
    5                   16.4656           17.0189             0.0000  
    7                   14.2937           14.6581             0.0000  
    8                   14.2590           14.5982             0.0000  
    9                   14.0471           13.9234             0.0000  
    12                   9.0634            8.8964             0.0000  
    13                   9.0381            9.3277             0.0000  
    14                   9.0367            9.3277             0.0000  
    17                   7.0862            7.3445             0.0000  
    19                   4.7304            4.8826             0.0000  
    21                   4.0984            4.2655             0.0000  
    23                   3.6291            3.7909             0.0000  
    24                   3.4611            3.6650             0.0000  
    27                   3.2958            3.4050             0.0000  
    28                   3.0651            3.1285             0.0000  
    29                   2.8558            2.9366             0.0000  
    30                   2.5665            3.0294             0.0000  
    33                   2.2972            2.3814             0.0000  
    38                   1.1733            1.1247             0.0000  
    39                   1.1226            1.3022             0.0000  
    40                   1.0719            1.2939             0.0000  
    43                   0.8719            0.8977             0.0000  
    44                   0.8693            0.8255             0.0000  
    45                   0.8253            1.0071             0.0000  
    49                   0.6053            0.6810             0.0000  
    54                   0.4213            0.4788             0.0000  
    55                   0.4200            0.4767             0.0000  
    56                   0.4053            0.4623             0.0000  
    57                   0.4040            0.4602             0.0000  
    58                   0.2920            0.2951             0.0000  
    59                   0.2813            0.2868             0.0000  
    60                   0.2506            0.2889             0.0000  
    61                   0.2400            0.2414             0.0000  
    62                   0.1987            0.2043             0.0000  
    15                   0.1733            8.8902             8.6735  
    16                   0.1720            8.8819             8.6652  
    63                   0.1347            0.1259             0.0000  
    64                   0.1173            0.0991             0.0000  
    65                   0.1000            0.1032             0.0000  
    66                   0.0480            0.0495             0.0000  
    67                   0.0413            0.0495             0.0000  
    68                   0.0200            0.0144             0.0000  
    69                   0.0160            0.0227             0.0000  
    70                   0.0080            0.0124             0.0000  
    71                   0.0067            0.0103             0.0000  
    74                   0.0040            0.0062             0.0000  
    3                    0.0000           23.0282            23.0282  
    
    === Change analysis: Distribution deltas (post - pre) on raw inputs ===
            n_nonmiss_pre  n_nonmiss_post     delta_mean     delta_std   delta_p50
    mkvalt          62655           75005  339908.830337 -1.766668e+06    119.9347
    rect            74125           75005  -24635.263120 -1.792322e+05   -216.0000
    act             64284           75005  -22454.818040 -3.155523e+05  14532.0000
    lco             68226           75005   21036.613834 -6.219595e+04   1871.5000
    caps            71931           75005  -19698.650266 -5.887712e+04  -2173.0000
    lct             64310           75005  -17372.380933 -2.682944e+05   6589.0000
    re              72409           75005  -12813.089960 -8.183700e+04    273.0000
    ppent           73282           75005  -12344.410435 -5.990450e+04    238.5000
    siv             72706           75005   -9823.145072 -1.511525e+05      0.0000
    lt              74904           75005   -6279.209873 -4.631426e+04     62.0000
    aco             68227           75005    4416.620600 -4.056789e+04    459.0000
    apalch          44634           75005    3844.702802 -7.037396e+04     15.0000
    cstk            73080           75005   -3371.893764 -1.293280e+04      0.3800
    ivaeq           71457           75005   -3175.746266 -2.180294e+04      0.0000
    recch           62416           75005    2935.206610 -2.586869e+04      6.7200
    invch           69690           75005    2632.929381 -3.631501e+04      0.0000
    intan           74351           75005   -2435.410689 -1.065323e+04     17.0000
    ap              74917           75005   -2409.458333 -2.223757e+04      4.0000
    dltt            74817           75005   -2284.657106 -1.061125e+04    -33.0000
    ceq             74856           75005    1948.514470 -2.647329e+03    -16.5000
    dp              72283           75005    1514.527654 -1.139971e+04    141.5300
    xint            64469           75005   -1348.022514 -7.828917e+03    -15.0000
    csho            74201           75005   -1198.257302 -2.703964e+04    -64.0000
    invt            74353           75005    -988.862996 -5.809818e+03      7.0900
    oibdp           72863           75005    -792.923284 -2.530035e+04    170.6000
    oancf           74702           75005    -727.783680 -4.392208e+03     -5.2000
    mibt            74163           75005    -579.008278 -3.365934e+03      0.0000
    ivncf           74689           75005     558.836145 -3.279674e+03      7.0000
    esubc           68207           75005     201.211062 -4.017797e+03      0.0000
    ibc             74551           75005    -190.518158 -2.001482e+03     -0.1700
    chech           74701           75005    -184.196906 -4.120815e+03     -0.1100
    dlcch           41862           75005     144.280720 -2.911747e+05      0.0000
    tstk            74825           75005    -140.814051 -5.864455e+02      0.0000
    txach           52214           75005     -97.669890 -5.200987e+03      0.0000
    txdc            72533           75005      84.138237 -1.424961e+03      0.0000
    che             74999           75005     -70.624838 -6.643516e+02     -2.0000
    dlc             74974           75005     -60.471895 -1.545875e+03      1.0000
    fincf           74690           75005     -43.033951 -3.252390e+03      0.0700
    seq             75002           75005     -38.472134 -1.212666e+02     -3.5000
    ao              75000           75005     -37.929044 -4.493026e+02     -0.1000
    pstk            74786           75005     -33.132292 -3.322790e+02      0.0000
    pstkn           74794           75005     -12.933761 -2.168597e+02      0.0000
    pstkr           74930           75005      -7.086945 -8.506999e+01      0.0000
    nopi            74993           75005       6.575298 -6.715941e+01      0.0000
    txt             74990           75005      -5.758203 -2.315719e+01      0.0000
    txbcof          74386           75005      -1.718148 -1.804855e+01      0.0000
    prcc_c          69039           69167      -1.207154 -9.250138e+00     -0.0100
    prcc_f          69029           69158      -1.191174 -9.460768e+00     -0.0200
    dvp             74969           75005      -0.331571 -3.001890e+00      0.0000
    at              75005           75005       0.000000  0.000000e+00      0.0000
    
    === Change analysis: Imputed-only vs observed (pre) summary ===
            n_imputed  imputed_mean  imputed_median   imputed_std  \
    col                                                             
    dlcch       33143  2.494638e+03        0.000000  6.300460e+04   
    apalch      30371  1.788019e+04      115.700000  7.037716e+04   
    txach       22791  0.000000e+00        0.000000  0.000000e+00   
    ivstch      19194           NaN             NaN           NaN   
    recch       12589  4.302616e+03        0.000000  7.211934e+04   
    mkvalt      12350  2.993252e+06     5729.901897  5.252785e+06   
    sppe        12239           NaN             NaN           NaN   
    act         10721  4.869998e+05   139778.546064  1.320445e+06   
    lct         10695  3.536747e+05    68153.642247  1.165729e+06   
    xint        10536  1.799479e+04      418.000000  3.653868e+04   
    txditc       9069           NaN             NaN           NaN   
    txp          7942           NaN             NaN           NaN   
    esubc        6798  0.000000e+00        0.000000  0.000000e+00   
    lco          6779  4.367362e+05    88275.927500  8.491035e+05   
    aco          6778  1.046065e+05    20443.802494  2.162451e+05   
    prcc_f       5956  4.521732e+01        3.050000  1.277017e+02   
    prcc_c       5942  5.445709e+01        2.100000  1.993917e+02   
    invch        5315 -1.444317e+03        0.000000  3.507483e+04   
    sppiv        3967           NaN             NaN           NaN   
    ivaeq        3548  1.595255e+03        0.000000  2.544047e+04   
    prstkc       3410           NaN             NaN           NaN   
    caps         3074  4.176843e+04     6931.009560  1.082224e+05   
    aqc          2812           NaN             NaN           NaN   
    dp           2722  1.216232e+05    53082.966710  2.300360e+05   
    re           2596  3.610538e+04      -63.518032  5.242238e+05   
    ivch         2540           NaN             NaN           NaN   
    ivao         2529           NaN             NaN           NaN   
    txdc         2472  0.000000e+00        0.000000  0.000000e+00   
    siv          2299  0.000000e+00        0.000000  0.000000e+00   
    oibdp        2142  2.194991e+05    90782.404109  4.015096e+05   
    cstk         1925  1.270890e+04      204.974675  1.374580e+05   
    dpc          1757           NaN             NaN           NaN   
    dltis        1756           NaN             NaN           NaN   
    ppent        1723  4.968479e+04    15615.380231  1.276160e+05   
    sstk         1436           NaN             NaN           NaN   
    dltr         1388           NaN             NaN           NaN   
    dv           1137           NaN             NaN           NaN   
    spi          1126           NaN             NaN           NaN   
    rect          880  2.223032e+04      140.926861  1.245324e+05   
    mibt          842  6.294546e+01        0.000000  7.999841e+02   
    csho          804  1.136512e+05    34012.802470  3.595284e+05   
    dvt           706           NaN             NaN           NaN   
    dvc           703           NaN             NaN           NaN   
    intan         654  1.037768e+05     3307.075685  5.935900e+05   
    invt          652  3.338837e+04      614.036198  1.333228e+05   
    txbcof        619  0.000000e+00        0.000000  0.000000e+00   
    capx          503           NaN             NaN           NaN   
    xidoc         477           NaN             NaN           NaN   
    aoloch        462           NaN             NaN           NaN   
    ibc           454  4.592746e+04        0.070076  2.269067e+05   
    
            observed_mean_pre  observed_median_pre  \
    col                                              
    dlcch        2.168121e+03               0.0000   
    apalch       8.385212e+03              50.0000   
    txach        3.214308e+02               0.0000   
    ivstch       6.871673e+04               0.0000   
    recch       -1.318528e+04             -17.0000   
    mkvalt       9.288907e+05             467.1767   
    sppe         6.939724e+03               0.0000   
    act          6.440956e+05           23074.0000   
    lct          4.755088e+05           11405.0000   
    xint         2.759126e+04             433.0000   
    txditc       4.384545e+04               0.0000   
    txp          1.062620e+04               0.0000   
    esubc       -2.220041e+03               0.0000   
    lco          2.039805e+05            3986.5000   
    aco          5.573244e+04             946.0000   
    prcc_f       6.838158e+02              13.9200   
    prcc_c       7.067634e+02              14.0100   
    invch       -3.860008e+04               0.0000   
    sppiv       -7.943608e+03               0.0000   
    ivaeq        6.873078e+04               0.0000   
    prstkc       2.902337e+04               0.0000   
    caps         5.224117e+05           26381.0000   
    aqc          2.583727e+04               0.0000   
    dp           7.989028e+04            1079.4700   
    re           4.063079e+05           -1251.0000   
    ivch         4.336588e+05               0.0000   
    ivao         1.117387e+06               0.0000   
    txdc        -2.552908e+03               0.0000   
    siv          3.204806e+05               0.0000   
    oibdp        2.472644e+05             800.4000   
    cstk         1.440901e+05             197.0000   
    dpc          8.009984e+04            1542.0000   
    dltis        2.584506e+05               0.1500   
    ppent        5.870571e+05            8837.5000   
    sstk         2.949253e+04              58.0000   
    dltr         2.679871e+05             140.7000   
    dv           4.155940e+04               0.0000   
    spi         -1.290533e+04               0.0000   
    rect         2.121967e+06            5841.0000   
    mibt         5.164076e+04               0.0000   
    csho         2.254364e+05           36145.0000   
    dvt          4.154428e+04               0.0000   
    dvc          4.081004e+04               0.0000   
    intan        3.830857e+05            1365.0000   
    invt         1.471455e+05             275.3000   
    txbcof       2.081901e+02               0.0000   
    capx         8.862010e+04             887.0000   
    xidoc        8.325909e+02               0.0000   
    aoloch       3.689360e+04              -0.0100   
    ibc          7.740282e+04               2.0000   
    
            mean_gap_imputed_minus_observed  median_gap_imputed_minus_observed  
    col                                                                         
    dlcch                      3.265177e+02                           0.000000  
    apalch                     9.494977e+03                          65.700000  
    txach                     -3.214308e+02                           0.000000  
    ivstch                              NaN                                NaN  
    recch                      1.748790e+04                          17.000000  
    mkvalt                     2.064361e+06                        5262.725197  
    sppe                                NaN                                NaN  
    act                       -1.570958e+05                      116704.546064  
    lct                       -1.218341e+05                       56748.642247  
    xint                      -9.596472e+03                         -15.000000  
    txditc                              NaN                                NaN  
    txp                                 NaN                                NaN  
    esubc                      2.220041e+03                           0.000000  
    lco                        2.327557e+05                       84289.427500  
    aco                        4.887410e+04                       19497.802494  
    prcc_f                    -6.385985e+02                         -10.870000  
    prcc_c                    -6.523063e+02                         -11.910000  
    invch                      3.715576e+04                           0.000000  
    sppiv                               NaN                                NaN  
    ivaeq                     -6.713553e+04                           0.000000  
    prstkc                              NaN                                NaN  
    caps                      -4.806432e+05                      -19449.990440  
    aqc                                 NaN                                NaN  
    dp                         4.173297e+04                       52003.496710  
    re                        -3.702025e+05                        1187.481968  
    ivch                                NaN                                NaN  
    ivao                                NaN                                NaN  
    txdc                       2.552908e+03                           0.000000  
    siv                       -3.204806e+05                           0.000000  
    oibdp                     -2.776527e+04                       89982.004109  
    cstk                      -1.313812e+05                           7.974675  
    dpc                                 NaN                                NaN  
    dltis                               NaN                                NaN  
    ppent                     -5.373723e+05                        6777.880231  
    sstk                                NaN                                NaN  
    dltr                                NaN                                NaN  
    dv                                  NaN                                NaN  
    spi                                 NaN                                NaN  
    rect                      -2.099736e+06                       -5700.073139  
    mibt                      -5.157781e+04                           0.000000  
    csho                      -1.117852e+05                       -2132.197530  
    dvt                                 NaN                                NaN  
    dvc                                 NaN                                NaN  
    intan                     -2.793088e+05                        1942.075685  
    invt                      -1.137572e+05                         338.736198  
    txbcof                    -2.081901e+02                           0.000000  
    capx                                NaN                                NaN  
    xidoc                               NaN                                NaN  
    aoloch                              NaN                                NaN  
    ibc                       -3.147536e+04                          -1.929924  



```python
# =============================================================================
# Feature Engineering & Target Construction
#   - Build leverage / coverage / cash-flow-to-debt ratios commonly used in credit analysis
#   - Define a 'highly leveraged' distress proxy from multiple ratio-based conditions
#   - Create the supervised-learning target: next-year distress within the same firm
# =============================================================================

firm_col = "firm_id"

dlc = pd.to_numeric(df.get("dlc", np.nan), errors="coerce")
dltt = pd.to_numeric(df.get("dltt", np.nan), errors="coerce")
df["total_debt"] = pd.concat([dlc, dltt], axis=1).sum(axis=1, min_count=1)

seq = pd.to_numeric(df.get("seq", np.nan), errors="coerce")
mibt = pd.to_numeric(df.get("mibt", 0.0), errors="coerce")
df["equity_plus_mi_sp"] = seq + mibt
df["total_capital_sp"] = df["total_debt"] + df["equity_plus_mi_sp"]

# --- CHANGED (minimal): handle total capital <= 0 as extreme leverage + flag ---
cap_s = pd.to_numeric(df["total_capital_sp"], errors="coerce")
df["cap_nonpos_flag"] = (cap_s.notna() & (cap_s <= 0)).astype("int8")
df["sp_debt_to_capital"] = safe_divide(df["total_debt"], cap_s)
df.loc[df["cap_nonpos_flag"] == 1, "sp_debt_to_capital"] = np.inf
# ---------------------------------------------------------------------------

oibdp = pd.to_numeric(df.get("oibdp", np.nan), errors="coerce")
xint = pd.to_numeric(df.get("xint", np.nan), errors="coerce")

# --- CHANGED (minimal): handle EBITDA <= 0 as extreme leverage + flag ---
df["ebitda_nonpos_flag"] = (oibdp.notna() & (oibdp <= 0)).astype("int8")
df["sp_debt_to_ebitda"] = safe_divide(df["total_debt"], oibdp)
df.loc[df["ebitda_nonpos_flag"] == 1, "sp_debt_to_ebitda"] = np.inf
# ---------------------------------------------------------------------------

# --- OPTIONAL transparency (simple): flag if tax proxy components missing (no complex proxying) ---
txt_raw = pd.to_numeric(df.get("txt", np.nan), errors="coerce")
txdc_raw = pd.to_numeric(df.get("txdc", np.nan), errors="coerce")
txach_raw = pd.to_numeric(df.get("txach", np.nan), errors="coerce")
df["tax_proxy_incomplete"] = (txt_raw.isna() | txdc_raw.isna() | txach_raw.isna()).astype("int8")

txt = txt_raw.fillna(0.0)
txdc = txdc_raw.fillna(0.0)
txach = txach_raw.fillna(0.0)
df["cash_tax_paid_proxy"] = txt - txdc - txach

df["ffo_proxy"] = oibdp - xint - pd.to_numeric(df["cash_tax_paid_proxy"], errors="coerce")
df["sp_ffo_to_debt"] = safe_divide(df["ffo_proxy"], df["total_debt"])

oancf = pd.to_numeric(df.get("oancf", np.nan), errors="coerce")
capx = pd.to_numeric(df.get("capx", np.nan), errors="coerce")
df["sp_cfo_to_debt"] = safe_divide(oancf, df["total_debt"])
df["focf"] = oancf - capx
df["sp_focf_to_debt"] = safe_divide(df["focf"], df["total_debt"])

# --- CHANGED (minimal): make outflows robust to sign conventions ---
dv = pd.to_numeric(df.get("dv", 0.0), errors="coerce").fillna(0.0).abs()
prstkc = pd.to_numeric(df.get("prstkc", 0.0), errors="coerce").fillna(0.0).abs()
df["dcf"] = df["focf"] - dv - prstkc
df["sp_dcf_to_debt"] = safe_divide(df["dcf"], df["total_debt"])
# ------------------------------------------------------------------

# Log transforms (log1p handles 0 nicely). Negative → NaN.
for c in ["at", "mkvalt"]:
    if c in df.columns:
        s = pd.to_numeric(df[c], errors="coerce")
        df[f"log_{c}"] = np.where(s >= 0, np.log1p(s), np.nan)

# Interest coverage: EBITDA / |interest expense|
# Interest coverage can explode when interest expense is near zero.
# Stabilize by flooring the denominator, capping extreme values, and log-transforming.
INT_FLOOR = 1.0  # minimum |interest expense| to avoid blow-ups
IC_CAP = 100.0  # cap extreme coverage magnitudes before log transform
denom_ic = np.maximum(xint.abs(), INT_FLOOR)
df["sp_interest_coverage_raw"] = safe_divide(oibdp, denom_ic)
df["sp_interest_coverage_is_capped"] = (df["sp_interest_coverage_raw"].abs() > IC_CAP).astype("int8")
df["sp_interest_coverage_denom_floored"] = (xint.abs() < INT_FLOOR).astype("int8")
df["sp_interest_coverage"] = np.sign(df["sp_interest_coverage_raw"]) * np.log1p(
    np.minimum(df["sp_interest_coverage_raw"].abs(), IC_CAP)
)

# Distress proxy: 'highly leveraged' condition using multiple credit-ratio thresholds
# NOTE (important): avoid defaulting missing label components to non-distress.
# Pandas BooleanDtype preserves <NA>, so missingness becomes an *unknown* label rather than 0.

td_s  = pd.to_numeric(df["total_debt"], errors="coerce")
cap_s = pd.to_numeric(df["total_capital_sp"], errors="coerce")
eb_s  = pd.to_numeric(oibdp, errors="coerce")
ffo_s = pd.to_numeric(df["ffo_proxy"], errors="coerce")

# Ratios (keep as Series aligned to df.index)
ffo_to_debt_pct = pd.Series(100.0 * safe_divide(ffo_s, td_s), index=df.index, dtype="float64")

debt_to_capital_pct = pd.Series(100.0 * safe_divide(td_s, cap_s), index=df.index, dtype="float64")
# cap <= 0 => extreme leverage (so hl_cap can still trigger)
debt_to_capital_pct = debt_to_capital_pct.mask(cap_s.notna() & (cap_s <= 0), np.inf)

debt_to_ebitda = pd.Series(safe_divide(td_s, eb_s), index=df.index, dtype="float64")
# EBITDA <= 0 => extreme leverage (so hl_deb can still trigger)
debt_to_ebitda = debt_to_ebitda.mask(eb_s.notna() & (eb_s <= 0), np.inf)

def _cmp_with_na(x: pd.Series, op: str, thr: float) -> pd.Series:
    """Comparison that returns <NA> when x is missing (prevents NaN -> False label drift)."""
    if op == "<":
        arr = np.where(x.isna(), pd.NA, x < thr)
    elif op == ">":
        arr = np.where(x.isna(), pd.NA, x > thr)
    else:
        raise ValueError(f"Unsupported op: {op}")
    return pd.Series(pd.array(arr, dtype="boolean"), index=x.index)

# td > 0 should also be <NA> when total debt is missing
td_pos = pd.Series(pd.array(np.where(td_s.isna(), pd.NA, td_s > 0), dtype="boolean"), index=df.index)

# Three “highly leveraged” conditions (S&P table), now missingness-aware
hl_ffo = td_pos & _cmp_with_na(ffo_to_debt_pct, "<", 15.0)      # FFO/total debt < 15%
hl_cap = td_pos & _cmp_with_na(debt_to_capital_pct, ">", 55.0)  # TD/total capital > 55% (or cap<=0 => inf)
hl_deb = td_pos & _cmp_with_na(debt_to_ebitda, ">", 4.5)        # TD/EBITDA > 4.5 (or EBITDA<=0 => inf)

is_highly_leveraged = hl_ffo & hl_cap & hl_deb  # BooleanDtype (can be <NA>)

# Equity strictly negative and not missing (kept as a separate, deterministic distress trigger)
is_equity_negative = (seq.notna() & (seq < 0))
eq_bool = pd.Series(pd.array(is_equity_negative.to_numpy(dtype=bool), dtype="boolean"), index=df.index)

# Final distress rule:
#   - 1 if evidence exists (high leverage or neg equity)
#   - <NA> if high leverage is <NA> and no neg-equity trigger (unknown rather than 0)
distress_bool = is_highly_leveraged | eq_bool
df["distress_dummy"] = distress_bool.astype("Int8")  # nullable integer to preserve <NA>

# Diagnostics (useful for auditability / reporting)
df["distress_label_incomplete"] = df["distress_dummy"].isna().astype("int8")
n_incomplete = int(df["distress_label_incomplete"].sum())
print(
    f"Distress proxy: incomplete label components -> distress_dummy=<NA> for {n_incomplete:,} rows "
    f"({(n_incomplete / max(len(df), 1)):.1%})."
)
# ------------------------------------------------------------------------------

# Target: next year's distress (within firm)
df["target_next_year_distress"] = (
    df.groupby(firm_col)["distress_dummy"].shift(-1)
)

# Indicator for whether the next-year label is observed (attrition / survivorship considerations)
df["has_next_year_obs"] = df["target_next_year_distress"].notna().astype("int8")
n_total = len(df)
n_missing_next = int((df["has_next_year_obs"] == 0).sum())
print(
    f"Next-year label availability: {n_total - n_missing_next:,}/{n_total:,} observed "
    f"({(1 - n_missing_next / max(n_total, 1)):.1%}); missing next-year={n_missing_next:,}."
)

# ---- Attrition diagnostics (next-year label missingness) ----
# Missing next-year observations are rarely random in corporate panels (delistings, M&A, coverage breaks).
# We report simple, publication-friendly diagnostics for selection risk.

try:
    _tmp = df[[firm_col, "fyear", "has_next_year_obs", "distress_dummy", "at"]].copy()
    _tmp["at"] = pd.to_numeric(_tmp["at"], errors="coerce")
    # Size deciles within year to avoid secular size drift
    _tmp["size_decile"] = _tmp.groupby("fyear")["at"].transform(
        lambda s: pd.qcut(s, 10, labels=False, duplicates="drop") + 1
        if s.notna().sum() >= 20 else pd.Series([np.nan] * len(s), index=s.index)
    )
    # 1) By year
    year_attr = (_tmp.groupby("fyear")["has_next_year_obs"]
                 .agg(["count", "mean"])
                 .rename(columns={"count": "n", "mean": "share_observed"}))
    year_attr["attrition_rate"] = 1.0 - year_attr["share_observed"]
    print("\nAttrition by year (target observability):")
    display(year_attr.reset_index().sort_values("fyear"))

    # 2) By current distress state
    state_attr = (_tmp.groupby("distress_dummy")["has_next_year_obs"]
                  .agg(["count", "mean"])
                  .rename(columns={"count": "n", "mean": "share_observed"}))
    state_attr["attrition_rate"] = 1.0 - state_attr["share_observed"]
    print("\nAttrition by current distress state (t):")
    display(state_attr.reset_index())

    # 3) By size decile (pooled)
    if "size_decile" in _tmp.columns:
        size_attr = (_tmp.groupby("size_decile")["has_next_year_obs"]
                     .agg(["count", "mean"])
                     .rename(columns={"count": "n", "mean": "share_observed"}))
        size_attr["attrition_rate"] = 1.0 - size_attr["share_observed"]
        print("\nAttrition by size decile (within-year deciles, pooled):")
        display(size_attr.reset_index().sort_values("size_decile"))
except Exception as e:
    print(f"Attrition diagnostics skipped due to error: {e}")


# Modeling sample: restrict to observed next-year labels (do NOT overwrite df)
df_model = df[df["has_next_year_obs"] == 1].copy().reset_index(drop=True)
df_model["target_next_year_distress"] = df_model["target_next_year_distress"].astype("int8")


```

    Distress proxy: incomplete label components -> distress_dummy=<NA> for 0 rows (0.0%).
    Next-year label availability: 63,602/75,005 observed (84.8%); missing next-year=11,403.
    
    Attrition by year (target observability):



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fyear</th>
      <th>n</th>
      <th>share_observed</th>
      <th>attrition_rate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2014</td>
      <td>7455</td>
      <td>0.908518</td>
      <td>0.091482</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2015</td>
      <td>7178</td>
      <td>0.915297</td>
      <td>0.084703</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2016</td>
      <td>6970</td>
      <td>0.922812</td>
      <td>0.077188</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2017</td>
      <td>6831</td>
      <td>0.927683</td>
      <td>0.072317</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2018</td>
      <td>6672</td>
      <td>0.925210</td>
      <td>0.074790</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2019</td>
      <td>6649</td>
      <td>0.937434</td>
      <td>0.062566</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2020</td>
      <td>6703</td>
      <td>0.934656</td>
      <td>0.065344</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2021</td>
      <td>6851</td>
      <td>0.936360</td>
      <td>0.063640</td>
    </tr>
    <tr>
      <th>8</th>
      <td>2022</td>
      <td>6848</td>
      <td>0.923919</td>
      <td>0.076081</td>
    </tr>
    <tr>
      <th>9</th>
      <td>2023</td>
      <td>6611</td>
      <td>0.919226</td>
      <td>0.080774</td>
    </tr>
    <tr>
      <th>10</th>
      <td>2024</td>
      <td>6237</td>
      <td>0.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>


    
    Attrition by current distress state (t):



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>distress_dummy</th>
      <th>n</th>
      <th>share_observed</th>
      <th>attrition_rate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>55649</td>
      <td>0.859782</td>
      <td>0.140218</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>19356</td>
      <td>0.814011</td>
      <td>0.185989</td>
    </tr>
  </tbody>
</table>
</div>


    
    Attrition by size decile (within-year deciles, pooled):



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>size_decile</th>
      <th>n</th>
      <th>share_observed</th>
      <th>attrition_rate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>7507</td>
      <td>0.814440</td>
      <td>0.185560</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>7499</td>
      <td>0.843846</td>
      <td>0.156154</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>7500</td>
      <td>0.856267</td>
      <td>0.143733</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>7498</td>
      <td>0.848493</td>
      <td>0.151507</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>7502</td>
      <td>0.844575</td>
      <td>0.155425</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
      <td>7498</td>
      <td>0.830888</td>
      <td>0.169112</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>7498</td>
      <td>0.838357</td>
      <td>0.161643</td>
    </tr>
    <tr>
      <th>7</th>
      <td>8</td>
      <td>7500</td>
      <td>0.853867</td>
      <td>0.146133</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9</td>
      <td>7501</td>
      <td>0.862685</td>
      <td>0.137315</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10</td>
      <td>7502</td>
      <td>0.886297</td>
      <td>0.113703</td>
    </tr>
  </tbody>
</table>
</div>



```python
# =============================================================================
# 4. Distinct moments / event indicators (interpretable "drivers & levers")
# Purpose:
#   - Translate continuous accounting ratios into discrete, management-relevant "events"
#   - Use TRAIN pool only for any distribution-based thresholds (no look-ahead)
# =============================================================================

df_events = df.copy()


# ----------------------------
# Helpers
# ----------------------------
def _as_series(x, index) -> pd.Series:
    """Ensure x is a pandas Series aligned to index (handles scalars, arrays, Series)."""
    if isinstance(x, pd.Series):
        return x.reindex(index)
    if isinstance(x, (pd.Index, list, tuple, np.ndarray)):
        return pd.Series(x, index=index, dtype="float64")
    return pd.Series([x] * len(index), index=index, dtype="float64")


def lag(series: pd.Series, k: int = 1) -> pd.Series:
    """Firm-level lag (t-k) using firm_id."""
    s = _as_series(series, df_events.index)
    return s.groupby(df_events["firm_id"]).shift(k)


def ratio(a, b) -> pd.Series:
    """Safe ratio a/b (handles zeros, inf, NaN)."""
    a_s = _as_series(a, df_events.index)
    b_s = _as_series(b, df_events.index)
    return pd.Series(safe_divide(a_s, b_s), index=df_events.index, dtype="float64")


# Define training mask for distribution-aware thresholds (exclude val/test)
pool_mask = (df_events["label_year"] <= TRAIN_CUTOFF_LABEL_YEAR)
train_mask = pool_mask.copy()
if "val_years" in globals() and len(val_years) > 0:
    train_mask = train_mask & (~df_events["label_year"].isin(val_years))

# =============================================================================
# 4.1 Dividend "moments"
# =============================================================================
# Distress literature treats dividend cuts as a signal of cash-flow stress / financing constraints.
# Given the mass at zero (many firms pay no dividends), define events conditional on prior payment.

div = pd.to_numeric(
    _as_series(df_events["dv"] if "dv" in df_events.columns else 0.0, df_events.index),
    errors="coerce",
).fillna(0.0)
div_l1 = lag(div, 1).fillna(0.0)

# Distribution-aware cut threshold among dividend payers (TRAIN pool only)
payer_mask_train = train_mask & (div_l1 > 0)
pct_change = (div - div_l1) / div_l1.replace(0, np.nan)
df_events["div_pct_change"] = pct_change

cut_q = pct_change.loc[payer_mask_train].quantile(0.10)  # 10th percentile among payers
cut_threshold = float(cut_q) if pd.notna(cut_q) else -0.25
cut_threshold = max(min(cut_threshold, -0.10), -0.50)  # clamp to reasonable range

df_events["evt_div_cut"] = ((div_l1 > 0) & (pct_change <= cut_threshold)).astype("int8")
df_events["evt_div_suspend"] = ((div_l1 > 0) & (div <= 0)).astype("int8")
df_events["evt_div_initiate"] = ((div_l1 <= 0) & (div > 0)).astype("int8")

# =============================================================================
# 4.2 / liquidity / profitability "moments"
# =============================================================================

# --- Liquidity squeeze ---
act = pd.to_numeric(_as_series(df_events.get("act", np.nan), df_events.index), errors="coerce")
lct = pd.to_numeric(_as_series(df_events.get("lct", np.nan), df_events.index), errors="coerce")
invt = pd.to_numeric(_as_series(df_events.get("invt", 0.0), df_events.index), errors="coerce").fillna(0.0)

cr = ratio(act, lct)
qr = ratio(act - invt, lct)

df_events["evt_liquidity_squeeze"] = (cr.notna() & (cr < 1.0)).astype("int8")
df_events["evt_quick_squeeze"] = (qr.notna() & (qr < 0.8)).astype("int8")

# --- EBITDA and CFO stress ---
oibdp = pd.to_numeric(_as_series(df_events.get("oibdp", np.nan), df_events.index), errors="coerce")
oibdp_l1 = lag(oibdp, 1)
# Define "drop" as a multiplicative deterioration only where that ratio is meaningful.
doibdp = pd.Series(np.nan, index=df_events.index, dtype="float64")
m_eb = (
    oibdp.notna()
    & oibdp_l1.notna()
    & np.isfinite(oibdp)
    & np.isfinite(oibdp_l1)
    & (oibdp_l1 > 0)
    & (oibdp >= 0)
)
doibdp.loc[m_eb] = (oibdp.loc[m_eb] / oibdp_l1.loc[m_eb]).astype("float64")

df_events["doibdp"] = doibdp
doibdp_cal = doibdp.loc[train_mask].replace([np.inf, -np.inf], np.nan).dropna()
doibdp_q = doibdp_cal.quantile(0.05)
doibdp_thr = float(doibdp_q) if pd.notna(doibdp_q) else 0.5
doibdp_thr = float(np.clip(doibdp_thr, 0.10, 0.90))  # guardrails

df_events["evt_ebitda_drop"] = (m_eb & (doibdp < doibdp_thr)).astype("int8")
df_events["evt_ebitda_neg"] = (oibdp.notna() & (oibdp < 0)).astype("int8")

oancf = pd.to_numeric(_as_series(df_events.get("oancf", np.nan), df_events.index), errors="coerce")
oancf_l1 = lag(oancf, 1)
# Define "drop" as a multiplicative deterioration only where that ratio is meaningful.
dcfo = pd.Series(np.nan, index=df_events.index, dtype="float64")
m_cfo = (
    oancf.notna()
    & oancf_l1.notna()
    & np.isfinite(oancf)
    & np.isfinite(oancf_l1)
    & (oancf_l1 > 0)
    & (oancf >= 0)
)
dcfo.loc[m_cfo] = (oancf.loc[m_cfo] / oancf_l1.loc[m_cfo]).astype("float64")

df_events["dcfo"] = dcfo
dcfo_cal = dcfo.loc[train_mask].replace([np.inf, -np.inf], np.nan).dropna()
cfo_drop_q = dcfo_cal.quantile(0.05)
cfo_drop_thr = float(cfo_drop_q) if pd.notna(cfo_drop_q) else 0.5
cfo_drop_thr = float(np.clip(cfo_drop_thr, 0.10, 0.90))  # guardrails

df_events["evt_cfo_drop"] = (m_cfo & (dcfo < cfo_drop_thr)).astype("int8")
df_events["evt_cfo_neg"] = (oancf.notna() & (oancf < 0)).astype("int8")

# =============================================================================
# 4.3 Sanity summary (TRAIN pool only)
# =============================================================================
event_cols = [c for c in df_events.columns if c.startswith("evt_")]
summary_pool = (
    df_events.loc[train_mask, event_cols]
    .mean()
    .sort_values(ascending=False)
    .to_frame("train_event_rate")
    .reset_index()
    .rename(columns={"index": "event"})
)

print("Dividend cut threshold (TRAIN 10th pct):", round(cut_threshold, 3))
print("EBITDA drop threshold (TRAIN 5th pct):", round(doibdp_thr, 3))
print("CFO drop threshold (TRAIN 5th pct):", round(cfo_drop_thr, 3))

display(summary_pool.head(15))

```

    Dividend cut threshold (TRAIN 10th pct): -0.5
    EBITDA drop threshold (TRAIN 5th pct): 0.1
    CFO drop threshold (TRAIN 5th pct): 0.1



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>event</th>
      <th>train_event_rate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>evt_ebitda_neg</td>
      <td>0.374964</td>
    </tr>
    <tr>
      <th>1</th>
      <td>evt_cfo_neg</td>
      <td>0.370465</td>
    </tr>
    <tr>
      <th>2</th>
      <td>evt_quick_squeeze</td>
      <td>0.301147</td>
    </tr>
    <tr>
      <th>3</th>
      <td>evt_liquidity_squeeze</td>
      <td>0.283751</td>
    </tr>
    <tr>
      <th>4</th>
      <td>evt_div_initiate</td>
      <td>0.081803</td>
    </tr>
    <tr>
      <th>5</th>
      <td>evt_div_cut</td>
      <td>0.057638</td>
    </tr>
    <tr>
      <th>6</th>
      <td>evt_cfo_drop</td>
      <td>0.043481</td>
    </tr>
    <tr>
      <th>7</th>
      <td>evt_ebitda_drop</td>
      <td>0.039106</td>
    </tr>
    <tr>
      <th>8</th>
      <td>evt_div_suspend</td>
      <td>0.018676</td>
    </tr>
  </tbody>
</table>
</div>



```python

# =============================================================================
# Train / Validation / Test Split (out-of-time)
#   - Split by label_year to respect the t → t+1 prediction structure
#   - Keep the last label year(s) inside the training pool as validation
# =============================================================================
# Use the observed-label sample for modeling to avoid silently conditioning on future existence
df_split = df_events[df_events["has_next_year_obs"] == 1].copy().reset_index(drop=True)

train_pool = df_split[df_split["label_year"] <= TRAIN_CUTOFF_LABEL_YEAR].copy()
test = df_split[df_split["label_year"] > TRAIN_CUTOFF_LABEL_YEAR].copy()

years = np.sort(train_pool["label_year"].dropna().unique())
val_years = years[-VAL_YEARS:] if len(years) else np.array([], dtype=int)

val = train_pool[train_pool["label_year"].isin(val_years)].copy()
train = train_pool[~train_pool["label_year"].isin(val_years)].copy()

print(
    "Split:",
    f"train={len(train):,}",
    f"val={len(val):,}",
    f"test={len(test):,}",
    "| val_years:",
    list(val_years),
)
```

    Split: train=44,783 val=6,415 test=12,404 | val_years: [np.int64(2022)]



```python

# =============================================================================
# Modeling-Ready Preprocessing (fit on TRAIN only)
#   - Handle infinities and remaining NaNs
#   - Winsorize continuous features using TRAIN quantile bounds
#   - Standardize continuous features to z-scores using TRAIN stats
#   - Keep binary (event) features as-is
# =============================================================================
continuous_feats = [
    # "sp_debt_to_capital",   # EXCLUDED: part of distress dummy definition
    # "sp_ffo_to_debt",      # EXCLUDED: part of distress dummy definition
    "sp_cfo_to_debt",
    "sp_focf_to_debt",
    "sp_dcf_to_debt",
    # "sp_debt_to_ebitda",    # EXCLUDED: part of distress dummy definition
    # "sp_interest_coverage", # EXCLUDED: coverage ratio proxy (potential circularity)
    "log_at",
    "log_mkvalt",
]
# Ensure they exist in all splits
continuous_feats = [c for c in continuous_feats if c in train.columns and c in val.columns and c in test.columns]

event_feats = []
if "event_cols" in globals():
    # EXCLUDE events derived from the distress-definition variables to avoid endogeneity
    excluded_event_stems = ["evt_cov_breach", "evt_cov_collapse", "evt_lev_spike"]
    event_feats = [
        c for c in event_cols 
        if c in train.columns and c in val.columns and c in test.columns
        and not any(s in c for s in excluded_event_stems)
    ]

# Total features to be used
all_feats = continuous_feats + event_feats

# Replace +/-inf with NaN
for d in (train, val, test):
    d[all_feats] = d[all_feats].replace([np.inf, -np.inf], np.nan)

# Impute NaNs (train-only medians)
fill = train[all_feats].median(numeric_only=True)
for d in (train, val, test):
    d[all_feats] = d[all_feats].fillna(fill)

# Winsorize ONLY continuous features
bounds = {}
for c in continuous_feats:
    s = pd.to_numeric(train[c], errors="coerce")
    bounds[c] = (s.quantile(WINSOR_LOWER_Q), s.quantile(WINSOR_UPPER_Q))

for d in (train, val, test):
    for c, (lo, hi) in bounds.items():
        s = pd.to_numeric(d[c], errors="coerce")
        d[c] = s.clip(lo, hi)

# Standardize (z-scores) ONLY continuous features
scaler = StandardScaler().fit(train[continuous_feats].to_numpy(dtype=float))

# Map to new columns
z_cols_cont = [f"z_{c}" for c in continuous_feats]
train[z_cols_cont] = scaler.transform(train[continuous_feats].to_numpy(dtype=float))
val[z_cols_cont] = scaler.transform(val[continuous_feats].to_numpy(dtype=float))
test[z_cols_cont] = scaler.transform(test[continuous_feats].to_numpy(dtype=float))

# Final model features: scaled continuous + raw binary events
MODEL_FEATS = z_cols_cont + event_feats
# Update z_cols for compatibility with later cells
z_cols = MODEL_FEATS
```


```python

# =============================================================================
# Diagnostics & Monitoring Proxies
#   - Correlation screen (TRAIN) for rough signal strength and sanity checks
#   - Expanding-window time folds for temporal stability checks
#   - Dataset overview (rows/firms/years/target rate) + target rate by year
#   - Distribution summaries, collinearity scan, and simple drift proxy (SMD) Train→Test
# =============================================================================
t = "target_next_year_distress"

feats = [c for c in (all_feats if "all_feats" in globals() else z_cols) if c in train.columns and c in test.columns]

corr = (
    train[[t] + feats]
    .corr(numeric_only=True)[t]
    .drop(t)
    .sort_values(key=np.abs, ascending=False)
)
print("Correlation with target:")
print(corr)


# Multicollinearity: Variance Inflation Factor (VIF)
def calculate_vif(df, features):
    vif_data = pd.DataFrame()
    vif_data["feature"] = features
    vif_data["VIF"] = [variance_inflation_factor(df[features].values, i) for i in range(len(features))]
    return vif_data.sort_values("VIF", ascending=False)


vif_df = calculate_vif(train, z_cols)
print("\n=== Multicollinearity Diagnostic (VIF) ===")
print(vif_df)

folds = rolling_year_folds(train_pool, n_splits=N_SPLITS_TIME_CV, min_train_years=3)
for i, (tr_idx, va_idx, tr_years, va_year) in enumerate(folds, 1):
    print(
        f"Fold {i}: train_years={tr_years[0]}..{tr_years[-1]} (n={len(tr_idx)}), "
        f"val_year={va_year} (n={len(va_idx)})"
    )


def _overview(d: pd.DataFrame, name: str) -> None:
    n_rows = len(d)
    n_firms = d["firm_id"].nunique() if "firm_id" in d.columns else np.nan
    n_years = d["fyear"].nunique() if "fyear" in d.columns else np.nan
    target_rate = float(d[t].mean()) if t in d.columns else np.nan

    print(f"\n=== {name} === rows={n_rows:,} | firms={n_firms:,} | years={n_years} | target_rate={target_rate:.4f}")

    if "label_year" in d.columns:
        by_year = d.groupby("label_year")[t].agg(["mean", "count"])
        print("\nTarget by label_year (tail):")
        print(by_year.tail(12))


_overview(train, "TRAIN")
_overview(val, "VAL")
_overview(test, "TEST")

post_miss = pd.DataFrame({"col": raw})
post_miss["train_pct_na"] = [train[c].isna().mean() * 100 if c in train.columns else np.nan for c in raw]
post_miss["val_pct_na"] = [val[c].isna().mean() * 100 if c in val.columns else np.nan for c in raw]
post_miss["test_pct_na"] = [test[c].isna().mean() * 100 if c in test.columns else np.nan for c in raw]
if not post_miss.empty:
    post_miss = post_miss.sort_values("train_pct_na", ascending=False)
    print("\nPost-imputation missingness on raw inputs (pct):")
    print(post_miss.head(50).round(4))


def _dist(d: pd.DataFrame, cols: list[str], name: str) -> pd.DataFrame:
    x = d[cols].replace([np.inf, -np.inf], np.nan)
    q = x.quantile([0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99]).T
    out = pd.DataFrame(
        {
            "n": x.notna().sum(),
            "mean": x.mean(),
            "std": x.std(ddof=0),
            "min": x.min(),
            "p01": q[0.01],
            "p05": q[0.05],
            "p25": q[0.25],
            "p50": q[0.50],
            "p75": q[0.75],
            "p95": q[0.95],
            "p99": q[0.99],
            "max": x.max(),
            "skew": x.skew(numeric_only=True),
            "kurt": x.kurtosis(numeric_only=True),
        }
    )
    print(f"\nDistribution summary ({name})")
    print(out.round(4).sort_values("skew", key=lambda s: s.abs(), ascending=False))
    return out


_ = _dist(train, feats, "TRAIN | winsorized raw feats")
_ = _dist(train, z_cols, "TRAIN | standardized feats")


def _hi_corr(d: pd.DataFrame, cols: list[str], thr: float = 0.80) -> list[tuple[str, str, float]]:
    cm = d[cols].corr(numeric_only=True)
    pairs = []
    for i in range(len(cols)):
        for j in range(i + 1, len(cols)):
            r = cm.iloc[i, j]
            if np.isfinite(r) and abs(r) >= thr:
                pairs.append((cols[i], cols[j], float(r)))
    return sorted(pairs, key=lambda x: abs(x[2]), reverse=True)


pairs = _hi_corr(train, feats, thr=0.80)
print("\nHigh collinearity pairs among feats (|corr|>=0.80) [top 25]:")
for a, b, r in pairs[:25]:
    print(f"{a} vs {b}: r={r:.3f}")


def _drift_smd(a_df: pd.DataFrame, b_df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:
    rows = []
    for c in cols:
        a = pd.to_numeric(a_df[c], errors="coerce").replace([np.inf, -np.inf], np.nan)
        b = pd.to_numeric(b_df[c], errors="coerce").replace([np.inf, -np.inf], np.nan)

        ma, mb = float(a.mean()), float(b.mean())
        sa, sb = float(a.std(ddof=0)), float(b.std(ddof=0))
        sp = np.sqrt(0.5 * (sa ** 2 + sb ** 2))
        smd = (mb - ma) / sp if sp > 0 else np.nan

        rows.append((c, ma, mb, smd, abs(smd) if np.isfinite(smd) else np.nan))

        out = pd.DataFrame(rows, columns=["feature", "mean_train", "mean_test", "smd", "abs_smd"])
    return out.sort_values("abs_smd", ascending=False)


drift = _drift_smd(train, test, feats)
print("\nTrain→Test drift (SMD) [top 15]:")
print(drift.head(15).round(4))


def _group_diff(d: pd.DataFrame, cols: list[str]) -> pd.Series:
    g = d.groupby(t)[cols].mean(numeric_only=True)
    if 0 in g.index and 1 in g.index:
        return (g.loc[1] - g.loc[0]).sort_values(key=np.abs, ascending=False)
    return pd.Series(dtype="float64")


diff = _group_diff(train, feats)
if not diff.empty:
    print("\nMean difference (target=1 minus target=0) on TRAIN feats [top 15]:")
    print(diff.head(15).round(4))

```

    Correlation with target:
    evt_liquidity_squeeze    0.263844
    evt_quick_squeeze        0.247242
    log_at                  -0.170695
    evt_cfo_neg              0.167042
    evt_ebitda_neg           0.164774
    log_mkvalt              -0.118970
    sp_cfo_to_debt          -0.075422
    sp_focf_to_debt         -0.041535
    evt_ebitda_drop         -0.032592
    evt_div_initiate        -0.031913
    evt_cfo_drop            -0.020665
    sp_dcf_to_debt          -0.018879
    evt_div_cut             -0.003884
    evt_div_suspend          0.001658
    Name: target_next_year_distress, dtype: float64
    
    === Multicollinearity Diagnostic (VIF) ===
                      feature        VIF
    1       z_sp_focf_to_debt  10.773518
    2        z_sp_dcf_to_debt   6.454998
    11         evt_ebitda_neg   4.110074
    13            evt_cfo_neg   3.959859
    8   evt_liquidity_squeeze   3.782816
    9       evt_quick_squeeze   3.754028
    0        z_sp_cfo_to_debt   3.110708
    5             evt_div_cut   1.529371
    6         evt_div_suspend   1.466063
    3                z_log_at   1.335437
    4            z_log_mkvalt   1.268416
    12           evt_cfo_drop   1.054331
    10        evt_ebitda_drop   1.050861
    7        evt_div_initiate   1.042988
    Fold 1: train_years=2015..2017 (n=19775), val_year=2018 (n=6337)
    Fold 2: train_years=2015..2018 (n=26112), val_year=2019 (n=6173)
    Fold 3: train_years=2015..2019 (n=32285), val_year=2020 (n=6233)
    Fold 4: train_years=2015..2020 (n=38518), val_year=2021 (n=6265)
    
    === TRAIN === rows=44,783 | firms=9,220 | years=7 | target_rate=0.2527
    
    Target by label_year (tail):
                    mean  count
    label_year                 
    2015        0.261036   6773
    2016        0.249619   6570
    2017        0.237718   6432
    2018        0.240019   6337
    2019        0.276365   6173
    2020        0.277715   6233
    2021        0.226656   6265
    
    === VAL === rows=6,415 | firms=6,415 | years=1 | target_rate=0.2469
    
    Target by label_year (tail):
                    mean  count
    label_year                 
    2022        0.246921   6415
    
    === TEST === rows=12,404 | firms=6,633 | years=2 | target_rate=0.2627
    
    Target by label_year (tail):
                    mean  count
    label_year                 
    2023        0.266635   6327
    2024        0.258516   6077
    
    Post-imputation missingness on raw inputs (pct):
             col  train_pct_na  val_pct_na  test_pct_na
    48    ivstch       23.4218     30.1949      30.3773
    71      sppe       16.6559     16.3055      16.1641
    78    txditc       12.4668     11.9719      12.1251
    79       txp       10.9283     10.4910      10.7546
    60    prcc_f        8.6506      6.0483       7.1993
    59    prcc_c        8.6417      6.0171       7.1832
    61    prstkc        4.9997      4.4895       3.3538
    72     sppiv        4.9997      5.5495       5.9174
    46      ivch        3.4723      3.3359       3.5069
    6        aqc        3.3227      4.6921       4.5953
    45      ivao        3.2646      3.4451       3.7730
    19     dltis        2.4585      2.3071       1.9832
    24       dpc        2.4250      2.2136       2.1928
    20      dltr        2.0276      1.2627       1.6608
    73      sstk        1.8869      1.9174       2.0477
    25        dv        1.5899      1.4030       1.3463
    70       spi        1.4112      1.6680       1.8381
    28       dvt        0.8374      0.5924       0.4837
    26       dvc        0.8329      0.5924       0.4837
    9       capx        0.7079      0.4365       0.5160
    83     xidoc        0.6967      0.4677       0.4434
    3     aoloch        0.6677      0.4677       0.4353
    33      fopo        0.6319      0.4209       0.3950
    30      exre        0.5940      0.3118       0.2902
    31      fiao        0.4868      0.3118       0.2660
    43     ivaco        0.4712      0.2962       0.2660
    15     cstke        0.0089      0.0000       0.0000
    82      xido        0.0067      0.0000       0.0000
    22        do        0.0022      0.0000       0.0000
    81        xi        0.0022      0.0000       0.0000
    23        dp        0.0000      0.0000       0.0000
    14      cstk        0.0000      0.0000       0.0000
    16  datadate        0.0000      0.0000       0.0000
    10       ceq        0.0000      0.0000       0.0000
    11       che        0.0000      0.0000       0.0000
    2         ao        0.0000      0.0000       0.0000
    1        act        0.0000      0.0000       0.0000
    4         ap        0.0000      0.0000       0.0000
    5     apalch        0.0000      0.0000       0.0000
    7         at        0.0000      0.0000       0.0000
    8       caps        0.0000      0.0000       0.0000
    0        aco        0.0000      0.0000       0.0000
    21      dltt        0.0000      0.0000       0.0000
    18     dlcch        0.0000      0.0000       0.0000
    17       dlc        0.0000      0.0000       0.0000
    12     chech        0.0000      0.0000       0.0000
    13      csho        0.0000      0.0000       0.0000
    38       ibc        0.0000      0.0000       0.0000
    37     ibadj        0.0000      0.0000       0.0000
    40     invch        0.0000      0.0000       0.0000
    
    Distribution summary (TRAIN | winsorized raw feats)
                               n     mean       std        min        p01  \
    evt_div_suspend        44783   0.0183    0.1339     0.0000     0.0000   
    sp_dcf_to_debt         44783 -26.1977  206.7528 -1724.5155 -1724.0928   
    sp_focf_to_debt        44783 -13.6823  181.8769 -1436.7039 -1435.7623   
    evt_ebitda_drop        44783   0.0402    0.1964     0.0000     0.0000   
    evt_cfo_drop           44783   0.0443    0.2059     0.0000     0.0000   
    evt_div_cut            44783   0.0579    0.2336     0.0000     0.0000   
    evt_div_initiate       44783   0.0837    0.2769     0.0000     0.0000   
    evt_liquidity_squeeze  44783   0.2754    0.4467     0.0000     0.0000   
    evt_quick_squeeze      44783   0.2932    0.4552     0.0000     0.0000   
    log_mkvalt             44783   7.0526    4.0063     0.4260     0.4260   
    log_at                 44783  11.0730    3.5650     0.6931     0.6931   
    evt_cfo_neg            44783   0.3598    0.4799     0.0000     0.0000   
    evt_ebitda_neg         44783   0.3633    0.4810     0.0000     0.0000   
    sp_cfo_to_debt         44783   4.8503  158.0296  -954.5940  -954.3091   
    
                               p05     p25      p50      p75      p95       p99  \
    evt_div_suspend         0.0000  0.0000   0.0000   0.0000   0.0000    1.0000   
    sp_dcf_to_debt        -34.0200 -0.2690  -0.0162   0.0922   3.1968  357.6861   
    sp_focf_to_debt       -20.7545 -0.1556   0.0470   0.1927   7.2503  613.8415   
    evt_ebitda_drop         0.0000  0.0000   0.0000   0.0000   0.0000    1.0000   
    evt_cfo_drop            0.0000  0.0000   0.0000   0.0000   0.0000    1.0000   
    evt_div_cut             0.0000  0.0000   0.0000   0.0000   1.0000    1.0000   
    evt_div_initiate        0.0000  0.0000   0.0000   0.0000   1.0000    1.0000   
    evt_liquidity_squeeze   0.0000  0.0000   0.0000   1.0000   1.0000    1.0000   
    evt_quick_squeeze       0.0000  0.0000   0.0000   1.0000   1.0000    1.0000   
    log_mkvalt              1.5667  4.1728   6.3628   9.0370  15.6682   16.4558   
    log_at                  4.3694  8.8541  11.4487  13.7914  16.0542   17.8535   
    evt_cfo_neg             0.0000  0.0000   0.0000   1.0000   1.0000    1.0000   
    evt_ebitda_neg          0.0000  0.0000   0.0000   1.0000   1.0000    1.0000   
    sp_cfo_to_debt         -7.6060  0.0001   0.1333   0.3443  17.9883  944.6857   
    
                                max    skew     kurt  
    evt_div_suspend          1.0000  7.1951  49.7712  
    sp_dcf_to_debt         357.7370 -6.8666  51.2077  
    sp_focf_to_debt        614.0150 -5.3263  42.4355  
    evt_ebitda_drop          1.0000  4.6837  19.9375  
    evt_cfo_drop             1.0000  4.4269  17.5978  
    evt_div_cut              1.0000  3.7842  12.3204  
    evt_div_initiate         1.0000  3.0067   7.0408  
    evt_liquidity_squeeze    1.0000  1.0055  -0.9890  
    evt_quick_squeeze        1.0000  0.9085  -1.1747  
    log_mkvalt              16.4558  0.7284  -0.1123  
    log_at                  17.8537 -0.6152   0.1416  
    evt_cfo_neg              1.0000  0.5844  -1.6586  
    evt_ebitda_neg           1.0000  0.5684  -1.6769  
    sp_cfo_to_debt         944.8613  0.2324  27.9858  
    
    Distribution summary (TRAIN | standardized feats)
                               n    mean     std     min     p01     p05     p25  \
    evt_div_suspend        44783  0.0183  0.1339  0.0000  0.0000  0.0000  0.0000   
    z_sp_dcf_to_debt       44783  0.0000  1.0000 -8.2142 -8.2122 -0.0378  0.1254   
    z_sp_focf_to_debt      44783 -0.0000  1.0000 -7.8241 -7.8189 -0.0389  0.0744   
    evt_ebitda_drop        44783  0.0402  0.1964  0.0000  0.0000  0.0000  0.0000   
    evt_cfo_drop           44783  0.0443  0.2059  0.0000  0.0000  0.0000  0.0000   
    evt_div_cut            44783  0.0579  0.2336  0.0000  0.0000  0.0000  0.0000   
    evt_div_initiate       44783  0.0837  0.2769  0.0000  0.0000  0.0000  0.0000   
    evt_liquidity_squeeze  44783  0.2754  0.4467  0.0000  0.0000  0.0000  0.0000   
    evt_quick_squeeze      44783  0.2932  0.4552  0.0000  0.0000  0.0000  0.0000   
    z_log_mkvalt           44783  0.0000  1.0000 -1.6541 -1.6540 -1.3693 -0.7188   
    z_log_at               44783  0.0000  1.0000 -2.9116 -2.9116 -1.8804 -0.6224   
    evt_cfo_neg            44783  0.3598  0.4799  0.0000  0.0000  0.0000  0.0000   
    evt_ebitda_neg         44783  0.3633  0.4810  0.0000  0.0000  0.0000  0.0000   
    z_sp_cfo_to_debt       44783  0.0000  1.0000 -6.0713 -6.0695 -0.0788 -0.0307   
    
                              p50     p75     p95     p99     max    skew     kurt  
    evt_div_suspend        0.0000  0.0000  0.0000  1.0000  1.0000  7.1951  49.7712  
    z_sp_dcf_to_debt       0.1266  0.1272  0.1422  1.8567  1.8570 -6.8666  51.2077  
    z_sp_focf_to_debt      0.0755  0.0763  0.1151  3.4503  3.4512 -5.3263  42.4355  
    evt_ebitda_drop        0.0000  0.0000  0.0000  1.0000  1.0000  4.6837  19.9375  
    evt_cfo_drop           0.0000  0.0000  0.0000  1.0000  1.0000  4.4269  17.5978  
    evt_div_cut            0.0000  0.0000  1.0000  1.0000  1.0000  3.7842  12.3204  
    evt_div_initiate       0.0000  0.0000  1.0000  1.0000  1.0000  3.0067   7.0408  
    evt_liquidity_squeeze  0.0000  1.0000  1.0000  1.0000  1.0000  1.0055  -0.9890  
    evt_quick_squeeze      0.0000  1.0000  1.0000  1.0000  1.0000  0.9085  -1.1747  
    z_log_mkvalt          -0.1722  0.4953  2.1505  2.3471  2.3471  0.7284  -0.1123  
    z_log_at               0.1054  0.7625  1.3973  1.9020  1.9020 -0.6152   0.1416  
    evt_cfo_neg            0.0000  1.0000  1.0000  1.0000  1.0000  0.5844  -1.6586  
    evt_ebitda_neg         0.0000  1.0000  1.0000  1.0000  1.0000  0.5684  -1.6769  
    z_sp_cfo_to_debt      -0.0298 -0.0285  0.0831  5.9472  5.9483  0.2324  27.9858  
    
    High collinearity pairs among feats (|corr|>=0.80) [top 25]:
    sp_focf_to_debt vs sp_dcf_to_debt: r=0.899
    
    Train→Test drift (SMD) [top 15]:
                      feature  mean_train  mean_test     smd  abs_smd
    7        evt_div_initiate      0.0837     0.0322 -0.2215   0.2215
    0          sp_cfo_to_debt      4.8503    -8.2457 -0.0923   0.0923
    3                  log_at     11.0730    11.3644  0.0836   0.0836
    13            evt_cfo_neg      0.3598     0.3968  0.0764   0.0764
    11         evt_ebitda_neg      0.3633     0.3874  0.0497   0.0497
    8   evt_liquidity_squeeze      0.2754     0.2583 -0.0387   0.0387
    4              log_mkvalt      7.0526     7.2055  0.0377   0.0377
    10        evt_ebitda_drop      0.0402     0.0471  0.0338   0.0338
    9       evt_quick_squeeze      0.2932     0.2793 -0.0309   0.0309
    1         sp_focf_to_debt    -13.6823   -18.3124 -0.0271   0.0271
    5             evt_div_cut      0.0579     0.0536 -0.0189   0.0189
    12           evt_cfo_drop      0.0443     0.0460  0.0077   0.0077
    2          sp_dcf_to_debt    -26.1977   -24.7176  0.0076   0.0076
    6         evt_div_suspend      0.0183     0.0184  0.0009   0.0009
    
    Mean difference (target=1 minus target=0) on TRAIN feats [top 15]:
    sp_cfo_to_debt          -27.4287
    sp_focf_to_debt         -17.3843
    sp_dcf_to_debt           -8.9825
    log_at                   -1.4004
    log_mkvalt               -1.0969
    evt_liquidity_squeeze     0.2712
    evt_quick_squeeze         0.2590
    evt_cfo_neg               0.1845
    evt_ebitda_neg            0.1824
    evt_div_initiate         -0.0203
    evt_ebitda_drop          -0.0147
    evt_cfo_drop             -0.0098
    evt_div_cut              -0.0021
    evt_div_suspend           0.0005
    dtype: float64



```python
# =============================================================================
# Visual EDA: Feature distributions by distress flag
#   - Quick separation check: do distressed vs non-distressed firms differ in levels?
#   - Uses a horizontal boxplot for comparability across features
# =============================================================================

import matplotlib.pyplot as plt
import seaborn as sns

# Objective: visualize feature distribution differences for distressed vs non-distressed observations.

plot_df = train_pool.copy() if "train_pool" in globals() else df.copy()

flag_col = "distress_dummy" if "distress_dummy" in plot_df.columns else (
    "target_next_year_distress" if "target_next_year_distress" in plot_df.columns else None
)
if flag_col is None:
    raise KeyError("No distress flag found. Expected 'distress_dummy' or 'target_next_year_distress' in the data.")

plot_feats = [c for c in (feats if "feats" in globals() else []) if c in plot_df.columns]
if not plot_feats:
    plot_feats = [c for c in (z_cols if "z_cols" in globals() else []) if c in plot_df.columns]
if not plot_feats:
    raise KeyError(
        "No feature columns found to plot. Expected 'feats' or 'z_cols' to exist and be present in the data.")

tmp = plot_df[[flag_col] + plot_feats].copy()
tmp[flag_col] = pd.to_numeric(tmp[flag_col], errors="coerce").astype("Int64")
tmp = tmp[tmp[flag_col].isin([0, 1])].copy()

long = tmp.melt(id_vars=[flag_col], value_vars=plot_feats, var_name="feature", value_name="value")
long["value"] = pd.to_numeric(long["value"], errors="coerce")
long = long.dropna(subset=["value"])

fig, ax = plt.subplots(figsize=(12, max(4.5, 0.45 * len(plot_feats))))
sns.boxplot(
    data=long,
    x="value",
    y="feature",
    hue=flag_col,
    orient="h",
    showfliers=False,
    ax=ax,
)
ax.set_title(f"Feature distributions by {flag_col} (0=No distress, 1=Distress)")
ax.set_xlabel("Feature value")
ax.set_ylabel("")
ax.legend(title=flag_col, loc="best")
plt.tight_layout()
plt.show()
```


    
![png](05_01_26_v1_files/05_01_26_v1_7_0.png)
    



```python
# =============================================================================
# 6.1 Setup: Features, target, and train/val/test matrices
# =============================================================================
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    roc_auc_score,
    average_precision_score,
    brier_score_loss,
    confusion_matrix,
    classification_report,
    precision_recall_curve,
    roc_curve,
)
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Prefer standardized feature columns created earlier (fit on TRAIN only)
if "z_cols" in globals():
    MODEL_FEATS = list(z_cols)
else:
    MODEL_FEATS = [f"z_{c}" for c in feats]  # fallback

# Basic sanity checks
for df_name, df_ in [("train", train), ("val", val), ("test", test)]:
    missing_feats = [c for c in MODEL_FEATS if c not in df_.columns]
    if missing_feats:
        raise KeyError(f"{df_name}: missing feature columns: {missing_feats}")

X_train = train[MODEL_FEATS].to_numpy(dtype=float)
y_train = train[TARGET_COL].astype(int).to_numpy()

X_val = val[MODEL_FEATS].to_numpy(dtype=float)
y_val = val[TARGET_COL].astype(int).to_numpy()

X_test = test[MODEL_FEATS].to_numpy(dtype=float)
y_test = test[TARGET_COL].astype(int).to_numpy()

# Defensive: ensure model inputs are finite
def _assert_finite(name, X):
    bad = ~np.isfinite(X)
    if bad.any():
        rows, cols = np.where(bad)
        raise ValueError(f"{name}: found non-finite values at {len(rows)} cells (e.g., row={rows[0]}, col={MODEL_FEATS[cols[0]]}).")

_assert_finite("X_train", X_train)
_assert_finite("X_val", X_val)
_assert_finite("X_test", X_test)

print("Modeling matrix shapes:")
print(f"  X_train: {X_train.shape} | y_train mean={y_train.mean():.4f}")
print(f"  X_val:   {X_val.shape} | y_val mean={y_val.mean():.4f}")
print(f"  X_test:  {X_test.shape} | y_test mean={y_test.mean():.4f}")
```

    Modeling matrix shapes:
      X_train: (44783, 14) | y_train mean=0.2527
      X_val:   (6415, 14) | y_val mean=0.2469
      X_test:  (12404, 14) | y_test mean=0.2627



```python
### 6.2  Cost-sensitive boosted trees (non-linear) + calibrated PDs

import numpy as np
import pandas as pd
import xgboost as xgb

from xgboost import XGBClassifier
from sklearn.metrics import (
    roc_auc_score, average_precision_score, brier_score_loss,
    confusion_matrix
)
from sklearn.model_selection import train_test_split
from sklearn.isotonic import IsotonicRegression
from sklearn.tree import DecisionTreeRegressor, export_text


# =============================================================================
# 0) HELPER UTILITIES
# =============================================================================
def _safe_feature_names(n_features: int):
    cand = globals().get("FEATURE_COLS", globals().get("MODEL_FEATS", None))
    if isinstance(cand, (list, tuple)) and len(cand) == n_features:
        return list(cand)
    return [f"f{i}" for i in range(n_features)]


def gmean_tpr_tnr(y_true, proba, thr):
    y_hat = (proba >= thr).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_true, y_hat, labels=[0, 1]).ravel()
    tpr = tp / max(tp + fn, 1)
    tnr = tn / max(tn + fp, 1)
    gmean = float(np.sqrt(tpr * tnr))
    return gmean, tpr, tnr, np.array([[tn, fp], [fn, tp]])


def expected_cost(y_true, proba, thr, cost_fn: float, cost_fp: float):
    y_hat = (proba >= thr).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_true, y_hat, labels=[0, 1]).ravel()
    return float(cost_fn * fn + cost_fp * fp), np.array([[tn, fp], [fn, tp]])


def pd_decile_table(y_true, proba, n_bins=10):
    df = pd.DataFrame({"y": np.asarray(y_true).astype(int), "p": np.asarray(proba).astype(float)})
    # qcut can fail with many ties; use rank to stabilize
    r = df["p"].rank(method="first")
    df["bin"] = pd.qcut(r, q=n_bins, labels=False) + 1  # 1..n_bins (1=lowest risk)
    out = (
        df.groupby("bin", as_index=False)
        .agg(n=("y", "size"), realized_rate=("y", "mean"), avg_pd=("p", "mean"),
             min_pd=("p", "min"), max_pd=("p", "max"))
        .sort_values("bin")
    )
    out["lift_vs_base"] = out["realized_rate"] / max(df["y"].mean(), 1e-12)
    return out


# =============================================================================
# 1) SPLIT VALIDATION INTO:
#    - early-stopping set (VAL_ES)
#    - calibration + threshold selection set (VAL_CAL)
#    This prevents using the same data for early stopping AND calibration/thresholding.
# =============================================================================
CAL_SIZE = 0.50  # half of VAL held for calibration + threshold selection

try:
    X_val_es, X_val_cal, y_val_es, y_val_cal = train_test_split(
        X_val, y_val,
        test_size=CAL_SIZE,
        random_state=42,
        stratify=y_val
    )
except Exception:
    # fallback if stratify fails (e.g., too few positives)
    X_val_es, X_val_cal, y_val_es, y_val_cal = train_test_split(
        X_val, y_val,
        test_size=CAL_SIZE,
        random_state=42
    )

# =============================================================================
# 2) COST-SENSITIVE WEIGHTS (weighted cross-entropy spirit: α_FN > α_FP)
#    We implement via sample_weight to avoid double-counting with scale_pos_weight.
# =============================================================================
pos = int(np.sum(y_train))
neg = int(len(y_train) - pos)
imbalance_ratio = neg / max(pos, 1)

# Choose explicit costs (finance logic: FN typically more costly than FP).
# Default: match imbalance ratio -> roughly "balanced" effective loss.
COST_FP = 1.0
COST_FN = float(imbalance_ratio)

w_train = np.where(np.asarray(y_train).astype(int) == 1, COST_FN, COST_FP)
w_val_es = np.where(np.asarray(y_val_es).astype(int) == 1, COST_FN, COST_FP)

# =============================================================================
# 3) XGBOOST SPECIFICATION (strong tabular baseline; modest regularization)
#    Key change: early stopping explicitly on PR-AUC (aucpr), not logloss.
# =============================================================================
base_params = dict(
    objective="binary:logistic",
    booster="gbtree",
    tree_method="hist",
    n_estimators=5000,
    learning_rate=0.02,
    max_depth=4,
    min_child_weight=5,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_lambda=5.0,
    reg_alpha=0.0,
    gamma=0.0,
    max_delta_step=1,
    random_state=42,
    n_jobs=-1,
    # IMPORTANT: do NOT set scale_pos_weight if using sample_weight for costs
    scale_pos_weight=1.0,
)

# Early stop on PR-AUC explicitly (your learning curve peaked in aucpr before logloss)
try:
    es = xgb.callback.EarlyStopping(
        rounds=200,
        metric_name="aucpr",
        data_name="validation_0",
        maximize=True,
        save_best=True
    )
    xgb_clf = XGBClassifier(**base_params, eval_metric=["aucpr", "auc", "logloss"], callbacks=[es])
    xgb_clf.fit(
        X_train, y_train,
        sample_weight=w_train,
        eval_set=[(X_val_es, y_val_es)],
        sample_weight_eval_set=[w_val_es],
        verbose=200
    )
except TypeError:
    # fallback for older API variants
    xgb_clf = XGBClassifier(**base_params, eval_metric=["aucpr", "auc", "logloss"], early_stopping_rounds=200)
    xgb_clf.fit(
        X_train, y_train,
        sample_weight=w_train,
        eval_set=[(X_val_es, y_val_es)],
        sample_weight_eval_set=[w_val_es],
        verbose=200
    )

# =============================================================================
# 4) CALIBRATION (manual isotonic; avoids sklearn cv='prefit' deprecation path)
#    Calibrate ONLY on VAL_CAL (not used for early stopping).
# =============================================================================
raw_cal = xgb_clf.predict_proba(X_val_cal)[:, 1]
raw_test = xgb_clf.predict_proba(X_test)[:, 1]

iso = IsotonicRegression(out_of_bounds="clip")
iso.fit(raw_cal, np.asarray(y_val_cal).astype(int))

val_cal_proba = iso.predict(raw_cal)
test_proba = iso.predict(raw_test)

print("XGBoost (cost-sensitive + early stop on VAL_ES aucpr + isotonic calib on VAL_CAL):")
print(
    f"  VAL_CAL AUC={roc_auc_score(y_val_cal, val_cal_proba):.4f} | "
    f"PR-AUC={average_precision_score(y_val_cal, val_cal_proba):.4f} | "
    f"Brier={brier_score_loss(y_val_cal, val_cal_proba):.4f}"
)
print(
    f"  TEST    AUC={roc_auc_score(y_test, test_proba):.4f} | "
    f"PR-AUC={average_precision_score(y_test, test_proba):.4f} | "
    f"Brier={brier_score_loss(y_test, test_proba):.4f}"
)

# =============================================================================
# 5) THRESHOLDS: (i) cost-based, (ii) G-mean, (iii) capacity (top X%)
#    Thresholds chosen on VAL_CAL; then reported on TEST.
# =============================================================================
grid = np.linspace(0.01, 0.99, 99)

# (i) Cost-based threshold: minimize expected misclassification cost
best_cost = {"thr": 0.5, "cost": np.inf, "cm": None}
for t in grid:
    c, cm = expected_cost(y_val_cal, val_cal_proba, t, cost_fn=COST_FN, cost_fp=COST_FP)
    if c < best_cost["cost"]:
        best_cost.update({"thr": float(t), "cost": float(c), "cm": cm})

# (ii) G-mean threshold (imbalance-robust diagnostic)
best_g = {"thr": 0.5, "gmean": -1, "tpr": None, "tnr": None, "cm": None}
for t in grid:
    g, tpr, tnr, cm = gmean_tpr_tnr(y_val_cal, val_cal_proba, t)
    if g > best_g["gmean"]:
        best_g.update({"thr": float(t), "gmean": g, "tpr": tpr, "tnr": tnr, "cm": cm})

# (iii) Capacity rule: flag top X% by PD (common in finance screening / surveillance)
TOP_PCT = 0.25
thr_top = float(np.quantile(val_cal_proba, 1 - TOP_PCT))


def report_threshold(name, thr):
    g_val, tpr_val, tnr_val, cm_val = gmean_tpr_tnr(y_val_cal, val_cal_proba, thr)
    g_t, tpr_t, tnr_t, cm_t = gmean_tpr_tnr(y_test, test_proba, thr)

    # precision / flag rate
    tn, fp, fn, tp = cm_t.ravel()
    prec = tp / max(tp + fp, 1)
    flag = (tp + fp) / max((tp + fp + tn + fn), 1)

    print(f"\n{name}: threshold t={thr:.3f}")
    print(f"  VAL_CAL  G-mean={g_val:.3f} | TPR={tpr_val:.3f} | TNR={tnr_val:.3f} | CM:\n{cm_val}")
    print(
        f"  TEST     G-mean={g_t:.3f} | TPR={tpr_t:.3f} | TNR={tnr_t:.3f} | Precision={prec:.3f} | FlagRate={flag:.3f} | CM:\n{cm_t}")


report_threshold("Cost-based (min expected cost; COST_FN vs COST_FP)", best_cost["thr"])
report_threshold("G-mean (diagnostic)", best_g["thr"])
report_threshold(f"Top-{int(TOP_PCT * 100)}% capacity rule", thr_top)

# =============================================================================
# 6) FINANCE-STYLE PD SORTS (deciles): monotonicity check + lift
# =============================================================================
print("\nPD Deciles on VAL_CAL (1=lowest risk, 10=highest risk):")
print(pd_decile_table(y_val_cal, val_cal_proba, n_bins=10).to_string(index=False))

print("\nPD Deciles on TEST (1=lowest risk, 10=highest risk):")
print(pd_decile_table(y_test, test_proba, n_bins=10).to_string(index=False))

# =============================================================================
# 7) INTERPRETABILITY: “single-tree approximation” of the calibrated PD surface
#    (practical analogue to merging rules into an interpretable tree)
# =============================================================================
feature_names = _safe_feature_names(X_train.shape[1])

sur = DecisionTreeRegressor(
    max_depth=4,
    min_samples_leaf=max(50, int(0.01 * len(y_train))),
    random_state=42
)

# Fit surrogate on TRAIN predictions (do not use TEST); calibrated PD mapping included
raw_train = xgb_clf.predict_proba(X_train)[:, 1]
train_pd = iso.predict(raw_train)
sur.fit(X_train, train_pd)

print("\nSurrogate tree (depth<=4) approximating calibrated PDs:")
print(export_text(sur, feature_names=feature_names))

# =============================================================================
# 8) FEATURE IMPORTANCE: prefer SHAP; fallback to gain if SHAP unavailable
# =============================================================================
try:
    import shap  # optional

    explainer = shap.TreeExplainer(xgb_clf)
    idx = np.random.RandomState(42).choice(len(X_train), size=min(2000, len(X_train)), replace=False)
    shap_values = explainer.shap_values(X_train[idx])

    imp = np.mean(np.abs(shap_values), axis=0)
    top = np.argsort(-imp)[:20]
    print("\nTop-20 features by mean(|SHAP|):")
    for j in top:
        print(f"{feature_names[j]}: {imp[j]:.6f}")
except Exception:
    booster = xgb_clf.get_booster()
    score = booster.get_score(importance_type="gain")  # {"f0": gain, ...}
    imp = np.zeros(X_train.shape[1], dtype=float)
    for k, v in score.items():
        if isinstance(k, str) and k.startswith("f") and k[1:].isdigit():
            j = int(k[1:])
            if 0 <= j < imp.size:
                imp[j] = float(v)

    top = np.argsort(-imp)[:20]
    print("\nTop-20 features by XGBoost gain (fallback; SHAP unavailable):")
    for j in top:
        print(f"{feature_names[j]}: {imp[j]:.6f}")

```

    [0]	validation_0-aucpr:0.73895	validation_0-auc:0.75924	validation_0-logloss:0.68952
    [200]	validation_0-aucpr:0.79823	validation_0-auc:0.80375	validation_0-logloss:0.54161
    [400]	validation_0-aucpr:0.80232	validation_0-auc:0.80762	validation_0-logloss:0.53435
    [600]	validation_0-aucpr:0.80433	validation_0-auc:0.81019	validation_0-logloss:0.53134
    [800]	validation_0-aucpr:0.80456	validation_0-auc:0.81057	validation_0-logloss:0.53051
    [1000]	validation_0-aucpr:0.80506	validation_0-auc:0.81150	validation_0-logloss:0.52930
    [1200]	validation_0-aucpr:0.80549	validation_0-auc:0.81172	validation_0-logloss:0.52898
    [1400]	validation_0-aucpr:0.80543	validation_0-auc:0.81180	validation_0-logloss:0.52879
    [1600]	validation_0-aucpr:0.80615	validation_0-auc:0.81181	validation_0-logloss:0.52863
    [1800]	validation_0-aucpr:0.80624	validation_0-auc:0.81221	validation_0-logloss:0.52824
    [2000]	validation_0-aucpr:0.80666	validation_0-auc:0.81250	validation_0-logloss:0.52800
    [2200]	validation_0-aucpr:0.80659	validation_0-auc:0.81248	validation_0-logloss:0.52813
    [2400]	validation_0-aucpr:0.80682	validation_0-auc:0.81252	validation_0-logloss:0.52813
    [2510]	validation_0-aucpr:0.80670	validation_0-auc:0.81243	validation_0-logloss:0.52815
    XGBoost (cost-sensitive + early stop on VAL_ES aucpr + isotonic calib on VAL_CAL):
      VAL_CAL AUC=0.8018 | PR-AUC=0.5864 | Brier=0.1400
      TEST    AUC=0.7994 | PR-AUC=0.5819 | Brier=0.1488
    
    Cost-based (min expected cost; COST_FN vs COST_FP): threshold t=0.260
      VAL_CAL  G-mean=0.732 | TPR=0.679 | TNR=0.789 | CM:
    [[1906  510]
     [ 254  538]]
      TEST     G-mean=0.727 | TPR=0.688 | TNR=0.768 | Precision=0.514 | FlagRate=0.352 | CM:
    [[7026 2120]
     [1017 2241]]
    
    G-mean (diagnostic): threshold t=0.210
      VAL_CAL  G-mean=0.732 | TPR=0.681 | TNR=0.788 | CM:
    [[1903  513]
     [ 253  539]]
      TEST     G-mean=0.727 | TPR=0.689 | TNR=0.768 | Precision=0.514 | FlagRate=0.352 | CM:
    [[7020 2126]
     [1013 2245]]
    
    Top-25% capacity rule: threshold t=0.429
      VAL_CAL  G-mean=0.711 | TPR=0.595 | TNR=0.850 | CM:
    [[2053  363]
     [ 321  471]]
      TEST     G-mean=0.706 | TPR=0.600 | TNR=0.831 | Precision=0.558 | FlagRate=0.282 | CM:
    [[7600 1546]
     [1304 1954]]
    
    PD Deciles on VAL_CAL (1=lowest risk, 10=highest risk):
     bin   n  realized_rate   avg_pd   min_pd   max_pd  lift_vs_base
       1 321       0.037383 0.035138 0.000000 0.037931      0.151421
       2 321       0.052960 0.062377 0.037931 0.067901      0.214513
       3 321       0.084112 0.076658 0.067901 0.090164      0.340697
       4 320       0.118750 0.120830 0.090164 0.125899      0.480997
       5 321       0.146417 0.148129 0.125899 0.187500      0.593065
       6 321       0.208723 0.200683 0.187500 0.201107      0.845433
       7 320       0.221875 0.229367 0.201107 0.313953      0.898706
       8 321       0.386293 0.390459 0.313953 0.453488      1.564681
       9 321       0.498442 0.492475 0.453488 0.545455      2.018943
      10 321       0.713396 0.712263 0.545455 1.000000      2.889613
    
    PD Deciles on TEST (1=lowest risk, 10=highest risk):
     bin    n  realized_rate   avg_pd   min_pd   max_pd  lift_vs_base
       1 1241       0.031426 0.033970 0.000000 0.060606      0.119647
       2 1240       0.075000 0.064419 0.060606 0.067901      0.285543
       3 1240       0.075000 0.086331 0.067901 0.119149      0.285543
       4 1241       0.122482 0.122459 0.119149 0.125899      0.466318
       5 1240       0.174194 0.168408 0.125899 0.201107      0.663197
       6 1240       0.239516 0.201107 0.201107 0.201107      0.911896
       7 1241       0.266720 0.257276 0.201107 0.313953      1.015470
       8 1240       0.400806 0.422067 0.313953 0.470000      1.525968
       9 1240       0.531452 0.500757 0.470000 0.545455      2.023366
      10 1241       0.709911 0.724029 0.545455 1.000000      2.702806
    
    Surrogate tree (depth<=4) approximating calibrated PDs:
    |--- z_sp_cfo_to_debt <= -0.03
    |   |--- evt_liquidity_squeeze <= 0.50
    |   |   |--- z_sp_dcf_to_debt <= 0.13
    |   |   |   |--- evt_ebitda_neg <= 0.50
    |   |   |   |   |--- value: [0.15]
    |   |   |   |--- evt_ebitda_neg >  0.50
    |   |   |   |   |--- value: [0.26]
    |   |   |--- z_sp_dcf_to_debt >  0.13
    |   |   |   |--- evt_cfo_drop <= 0.50
    |   |   |   |   |--- value: [0.45]
    |   |   |   |--- evt_cfo_drop >  0.50
    |   |   |   |   |--- value: [0.27]
    |   |--- evt_liquidity_squeeze >  0.50
    |   |   |--- z_log_at <= -0.73
    |   |   |   |--- z_log_mkvalt <= -0.99
    |   |   |   |   |--- value: [0.84]
    |   |   |   |--- z_log_mkvalt >  -0.99
    |   |   |   |   |--- value: [0.69]
    |   |   |--- z_log_at >  -0.73
    |   |   |   |--- z_sp_dcf_to_debt <= 0.12
    |   |   |   |   |--- value: [0.35]
    |   |   |   |--- z_sp_dcf_to_debt >  0.12
    |   |   |   |   |--- value: [0.54]
    |--- z_sp_cfo_to_debt >  -0.03
    |   |--- evt_liquidity_squeeze <= 0.50
    |   |   |--- z_sp_cfo_to_debt <= -0.03
    |   |   |   |--- z_sp_cfo_to_debt <= -0.03
    |   |   |   |   |--- value: [0.08]
    |   |   |   |--- z_sp_cfo_to_debt >  -0.03
    |   |   |   |   |--- value: [0.17]
    |   |   |--- z_sp_cfo_to_debt >  -0.03
    |   |   |   |--- evt_ebitda_neg <= 0.50
    |   |   |   |   |--- value: [0.06]
    |   |   |   |--- evt_ebitda_neg >  0.50
    |   |   |   |   |--- value: [0.16]
    |   |--- evt_liquidity_squeeze >  0.50
    |   |   |--- z_log_at <= -0.90
    |   |   |   |--- evt_cfo_neg <= 0.50
    |   |   |   |   |--- value: [0.40]
    |   |   |   |--- evt_cfo_neg >  0.50
    |   |   |   |   |--- value: [0.59]
    |   |   |--- z_log_at >  -0.90
    |   |   |   |--- z_sp_cfo_to_debt <= -0.03
    |   |   |   |   |--- value: [0.20]
    |   |   |   |--- z_sp_cfo_to_debt >  -0.03
    |   |   |   |   |--- value: [0.14]
    


    /Users/test/anaconda3/envs/py313/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
      from .autonotebook import tqdm as notebook_tqdm


    
    Top-20 features by mean(|SHAP|):
    z_sp_cfo_to_debt: 0.933672
    z_sp_dcf_to_debt: 0.314111
    evt_liquidity_squeeze: 0.292642
    z_log_at: 0.287479
    evt_ebitda_neg: 0.212896
    z_sp_focf_to_debt: 0.192127
    z_log_mkvalt: 0.148419
    evt_quick_squeeze: 0.100125
    evt_cfo_drop: 0.021096
    evt_cfo_neg: 0.015542
    evt_ebitda_drop: 0.011551
    evt_div_initiate: 0.010286
    evt_div_cut: 0.005948
    evt_div_suspend: 0.001670



```python
# =============================================================================
# 6.3 Persistence dominance: correct benchmarks for (A) Surveillance vs (B) Early warning
#   - Task A (Surveillance/Triage): allow current state as predictor; compare vs persistence
#   - Task B (Early Warning): restrict to distress(t)=0; predict entry into distress(t+1)
# =============================================================================
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    roc_auc_score, average_precision_score, brier_score_loss, log_loss
)


# ----------------------------
# Helpers
# ----------------------------
def _prep_df(d: pd.DataFrame, feats: list[str], target: str, require_state: bool = False) -> pd.DataFrame:
    cols = feats + [target]
    if require_state and "distress_dummy" not in cols:
        cols = cols + ["distress_dummy"]
    out = d[cols].copy()

    # Coerce distress_dummy to numeric if present
    if "distress_dummy" in out.columns:
        out["distress_dummy"] = pd.to_numeric(out["distress_dummy"], errors="coerce")

    # Drop missing
    out = out.dropna(subset=[target] + feats)
    if require_state:
        out = out.dropna(subset=["distress_dummy"])

    # Ensure target is int 0/1
    out[target] = out[target].astype(int)
    return out


def _metrics(y_true: np.ndarray, p: np.ndarray) -> dict:
    y_true = np.asarray(y_true, dtype=int)
    p = np.asarray(p, dtype=float)

    # Clip probabilities for stability (also keeps log_loss happy)
    p = np.clip(p, 1e-15, 1 - 1e-15)

    # handle single-class edge cases for AUC/AP
    out = {}
    try:
        out["AUC"] = float(roc_auc_score(y_true, p))
    except ValueError:
        out["AUC"] = np.nan
    try:
        out["AP"] = float(average_precision_score(y_true, p))
    except ValueError:
        out["AP"] = np.nan

    out["Brier"] = float(brier_score_loss(y_true, p))
    out["LogLoss"] = float(log_loss(y_true, p, labels=[0, 1]))
    out["PosRate"] = float(np.mean(y_true)) if len(y_true) else np.nan
    out["N"] = int(len(y_true))
    return out


def _tune_logit(X_tr, y_tr, X_va, y_va, C_grid=(0.01, 0.1, 1.0, 10.0)):
    best = {"C": None, "va_auc": -np.inf, "model": None}
    for C in C_grid:
        clf = LogisticRegression(
            C=C,
            solver="lbfgs",
            max_iter=800,
            class_weight="balanced",
            random_state=42,
        )
        clf.fit(X_tr, y_tr)
        p_va = clf.predict_proba(X_va)[:, 1]
        try:
            auc = roc_auc_score(y_va, p_va)
        except ValueError:
            auc = np.nan
        if np.isfinite(auc) and auc > best["va_auc"]:
            best = {"C": C, "va_auc": auc, "model": clf}
    return best


def _print_transition_matrix(name: str, d: pd.DataFrame, target_col: str):
    tmp = d[["distress_dummy", target_col]].copy()
    tmp["distress_dummy"] = pd.to_numeric(tmp["distress_dummy"], errors="coerce")
    tmp = tmp.dropna(subset=["distress_dummy", target_col])
    if tmp.empty:
        print(f"\n{name}: (transition matrix unavailable — missing distress_dummy/target)")
        return
    tmp["distress_dummy"] = tmp["distress_dummy"].astype(int)
    tmp[target_col] = tmp[target_col].astype(int)

    # Rows: current state (t), Cols: next-year state (t+1)
    mat = pd.crosstab(tmp["distress_dummy"], tmp[target_col], normalize="index")
    counts = pd.crosstab(tmp["distress_dummy"], tmp[target_col])
    print(f"\n{name}: Transition matrix P(state(t+1) | state(t))")
    display(mat)
    print(f"{name}: Counts")
    display(counts)


# ----------------------------
# Preconditions
# ----------------------------
if "TARGET_COL" not in globals():
    TARGET_COL = "target_next_year_distress"
if "MODEL_FEATS" not in globals():
    raise RuntimeError("MODEL_FEATS not found. Run the preprocessing / feature setup cell first.")

# Transition matrix (this frames persistence dominance properly)
_print_transition_matrix("TRAIN", train, TARGET_COL)
_print_transition_matrix("VAL", val, TARGET_COL)
_print_transition_matrix("TEST", test, TARGET_COL)

# ----------------------------
# TASK A: Surveillance / triage
# ----------------------------
# Baseline: persistence (probability = 1 if distressed now else 0)
# Models: (i) features-only, (ii) features + distress_dummy (increment over persistence)

SURV_FEATS = list(MODEL_FEATS) + ["distress_dummy"]

trainA = _prep_df(train, MODEL_FEATS, TARGET_COL, require_state=True)
valA = _prep_df(val, MODEL_FEATS, TARGET_COL, require_state=True)
testA = _prep_df(test, MODEL_FEATS, TARGET_COL, require_state=True)

trainA_s = _prep_df(train, SURV_FEATS, TARGET_COL, require_state=True)
valA_s = _prep_df(val, SURV_FEATS, TARGET_COL, require_state=True)
testA_s = _prep_df(test, SURV_FEATS, TARGET_COL, require_state=True)

# Persistence baseline probs
# IMPORTANT: persistence is a *hard* classifier; using {0,1} as probabilities makes LogLoss explode.
# Use clipped probabilities for scoring-rule comparability (AUC/AP are unaffected).
eps = 1e-3
p_val_persist  = np.clip(valA["distress_dummy"].to_numpy(dtype=float), 0.0, 1.0)
p_test_persist = np.clip(testA["distress_dummy"].to_numpy(dtype=float), 0.0, 1.0)
p_val_persist  = np.clip(eps + (1.0 - 2.0 * eps) * p_val_persist, eps, 1.0 - eps)
p_test_persist = np.clip(eps + (1.0 - 2.0 * eps) * p_test_persist, eps, 1.0 - eps)

# Fit features-only logit (Task A)
Xtr = trainA[MODEL_FEATS].to_numpy(dtype=float);
ytr = trainA[TARGET_COL].to_numpy(dtype=int)
Xva = valA[MODEL_FEATS].to_numpy(dtype=float);
yva = valA[TARGET_COL].to_numpy(dtype=int)
Xte = testA[MODEL_FEATS].to_numpy(dtype=float);
yte = testA[TARGET_COL].to_numpy(dtype=int)

bestA = _tune_logit(Xtr, ytr, Xva, yva)
logitA = bestA["model"]
p_val_A = logitA.predict_proba(Xva)[:, 1]
p_test_A = logitA.predict_proba(Xte)[:, 1]

# Fit state-augmented logit (Task A incremental)
Xtr_s = trainA_s[SURV_FEATS].to_numpy(dtype=float)
Xva_s = valA_s[SURV_FEATS].to_numpy(dtype=float)
Xte_s = testA_s[SURV_FEATS].to_numpy(dtype=float)
ytr_s = trainA_s[TARGET_COL].to_numpy(dtype=int)
yva_s = valA_s[TARGET_COL].to_numpy(dtype=int)
yte_s = testA_s[TARGET_COL].to_numpy(dtype=int)

bestA_s = _tune_logit(Xtr_s, ytr_s, Xva_s, yva_s)
logitA_s = bestA_s["model"]
p_val_A_s = logitA_s.predict_proba(Xva_s)[:, 1]
p_test_A_s = logitA_s.predict_proba(Xte_s)[:, 1]

rows = []
rows.append(("Task A (Surveillance)", "VAL", "Persistence", *_metrics(yva, p_val_persist).values()))
rows.append(("Task A (Surveillance)", "VAL", f"Logit (feats-only) C={bestA['C']}", *_metrics(yva, p_val_A).values()))
rows.append(
    ("Task A (Surveillance)", "VAL", f"Logit (feats+state) C={bestA_s['C']}", *_metrics(yva_s, p_val_A_s).values()))

rows.append(("Task A (Surveillance)", "TEST", "Persistence", *_metrics(yte, p_test_persist).values()))
rows.append(("Task A (Surveillance)", "TEST", f"Logit (feats-only) C={bestA['C']}", *_metrics(yte, p_test_A).values()))
rows.append(
    ("Task A (Surveillance)", "TEST", f"Logit (feats+state) C={bestA_s['C']}", *_metrics(yte_s, p_test_A_s).values()))

# ----------------------------
# TASK B: Early warning (entry into distress)
# ----------------------------
# Restrict to firms not distressed at t; target remains distress(t+1)=1
trainB = _prep_df(train, MODEL_FEATS + ["distress_dummy"], TARGET_COL, require_state=True)
valB = _prep_df(val, MODEL_FEATS + ["distress_dummy"], TARGET_COL, require_state=True)
testB = _prep_df(test, MODEL_FEATS + ["distress_dummy"], TARGET_COL, require_state=True)

trainB = trainB[trainB["distress_dummy"].astype(int) == 0].copy()
valB = valB[valB["distress_dummy"].astype(int) == 0].copy()
testB = testB[testB["distress_dummy"].astype(int) == 0].copy()

# Baselines for Task B
# - Always-0 (prob=0)
# - Constant transition rate from TRAIN (prob = mean(y_train_B))
ytrB = trainB[TARGET_COL].to_numpy(dtype=int)
p_tr_rate = float(np.mean(ytrB)) if len(ytrB) else 0.0


def _eval_taskB(split_name, dB):
    y = dB[TARGET_COL].to_numpy(dtype=int)
    p0 = np.zeros_like(y, dtype=float)
    pconst = np.full_like(y, fill_value=p_tr_rate, dtype=float)

    # Fit features-only logit on Task B
    XtrB = trainB[MODEL_FEATS].to_numpy(dtype=float)
    XvaB = valB[MODEL_FEATS].to_numpy(dtype=float)
    XteB = testB[MODEL_FEATS].to_numpy(dtype=float)

    yvaB = valB[TARGET_COL].to_numpy(dtype=int)
    yteB = testB[TARGET_COL].to_numpy(dtype=int)

    bestB = _tune_logit(XtrB, ytrB, XvaB, yvaB)
    logitB = bestB["model"]

    if split_name == "VAL":
        p = logitB.predict_proba(XvaB)[:, 1]
        return bestB, [
            ("Task B (Early warning)", "VAL", "Always-0", *_metrics(yvaB, p0).values()),
            ("Task B (Early warning)", "VAL", f"Const rate={p_tr_rate:.4f}", *_metrics(yvaB, pconst).values()),
            ("Task B (Early warning)", "VAL", f"Logit (feats-only) C={bestB['C']}", *_metrics(yvaB, p).values()),
        ]
    else:
        p = logitB.predict_proba(XteB)[:, 1]
        return bestB, [
            ("Task B (Early warning)", "TEST", "Always-0", *_metrics(yteB, p0).values()),
            ("Task B (Early warning)", "TEST", f"Const rate={p_tr_rate:.4f}", *_metrics(yteB, pconst).values()),
            ("Task B (Early warning)", "TEST", f"Logit (feats-only) C={bestB['C']}", *_metrics(yteB, p).values()),
        ]


bestB, rowsB_val = _eval_taskB("VAL", valB)
_, rowsB_tst = _eval_taskB("TEST", testB)

rows.extend(rowsB_val)
rows.extend(rowsB_tst)

# ----------------------------
# Present results
# ----------------------------
cols = ["Task", "Split", "Model", "AUC", "AP", "Brier", "LogLoss", "PosRate", "N"]
results = pd.DataFrame(rows, columns=cols)

# make numeric columns numeric (for sorting/formatting)
for c in ["AUC", "AP", "Brier", "LogLoss", "PosRate"]:
    results[c] = pd.to_numeric(results[c], errors="coerce")

display(results)

```

    
    TRAIN: Transition matrix P(state(t+1) | state(t))



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>target_next_year_distress</th>
      <th>0</th>
      <th>1</th>
    </tr>
    <tr>
      <th>distress_dummy</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.897635</td>
      <td>0.102365</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.297584</td>
      <td>0.702416</td>
    </tr>
  </tbody>
</table>
</div>


    TRAIN: Counts



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>target_next_year_distress</th>
      <th>0</th>
      <th>1</th>
    </tr>
    <tr>
      <th>distress_dummy</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>30130</td>
      <td>3436</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3338</td>
      <td>7879</td>
    </tr>
  </tbody>
</table>
</div>


    
    VAL: Transition matrix P(state(t+1) | state(t))



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>target_next_year_distress</th>
      <th>0</th>
      <th>1</th>
    </tr>
    <tr>
      <th>distress_dummy</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.890612</td>
      <td>0.109388</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.282564</td>
      <td>0.717436</td>
    </tr>
  </tbody>
</table>
</div>


    VAL: Counts



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>target_next_year_distress</th>
      <th>0</th>
      <th>1</th>
    </tr>
    <tr>
      <th>distress_dummy</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>4421</td>
      <td>543</td>
    </tr>
    <tr>
      <th>1</th>
      <td>410</td>
      <td>1041</td>
    </tr>
  </tbody>
</table>
</div>


    
    TEST: Transition matrix P(state(t+1) | state(t))



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>target_next_year_distress</th>
      <th>0</th>
      <th>1</th>
    </tr>
    <tr>
      <th>distress_dummy</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.886969</td>
      <td>0.113031</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.285946</td>
      <td>0.714054</td>
    </tr>
  </tbody>
</table>
</div>


    TEST: Counts



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>target_next_year_distress</th>
      <th>0</th>
      <th>1</th>
    </tr>
    <tr>
      <th>distress_dummy</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>8263</td>
      <td>1053</td>
    </tr>
    <tr>
      <th>1</th>
      <td>883</td>
      <td>2205</td>
    </tr>
  </tbody>
</table>
</div>



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Task</th>
      <th>Split</th>
      <th>Model</th>
      <th>AUC</th>
      <th>AP</th>
      <th>Brier</th>
      <th>LogLoss</th>
      <th>PosRate</th>
      <th>N</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Task A (Surveillance)</td>
      <td>VAL</td>
      <td>Persistence</td>
      <td>0.786164</td>
      <td>0.556142</td>
      <td>0.148262</td>
      <td>1.027055</td>
      <td>0.246921</td>
      <td>6415</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Task A (Surveillance)</td>
      <td>VAL</td>
      <td>Logit (feats-only) C=10.0</td>
      <td>0.671981</td>
      <td>0.467880</td>
      <td>0.218035</td>
      <td>0.627410</td>
      <td>0.246921</td>
      <td>6415</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Task A (Surveillance)</td>
      <td>VAL</td>
      <td>Logit (feats+state) C=10.0</td>
      <td>0.832444</td>
      <td>0.675561</td>
      <td>0.138769</td>
      <td>0.457550</td>
      <td>0.246921</td>
      <td>6415</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Task A (Surveillance)</td>
      <td>TEST</td>
      <td>Persistence</td>
      <td>0.790125</td>
      <td>0.568161</td>
      <td>0.155768</td>
      <td>1.078998</td>
      <td>0.262657</td>
      <td>12404</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Task A (Surveillance)</td>
      <td>TEST</td>
      <td>Logit (feats-only) C=10.0</td>
      <td>0.676428</td>
      <td>0.473561</td>
      <td>0.219801</td>
      <td>0.631621</td>
      <td>0.262657</td>
      <td>12404</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Task A (Surveillance)</td>
      <td>TEST</td>
      <td>Logit (feats+state) C=10.0</td>
      <td>0.837228</td>
      <td>0.672849</td>
      <td>0.143325</td>
      <td>0.469873</td>
      <td>0.262657</td>
      <td>12404</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Task B (Early warning)</td>
      <td>VAL</td>
      <td>Always-0</td>
      <td>0.500000</td>
      <td>0.109388</td>
      <td>0.109388</td>
      <td>3.778114</td>
      <td>0.109388</td>
      <td>4964</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Task B (Early warning)</td>
      <td>VAL</td>
      <td>Const rate=0.1024</td>
      <td>0.500000</td>
      <td>0.109388</td>
      <td>0.097471</td>
      <td>0.345496</td>
      <td>0.109388</td>
      <td>4964</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Task B (Early warning)</td>
      <td>VAL</td>
      <td>Logit (feats-only) C=0.01</td>
      <td>0.642032</td>
      <td>0.175351</td>
      <td>0.238687</td>
      <td>0.671201</td>
      <td>0.109388</td>
      <td>4964</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Task B (Early warning)</td>
      <td>TEST</td>
      <td>Always-0</td>
      <td>0.500000</td>
      <td>0.113031</td>
      <td>0.113031</td>
      <td>3.903964</td>
      <td>0.113031</td>
      <td>9316</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Task B (Early warning)</td>
      <td>TEST</td>
      <td>Const rate=0.1024</td>
      <td>0.500000</td>
      <td>0.113031</td>
      <td>0.100369</td>
      <td>0.353407</td>
      <td>0.113031</td>
      <td>9316</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Task B (Early warning)</td>
      <td>TEST</td>
      <td>Logit (feats-only) C=0.01</td>
      <td>0.658226</td>
      <td>0.187677</td>
      <td>0.237710</td>
      <td>0.669462</td>
      <td>0.113031</td>
      <td>9316</td>
    </tr>
  </tbody>
</table>
</div>



```python
# =============================================================================
# 7.1 Baseline distress rate, event lift, and year clustering (distribution-aware)
# =============================================================================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# --- Robust references ---
_df = df_split.copy() if "df_split" in globals() else (df_events.copy() if "df_events" in globals() else df.copy())
_target = TARGET_COL if "TARGET_COL" in globals() else (
    "target_next_year_distress" if "target_next_year_distress" in _df.columns else "distress_dummy"
)
if _target not in _df.columns:
    raise KeyError(f"Target not found. Expected TARGET_COL or 'target_next_year_distress'/'distress_dummy'. Got: {_target}")

_evt_cols = [c for c in _df.columns if c.startswith("evt_")]
if not _evt_cols:
    raise ValueError("No event columns found (expected columns starting with 'evt_').")

# Year for reporting (feature-year)
if "fyear" in _df.columns:
    _year = _df["fyear"]
else:
    # label_year = fyear + 1 (in this project); use feature-year = label_year - 1
    _year = _df["label_year"] - 1 if "label_year" in _df.columns else pd.Series(np.nan, index=_df.index)

base_rate = float(pd.to_numeric(_df[_target], errors="coerce").mean())
print(f"Baseline distress rate (overall): {base_rate:.4%}  |  N={len(_df):,}")

def event_lift_table(df_in: pd.DataFrame, target: str, event_cols: list[str]) -> pd.DataFrame:
    y = pd.to_numeric(df_in[target], errors="coerce")
    base = float(y.mean())
    rows = []
    for e in event_cols:
        s = pd.to_numeric(df_in[e], errors="coerce").fillna(0).astype(int)
        n = int(s.sum())
        prev = float(s.mean())
        cond = float(y[s == 1].mean()) if n > 0 else np.nan
        lift = (cond / base) if (pd.notna(cond) and base > 0) else np.nan
        pp = (cond - base) if pd.notna(cond) else np.nan
        rows.append((e, n, prev, cond, lift, pp))
    out = pd.DataFrame(
        rows,
        columns=["event", "n_events", "event_rate", "distress_rate_given_event", "lift_vs_base", "pp_diff_vs_base"],
    )
    return out.sort_values(["lift_vs_base", "n_events"], ascending=[False, False]).reset_index(drop=True)

lift_all = event_lift_table(_df, _target, _evt_cols)
display(lift_all.head(20))

# --- Year clustering ---
year_tbl = (
    _df.assign(feature_year=_year)
    .groupby("feature_year", dropna=True)[[_target] + _evt_cols]
    .mean(numeric_only=True)
    .sort_index()
)

# Plot: distress rate + a few key event rates (auto-select top by lift)
top_events = list(lift_all["event"].head(6).values)
fig, ax = plt.subplots(figsize=(10, 4))
ax.plot(year_tbl.index, year_tbl[_target], marker="o")
ax.set_title("Distress rate by feature-year")
ax.set_xlabel("Feature year")
ax.set_ylabel("Rate")
ax.grid(True, alpha=0.3)
plt.show()

fig, ax = plt.subplots(figsize=(10, 5))
for e in top_events:
    if e in year_tbl.columns:
        ax.plot(year_tbl.index, year_tbl[e], marker="o", label=e)
ax.set_title("Top event rates by feature-year (top by lift)")
ax.set_xlabel("Feature year")
ax.set_ylabel("Event rate")
ax.grid(True, alpha=0.3)
ax.legend(loc="best")
plt.show()

```

    Baseline distress rate (overall): 25.4033%  |  N=63,602



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>event</th>
      <th>n_events</th>
      <th>event_rate</th>
      <th>distress_rate_given_event</th>
      <th>lift_vs_base</th>
      <th>pp_diff_vs_base</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>evt_liquidity_squeeze</td>
      <td>17043</td>
      <td>0.267963</td>
      <td>0.438186</td>
      <td>1.724917</td>
      <td>0.184153</td>
    </tr>
    <tr>
      <th>1</th>
      <td>evt_quick_squeeze</td>
      <td>18250</td>
      <td>0.286941</td>
      <td>0.416164</td>
      <td>1.638230</td>
      <td>0.162131</td>
    </tr>
    <tr>
      <th>2</th>
      <td>evt_cfo_neg</td>
      <td>23672</td>
      <td>0.372190</td>
      <td>0.352949</td>
      <td>1.389382</td>
      <td>0.098916</td>
    </tr>
    <tr>
      <th>3</th>
      <td>evt_ebitda_neg</td>
      <td>23591</td>
      <td>0.370916</td>
      <td>0.351660</td>
      <td>1.384307</td>
      <td>0.097627</td>
    </tr>
    <tr>
      <th>4</th>
      <td>evt_div_suspend</td>
      <td>1224</td>
      <td>0.019245</td>
      <td>0.269608</td>
      <td>1.061311</td>
      <td>0.015575</td>
    </tr>
    <tr>
      <th>5</th>
      <td>evt_div_cut</td>
      <td>3692</td>
      <td>0.058048</td>
      <td>0.250271</td>
      <td>0.985191</td>
      <td>-0.003762</td>
    </tr>
    <tr>
      <th>6</th>
      <td>evt_div_initiate</td>
      <td>4436</td>
      <td>0.069746</td>
      <td>0.209874</td>
      <td>0.826168</td>
      <td>-0.044159</td>
    </tr>
    <tr>
      <th>7</th>
      <td>evt_cfo_drop</td>
      <td>2835</td>
      <td>0.044574</td>
      <td>0.197178</td>
      <td>0.776191</td>
      <td>-0.056855</td>
    </tr>
    <tr>
      <th>8</th>
      <td>evt_ebitda_drop</td>
      <td>2698</td>
      <td>0.042420</td>
      <td>0.186434</td>
      <td>0.733899</td>
      <td>-0.067598</td>
    </tr>
  </tbody>
</table>
</div>



    
![png](05_01_26_v1_files/05_01_26_v1_11_2.png)
    



    
![png](05_01_26_v1_files/05_01_26_v1_11_3.png)
    



```python
# =============================================================================
# 7.2 Event co-occurrence and transitions (anticipation)
# =============================================================================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

_df = df_split.copy() if "df_split" in globals() else df_events.copy()
_target = TARGET_COL if "TARGET_COL" in globals() else "target_next_year_distress"
_evt_cols = [c for c in _df.columns if c.startswith("evt_")]

# Feature-year for ordering
if "fyear" in _df.columns:
    _df["_feature_year"] = _df["fyear"]
elif "label_year" in _df.columns:
    _df["_feature_year"] = _df["label_year"] - 1
else:
    raise KeyError("Need 'fyear' or 'label_year' to compute event transitions.")

if "firm_id" not in _df.columns:
    raise KeyError("Need 'firm_id' to compute firm-level event transitions.")

E = _df[_evt_cols].fillna(0).astype(int)

# --- Co-occurrence: Jaccard similarity (intersection/union) for binaries ---
A = E.to_numpy(dtype=int)
inter = A.T @ A  # intersection counts
marg = A.sum(axis=0)
union = marg.reshape(-1, 1) + marg.reshape(1, -1) - inter
jacc = np.divide(inter, np.maximum(union, 1), dtype=float)

fig, ax = plt.subplots(figsize=(7, 6))
im = ax.imshow(jacc, aspect="auto")
ax.set_title("Event co-occurrence (Jaccard similarity)")
ax.set_xticks(range(len(_evt_cols)))
ax.set_yticks(range(len(_evt_cols)))
ax.set_xticklabels(_evt_cols, rotation=90, fontsize=7)
ax.set_yticklabels(_evt_cols, fontsize=7)
fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
plt.tight_layout()
plt.show()

# --- Transitions: activation (0→1) and persistence (1→1) ---
tmp = _df[["firm_id", "_feature_year"] + _evt_cols].sort_values(["firm_id", "_feature_year"]).copy()
for e in _evt_cols:
    tmp[e + "_next"] = tmp.groupby("firm_id")[e].shift(-1)

rows = []
for e in _evt_cols:
    cur = tmp[e].fillna(0).astype(int)
    nxt = tmp[e + "_next"]
    m = nxt.notna()
    cur_m = cur[m]
    nxt_m = nxt[m].fillna(0).astype(int)

    # activation: P(next=1 | current=0)
    denom01 = int((cur_m == 0).sum())
    p01 = float((nxt_m[(cur_m == 0)] == 1).mean()) if denom01 > 0 else np.nan

    # persistence: P(next=1 | current=1)
    denom11 = int((cur_m == 1).sum())
    p11 = float((nxt_m[(cur_m == 1)] == 1).mean()) if denom11 > 0 else np.nan

    rows.append((e, denom01, p01, denom11, p11))

trans_tbl = pd.DataFrame(rows, columns=["event", "n_cur0", "p_activate_01", "n_cur1", "p_persist_11"])
trans_tbl = trans_tbl.sort_values("p_activate_01", ascending=False).reset_index(drop=True)
display(trans_tbl.head(20))

```


    
![png](05_01_26_v1_files/05_01_26_v1_12_0.png)
    



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>event</th>
      <th>n_cur0</th>
      <th>p_activate_01</th>
      <th>n_cur1</th>
      <th>p_persist_11</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>evt_quick_squeeze</td>
      <td>38234</td>
      <td>0.165638</td>
      <td>14998</td>
      <td>0.583945</td>
    </tr>
    <tr>
      <th>1</th>
      <td>evt_liquidity_squeeze</td>
      <td>39284</td>
      <td>0.153167</td>
      <td>13948</td>
      <td>0.569472</td>
    </tr>
    <tr>
      <th>2</th>
      <td>evt_cfo_neg</td>
      <td>34024</td>
      <td>0.094727</td>
      <td>19208</td>
      <td>0.821949</td>
    </tr>
    <tr>
      <th>3</th>
      <td>evt_div_cut</td>
      <td>50090</td>
      <td>0.064584</td>
      <td>3142</td>
      <td>0.145449</td>
    </tr>
    <tr>
      <th>4</th>
      <td>evt_ebitda_neg</td>
      <td>34165</td>
      <td>0.059886</td>
      <td>19067</td>
      <td>0.889442</td>
    </tr>
    <tr>
      <th>5</th>
      <td>evt_cfo_drop</td>
      <td>50812</td>
      <td>0.055440</td>
      <td>2420</td>
      <td>0.007438</td>
    </tr>
    <tr>
      <th>6</th>
      <td>evt_ebitda_drop</td>
      <td>50965</td>
      <td>0.052644</td>
      <td>2267</td>
      <td>0.006617</td>
    </tr>
    <tr>
      <th>7</th>
      <td>evt_div_initiate</td>
      <td>49199</td>
      <td>0.027175</td>
      <td>4033</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>8</th>
      <td>evt_div_suspend</td>
      <td>52197</td>
      <td>0.023450</td>
      <td>1035</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div>



```python
# =============================================================================
# 7.3 Tree-based explanations: SHAP focus on event binaries + model-implied ΔPD toggles
# =============================================================================
import numpy as np
import pandas as pd

# Preconditions: trained tree model and calibration (from section 6.2)
if "xgb_clf" not in globals():
    raise RuntimeError("xgb_clf not found. Run the tree-based benchmark cell (6.2) first.")
if "iso" not in globals():
    print("Warning: isotonic calibrator 'iso' not found. Proceeding with raw XGB probabilities.")
if "MODEL_FEATS" not in globals():
    raise RuntimeError("MODEL_FEATS not found. Run the preprocessing / feature setup cells first.")
if "test" not in globals():
    raise RuntimeError("test DataFrame not found.")

_evt_feats = [c for c in MODEL_FEATS if c.startswith("evt_")]
if not _evt_feats:
    raise ValueError("No event binaries found in MODEL_FEATS. Expected event features to be included as raw binaries.")

def _predict_pd(df_rows: pd.DataFrame) -> np.ndarray:
    X = df_rows[MODEL_FEATS].to_numpy(dtype=float)
    raw = xgb_clf.predict_proba(X)[:, 1]
    if "iso" in globals():
        return np.asarray(iso.predict(raw), dtype=float)
    return np.asarray(raw, dtype=float)

# Sample for explanation
N = min(3000, len(test))
sample = test.sample(N, random_state=42).copy()
sample_pd = _predict_pd(sample)

# --- 7.3a Observed conditional PD by event (descriptive) ---
rows = []
for e in _evt_feats:
    s = sample[e].fillna(0).astype(int)
    n1 = int(s.sum())
    if n1 == 0 or n1 == len(sample):
        continue
    pd1 = float(sample_pd[s == 1].mean())
    pd0 = float(sample_pd[s == 0].mean())
    rows.append((e, n1, float(s.mean()), pd0, pd1, pd1 - pd0))

obs_pd_tbl = pd.DataFrame(
    rows, columns=["event", "n_event1", "event_rate_in_sample", "mean_PD_if_0", "mean_PD_if_1", "PD_diff_1_minus_0"]
).sort_values("PD_diff_1_minus_0", ascending=False).reset_index(drop=True)

display(obs_pd_tbl.head(20))

# --- 7.3b Model-implied ΔPD toggles (counterfactual within the model; not causal) ---
def toggle_delta_pd(df_rows: pd.DataFrame, event_col: str, new_value: int) -> np.ndarray:
    tmp = df_rows.copy()
    tmp[event_col] = int(new_value)
    return _predict_pd(tmp)

toggle_rows = []
for e in _evt_feats:
    s = sample[e].fillna(0).astype(int)

    # 0 -> 1 toggle among those currently 0
    m0 = (s == 0)
    if m0.any():
        pd_base0 = sample_pd[m0]
        pd_t1 = toggle_delta_pd(sample.loc[m0, MODEL_FEATS], e, 1)
        d01 = float(np.mean(pd_t1 - pd_base0))
    else:
        d01 = np.nan

    # 1 -> 0 toggle among those currently 1
    m1 = (s == 1)
    if m1.any():
        pd_base1 = sample_pd[m1]
        pd_t0 = toggle_delta_pd(sample.loc[m1, MODEL_FEATS], e, 0)
        d10 = float(np.mean(pd_t0 - pd_base1))
    else:
        d10 = np.nan

    toggle_rows.append((e, float(s.mean()), d01, d10))

toggle_tbl = pd.DataFrame(
    toggle_rows,
    columns=["event", "event_rate_in_sample", "avg_ΔPD_if_toggle_0_to_1", "avg_ΔPD_if_toggle_1_to_0"],
).sort_values("avg_ΔPD_if_toggle_0_to_1", ascending=False).reset_index(drop=True)

display(toggle_tbl.head(25))

# --- Optional: SHAP values (can be slow; uses a smaller sample) ---
try:
    import shap

    N_SHAP = min(1500, len(sample))
    shap_sample = sample.iloc[:N_SHAP].copy()

    explainer = shap.TreeExplainer(xgb_clf)
    shap_values = explainer.shap_values(shap_sample[MODEL_FEATS].to_numpy(dtype=float))
    # shap_values is in log-odds space for binary classifiers (by default)
    shap_df = pd.DataFrame(shap_values, columns=MODEL_FEATS)

    # Summarize absolute SHAP contribution for event features
    evt_shap = (
        shap_df[_evt_feats]
        .abs()
        .mean()
        .sort_values(ascending=False)
        .rename("mean_abs_shap_logit")
        .reset_index()
        .rename(columns={"index": "event"})
    )
    display(evt_shap.head(20))

except Exception as e:
    print("SHAP computation skipped or failed:", repr(e))

```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>event</th>
      <th>n_event1</th>
      <th>event_rate_in_sample</th>
      <th>mean_PD_if_0</th>
      <th>mean_PD_if_1</th>
      <th>PD_diff_1_minus_0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>evt_liquidity_squeeze</td>
      <td>808</td>
      <td>0.269333</td>
      <td>0.203161</td>
      <td>0.445659</td>
      <td>0.242498</td>
    </tr>
    <tr>
      <th>1</th>
      <td>evt_quick_squeeze</td>
      <td>863</td>
      <td>0.287667</td>
      <td>0.205038</td>
      <td>0.425558</td>
      <td>0.220520</td>
    </tr>
    <tr>
      <th>2</th>
      <td>evt_cfo_neg</td>
      <td>1228</td>
      <td>0.409333</td>
      <td>0.208647</td>
      <td>0.354804</td>
      <td>0.146157</td>
    </tr>
    <tr>
      <th>3</th>
      <td>evt_ebitda_neg</td>
      <td>1199</td>
      <td>0.399667</td>
      <td>0.212052</td>
      <td>0.353225</td>
      <td>0.141173</td>
    </tr>
    <tr>
      <th>4</th>
      <td>evt_div_suspend</td>
      <td>49</td>
      <td>0.016333</td>
      <td>0.268139</td>
      <td>0.288640</td>
      <td>0.020500</td>
    </tr>
    <tr>
      <th>5</th>
      <td>evt_div_cut</td>
      <td>164</td>
      <td>0.054667</td>
      <td>0.270284</td>
      <td>0.237180</td>
      <td>-0.033104</td>
    </tr>
    <tr>
      <th>6</th>
      <td>evt_div_initiate</td>
      <td>94</td>
      <td>0.031333</td>
      <td>0.269630</td>
      <td>0.232746</td>
      <td>-0.036884</td>
    </tr>
    <tr>
      <th>7</th>
      <td>evt_cfo_drop</td>
      <td>139</td>
      <td>0.046333</td>
      <td>0.270257</td>
      <td>0.231787</td>
      <td>-0.038470</td>
    </tr>
    <tr>
      <th>8</th>
      <td>evt_ebitda_drop</td>
      <td>144</td>
      <td>0.048000</td>
      <td>0.272151</td>
      <td>0.195550</td>
      <td>-0.076601</td>
    </tr>
  </tbody>
</table>
</div>



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>event</th>
      <th>event_rate_in_sample</th>
      <th>avg_ΔPD_if_toggle_0_to_1</th>
      <th>avg_ΔPD_if_toggle_1_to_0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>evt_liquidity_squeeze</td>
      <td>0.269333</td>
      <td>0.095515</td>
      <td>-0.107840</td>
    </tr>
    <tr>
      <th>1</th>
      <td>evt_ebitda_neg</td>
      <td>0.399667</td>
      <td>0.059376</td>
      <td>-0.085584</td>
    </tr>
    <tr>
      <th>2</th>
      <td>evt_quick_squeeze</td>
      <td>0.287667</td>
      <td>0.033453</td>
      <td>-0.028140</td>
    </tr>
    <tr>
      <th>3</th>
      <td>evt_div_initiate</td>
      <td>0.031333</td>
      <td>0.008527</td>
      <td>-0.006418</td>
    </tr>
    <tr>
      <th>4</th>
      <td>evt_cfo_neg</td>
      <td>0.409333</td>
      <td>0.004291</td>
      <td>-0.009889</td>
    </tr>
    <tr>
      <th>5</th>
      <td>evt_div_cut</td>
      <td>0.054667</td>
      <td>-0.000247</td>
      <td>-0.006690</td>
    </tr>
    <tr>
      <th>6</th>
      <td>evt_div_suspend</td>
      <td>0.016333</td>
      <td>-0.005215</td>
      <td>0.006305</td>
    </tr>
    <tr>
      <th>7</th>
      <td>evt_cfo_drop</td>
      <td>0.046333</td>
      <td>-0.019664</td>
      <td>0.053344</td>
    </tr>
    <tr>
      <th>8</th>
      <td>evt_ebitda_drop</td>
      <td>0.048000</td>
      <td>-0.028011</td>
      <td>0.018527</td>
    </tr>
  </tbody>
</table>
</div>



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>event</th>
      <th>mean_abs_shap_logit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>evt_liquidity_squeeze</td>
      <td>0.293163</td>
    </tr>
    <tr>
      <th>1</th>
      <td>evt_ebitda_neg</td>
      <td>0.203098</td>
    </tr>
    <tr>
      <th>2</th>
      <td>evt_quick_squeeze</td>
      <td>0.099014</td>
    </tr>
    <tr>
      <th>3</th>
      <td>evt_cfo_drop</td>
      <td>0.022012</td>
    </tr>
    <tr>
      <th>4</th>
      <td>evt_cfo_neg</td>
      <td>0.015219</td>
    </tr>
    <tr>
      <th>5</th>
      <td>evt_ebitda_drop</td>
      <td>0.012896</td>
    </tr>
    <tr>
      <th>6</th>
      <td>evt_div_initiate</td>
      <td>0.008106</td>
    </tr>
    <tr>
      <th>7</th>
      <td>evt_div_cut</td>
      <td>0.006006</td>
    </tr>
    <tr>
      <th>8</th>
      <td>evt_div_suspend</td>
      <td>0.001575</td>
    </tr>
  </tbody>
</table>
</div>



```python
# =============================================================================
# 7.4 Accounting-consistent pro forma scenarios (simulation for a single firm-year)
# =============================================================================
import numpy as np
import pandas as pd

if "xgb_clf" not in globals():
    raise RuntimeError("xgb_clf not found. Run the tree-based benchmark cell (6.2) first.")
if "MODEL_FEATS" not in globals():
    raise RuntimeError("MODEL_FEATS not found. Run preprocessing / feature setup cells first.")
if "continuous_feats" not in globals():
    raise RuntimeError("continuous_feats not found. Expected it from preprocessing cell.")
if "event_feats" not in globals():
    # fall back: infer from MODEL_FEATS
    event_feats = [c for c in MODEL_FEATS if c.startswith("evt_")]

# Required objects from preprocessing (winsor bounds + scaler)
if "bounds" not in globals():
    raise RuntimeError("winsorization bounds not found (expected 'bounds' dict from preprocessing cell).")
if "scaler" not in globals():
    raise RuntimeError("scaler not found (expected StandardScaler 'scaler' from preprocessing cell).")

_df = df_split.copy() if "df_split" in globals() else df_events.copy()


# Utility: calibrated PD
def _predict_pd_from_features(df_rows: pd.DataFrame) -> float | np.ndarray:
    proba = xgb_clf.predict_proba(df_rows[MODEL_FEATS])[:, 1]
    return proba


def _recompute_events_one(raw_row: pd.Series, lag_row: pd.Series | None) -> dict:
    """Recompute event flags for a single row using the same logic as the event cell."""
    out = {}

    # Pull thresholds from the earlier event cell if available; otherwise use conservative defaults
    cut_thr = float(globals().get("cut_threshold", -0.25))
    cov_drop_thr = float(globals().get("cov_drop_thr", 0.7))
    dlev_thr = float(globals().get("dlev_thr", 5.0))
    doibdp_thr = globals().get("doibdp_q", np.nan)
    if isinstance(doibdp_thr, (pd.Series, pd.DataFrame)):
        doibdp_thr = float(pd.to_numeric(doibdp_thr, errors="coerce"))
    cfo_drop_thr = float(globals().get("cfo_drop_thr", 0.7))

    def _num(x):
        return pd.to_numeric(x, errors="coerce")

    def _finite(x) -> bool:
        try:
            return np.isfinite(float(x))
        except Exception:
            return False

    def g(name, default=np.nan):
        return _num(raw_row.get(name, default))

    def glag(name, default=np.nan):
        if lag_row is None:
            return _num(default)
        return _num(lag_row.get(name, default))

    # Dividend moments
    dv = g("dv", 0.0)
    dv_l1 = glag("dv", 0.0)
    pct = (float(dv - dv_l1) / float(dv_l1)) if (_finite(dv) and _finite(dv_l1) and float(dv_l1) != 0.0) else np.nan
    out["evt_div_cut"] = int((_finite(dv_l1) and float(dv_l1) > 0) and pd.notna(pct) and (pct <= cut_thr))
    out["evt_div_suspend"] = int((_finite(dv_l1) and float(dv_l1) > 0) and (_finite(dv) and float(dv) <= 0))
    out["evt_div_initiate"] = int((not _finite(dv_l1) or float(dv_l1) <= 0) and (_finite(dv) and float(dv) > 0))

    # Coverage (RAW interest coverage ratio)
    # NOTE: sp_interest_coverage is a signed log1p-transformed quantity (with caps),
    # so thresholding it at 1.0 is economically incorrect. Events are therefore computed on the RAW ratio.

    cov_raw = g("sp_interest_coverage_raw", np.nan)
    cov_raw_l1 = glag("sp_interest_coverage_raw", np.nan)

    # Fallback reconstruction (if raw coverage column is absent in the selected row(s))
    if not _finite(cov_raw):
        oibdp_now = g("oibdp", np.nan)
        xint_now = g("xint", np.nan)
        denom_now = max(abs(float(xint_now)), 1.0) if _finite(xint_now) else np.nan
        cov_raw = (float(oibdp_now) / denom_now) if (_finite(oibdp_now) and np.isfinite(denom_now) and denom_now != 0.0) else np.nan

    if not _finite(cov_raw_l1):
        oibdp_prev = glag("oibdp", np.nan)
        xint_prev = glag("xint", np.nan)
        denom_prev = max(abs(float(xint_prev)), 1.0) if _finite(xint_prev) else np.nan
        cov_raw_l1 = (float(oibdp_prev) / denom_prev) if (_finite(oibdp_prev) and np.isfinite(denom_prev) and denom_prev != 0.0) else np.nan

    out["evt_cov_breach"] = int(_finite(cov_raw) and (float(cov_raw) < 1.0))  # EBITDA/|interest| < 1 (raw)
    cov_ratio = (cov_raw / cov_raw_l1) if (_finite(cov_raw) and _finite(cov_raw_l1) and float(cov_raw_l1) != 0.0) else np.nan
    out["evt_cov_collapse"] = int(pd.notna(cov_ratio) and (cov_ratio < cov_drop_thr))

    # Leverage spike (debt-to-capital) — guard against inf - inf
    lev = g("sp_debt_to_capital", np.nan)
    lev_l1 = glag("sp_debt_to_capital", np.nan)
    dlev = (float(lev) - float(lev_l1)) if (_finite(lev) and _finite(lev_l1)) else np.nan
    out["evt_lev_spike"] = int(pd.notna(dlev) and (dlev >= dlev_thr))

    # Liquidity squeeze (act/lct and quick ratio)
    act = g("act", np.nan)
    lct = g("lct", np.nan)
    invt = g("invt", 0.0)
    cr = (act / lct) if (_finite(act) and _finite(lct) and float(lct) != 0.0) else np.nan
    qr = ((act - invt) / lct) if (_finite(act) and _finite(lct) and float(lct) != 0.0 and _finite(invt)) else np.nan
    out["evt_liquidity_squeeze"] = int(pd.notna(cr) and (cr < 1.0))
    out["evt_quick_squeeze"] = int(pd.notna(qr) and (qr < 0.8))

    # EBITDA and CFO stress
    oibdp = g("oibdp", np.nan)
    oibdp_l1 = glag("oibdp", np.nan)
    doibdp = (oibdp / oibdp_l1) if (_finite(oibdp) and _finite(oibdp_l1) and float(oibdp_l1) != 0.0) else np.nan
    out["evt_ebitda_drop"] = int(pd.notna(doibdp) and (doibdp < float(doibdp_thr) if np.isfinite(doibdp_thr) else doibdp < 0.7))
    out["evt_ebitda_neg"] = int(_finite(oibdp) and float(oibdp) < 0)

    oancf = g("oancf", np.nan)
    oancf_l1 = glag("oancf", np.nan)
    dcfo = (oancf / oancf_l1) if (_finite(oancf) and _finite(oancf_l1) and float(oancf_l1) != 0.0) else np.nan
    out["evt_cfo_drop"] = int(pd.notna(dcfo) and (dcfo < cfo_drop_thr))
    out["evt_cfo_neg"] = int(_finite(oancf) and float(oancf) < 0)

    return out


def _prepare_feature_row(raw_row: pd.Series, lag_row: pd.Series | None) -> pd.DataFrame:
    """Given a raw row, recompute events, winsorize continuous feats, scale to z, and return 1-row DF with MODEL_FEATS."""
    one = raw_row.copy()

    # Recompute events (only overwrite those present)
    ev = _recompute_events_one(one, lag_row)
    for k, v in ev.items():
        if k in _df.columns:
            one[k] = v

    # Winsorize continuous features using TRAIN bounds (stored in `bounds`)
    for c, (lo, hi) in bounds.items():
        if c in one.index:
            v = pd.to_numeric(one[c], errors="coerce")
            v = np.nan if not np.isfinite(v) else float(v)
            if pd.notna(v) and np.isfinite(lo) and np.isfinite(hi):
                one[c] = float(np.clip(v, lo, hi))
            else:
                one[c] = v

    # Build a 1-row df
    one_df = pd.DataFrame([one])

    # Standardize continuous feats to z_ columns
    one_df[continuous_feats] = one_df[continuous_feats].replace([np.inf, -np.inf], np.nan)
    one_df[continuous_feats] = one_df[continuous_feats].fillna(pd.DataFrame([_df[continuous_feats].median(numeric_only=True)]).iloc[0])

    z_cols = [f"z_{c}" for c in continuous_feats]
    one_df[z_cols] = scaler.transform(one_df[continuous_feats].to_numpy(dtype=float))

    # Ensure event features exist
    for e in event_feats:
        if e not in one_df.columns:
            one_df[e] = 0

    # Final feature row
    return one_df


# Pick a representative firm-year for scenario (highest PD in TEST if available)
if "test" in globals() and "test_proba" in globals():
    idx = int(np.argmax(test_proba))
    base_row = test.iloc[idx].copy()
else:
    base_row = _df.sample(1, random_state=42).iloc[0].copy()

# Pull lagged row if available
lag_row = None
if "firm_id" in _df.columns and "fyear" in _df.columns and "firm_id" in base_row.index:
    fid = base_row["firm_id"]
    y = int(base_row["fyear"])
    lag_match = _df[(_df["firm_id"] == fid) & (_df["fyear"] == (y - 1))]
    if len(lag_match) > 0:
        lag_row = lag_match.iloc[0].copy()

# Base PD
base_feat = _prepare_feature_row(base_row, lag_row)
base_pd = float(_predict_pd_from_features(base_feat)[0])

# Scenarios (simple, interpretable pro-forma adjustments)

scenarios = {
    "Base": {},
    "Dividend suspension (dv→0)": {"dv": 0.0},
    # Note: leverage/coverage scenarios intentionally dropped because leverage/coverage channels are excluded
    # from MODEL_FEATS in this publication specification (to avoid circularity with the label definition).
    "Liquidity buffer (CR>=1.2)": {},  # handled below
}

results = []
for name, adj in scenarios.items():
    row_s = base_row.copy()

    # Apply direct adjustments
    for k, v in adj.items():
        if pd.notna(v) and np.isfinite(v):
            row_s[k] = v

    # Liquidity buffer: minimally raise ACT to reach current ratio >= 1.2 (if inputs exist)
    if name == "Liquidity buffer (CR>=1.2)":
        act = pd.to_numeric(row_s.get("act", np.nan), errors="coerce")
        lct = pd.to_numeric(row_s.get("lct", np.nan), errors="coerce")
        if np.isfinite(act) and np.isfinite(lct) and lct > 0:
            target_act = 1.2 * lct
            row_s["act"] = float(max(act, target_act))

    feat = _prepare_feature_row(row_s, lag_row)

    # Audit: ensure the scenario actually moves at least one MODEL_FEATS column.
    def _neq(a, b):
        # NaN-safe inequality
        if pd.isna(a) and pd.isna(b):
            return False
        try:
            return float(a) != float(b)
        except Exception:
            return a != b

    changed = [c for c in MODEL_FEATS if _neq(feat[c].iloc[0], base_feat[c].iloc[0])]
    pd_s = float(_predict_pd_from_features(feat)[0])

    results.append((name, pd_s, pd_s - base_pd, ", ".join(changed) if changed else "(no MODEL_FEATS change)"))

scen_tbl = (
    pd.DataFrame(results, columns=["Scenario", "PD", "ΔPD vs Base", "MODEL_FEATS changed"])
      .sort_values("PD", ascending=False)
)
print(f"Base PD: {base_pd:.4f}")
display(scen_tbl)

```

    Base PD: 0.9867



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Scenario</th>
      <th>PD</th>
      <th>ΔPD vs Base</th>
      <th>MODEL_FEATS changed</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Base</td>
      <td>0.986714</td>
      <td>0.000000</td>
      <td>(no MODEL_FEATS change)</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Dividend suspension (dv→0)</td>
      <td>0.986714</td>
      <td>0.000000</td>
      <td>(no MODEL_FEATS change)</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Liquidity buffer (CR&gt;=1.2)</td>
      <td>0.962030</td>
      <td>-0.024684</td>
      <td>evt_liquidity_squeeze, evt_quick_squeeze</td>
    </tr>
  </tbody>
</table>
</div>



```python
# =============================================================================
# 7.5 Decision curves (net benefit) + cost curves (screening policy)
# =============================================================================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Use calibrated tree PDs if available; otherwise fall back to logistic
if "y_test" in globals() and "test_proba" in globals():
    y = np.asarray(y_test).astype(int)
    p = np.asarray(test_proba).astype(float)
    model_name = "XGBoost (calibrated)"

elif "test" in globals() and "TARGET_COL" in globals() and "test_proba" in globals():
    y = np.asarray(test[TARGET_COL]).astype(int)
    p = np.asarray(test_proba).astype(float)
    model_name = "Model"
else:
    raise RuntimeError("No TEST probabilities found. Run model cells first.")

def net_benefit(y_true, proba, thr):
    y_hat = (proba >= thr).astype(int)
    tp = int(((y_hat == 1) & (y_true == 1)).sum())
    fp = int(((y_hat == 1) & (y_true == 0)).sum())
    n = len(y_true)
    w = thr / max(1 - thr, 1e-12)  # harm/benefit trade-off implied by threshold
    return (tp / n) - (fp / n) * w

ths = np.linspace(0.01, 0.60, 60)
nb_model = np.array([net_benefit(y, p, t) for t in ths])

# Treat-all and treat-none baselines for decision curve analysis
prev = y.mean()
nb_all = prev - (1 - prev) * (ths / (1 - ths))
nb_none = np.zeros_like(ths)

fig, ax = plt.subplots(figsize=(9, 4))
ax.plot(ths, nb_model, label=model_name)
ax.plot(ths, nb_all, linestyle="--", label="Treat all")
ax.plot(ths, nb_none, linestyle="--", label="Treat none")
ax.set_title("Decision curve (Net Benefit)")
ax.set_xlabel("Threshold probability")
ax.set_ylabel("Net benefit")
ax.grid(True, alpha=0.3)
ax.legend(loc="best")
plt.show()

# Cost curve: expected cost under FP/FN costs
C_FN = float(globals().get("C_FN", 5.0))  # default: FN 5x as costly as FP
C_FP = float(globals().get("C_FP", 1.0))

def expected_cost(y_true, proba, thr, c_fn=C_FN, c_fp=C_FP):
    y_hat = (proba >= thr).astype(int)
    fn = int(((y_hat == 0) & (y_true == 1)).sum())
    fp = int(((y_hat == 1) & (y_true == 0)).sum())
    return (c_fn * fn + c_fp * fp) / max(len(y_true), 1)

costs = np.array([expected_cost(y, p, t) for t in ths])
fig, ax = plt.subplots(figsize=(9, 4))
ax.plot(ths, costs)
ax.set_title(f"Expected cost vs threshold (C_FN={C_FN:g}, C_FP={C_FP:g})")
ax.set_xlabel("Threshold probability")
ax.set_ylabel("Expected cost per observation")
ax.grid(True, alpha=0.3)
plt.show()

```


    
![png](05_01_26_v1_files/05_01_26_v1_15_0.png)
    



    
![png](05_01_26_v1_files/05_01_26_v1_15_1.png)
    

