{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e170ebd6",
   "metadata": {},
   "source": [
    "# Financial Distress Prediction Pipeline (Adjusted)\n",
    "\n",
    "This notebook follows the same structure as your current streamlined pipeline while fixing the material methodological and technical issues:\n",
    "\n",
    "- Panel-safe lag/lead construction (sorting enforced)\n",
    "- Missingness-aware distress proxy (avoids NaN → False “healthy” bias)\n",
    "- Leakage-free event threshold calibration (train-only)\n",
    "- Event indicators restricted to non-proxy channels (no coverage/leverage/EBITDA-proxy events)\n",
    "- Stable preprocessing (train-fitted clipping + median imputation + scaling)\n",
    "- Correct TreeSHAP extraction for XGBoost\n",
    "- Scenario analysis that propagates through engineered features (no deleveraging/coverage scenarios; no proxy-related shocks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "245e0eb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:13.422995Z",
     "start_time": "2026-01-04T21:21:13.391983Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 0. Project Overview — Financial Distress Prediction Pipeline\n",
    "# =============================================================================\n",
    "# This notebook follows the standard Data Science Lifecycle:\n",
    "#   (1) Data Cleaning and Quality Diagnostics\n",
    "#   (2) Missing-Data Handling (leakage-aware)\n",
    "#   (3) Feature Engineering and Label Construction\n",
    "#   (4) Event Indicators (interpretable drivers; non-proxy channels only)\n",
    "#   (5) Train / Validation / Test Split and Preprocessing\n",
    "#   (6) Logit Models (supervised benchmark + inference audit)\n",
    "#   (7) Tree-based Model (XGBoost with native TreeSHAP explainability)\n",
    "#   (8) Evaluation and Benchmarks (Persistence vs. Early Warning)\n",
    "#   (9) Decision Support and Scenario Analysis (primitive shocks; recompute features/events)\n",
    "#\n",
    "# Key design constraint (top-tier measurement integrity):\n",
    "#   - The distress outcome is a constructed proxy. To avoid circularity, the modeling feature set\n",
    "#     EXCLUDES leverage/coverage ratios that mechanically define the proxy.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, log_loss\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f77ea01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:15.226223Z",
     "start_time": "2026-01-04T21:21:13.485594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 75,005 firm-year observations, 89 variables.\n",
      "Years: 2014–2024\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1. Data Import and Cleaning\n",
    "# =============================================================================\n",
    "\n",
    "DATA_PATH = \"data.csv\"  # file must be in the same folder\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "df.columns = df.columns.str.lower().str.strip()\n",
    "\n",
    "# Keep a stable panel identifier (Compustat-style gvkey) and fiscal year\n",
    "if \"gvkey\" not in df.columns or \"fyear\" not in df.columns:\n",
    "    raise ValueError(\"Input must include columns: gvkey, fyear\")\n",
    "\n",
    "# Drop duplicates (keep last record for a given firm-year) and enforce ordering BEFORE any lag/lead ops\n",
    "df = df.drop_duplicates(subset=[\"gvkey\", \"fyear\"], keep=\"last\").copy()\n",
    "\n",
    "# Normalize identifiers\n",
    "df = df[df[\"gvkey\"].notna()].copy()\n",
    "df[\"gvkey\"] = (\n",
    "    df[\"gvkey\"]\n",
    "      .astype(str)\n",
    "      .str.strip()\n",
    "      .str.replace(r\"\\.0$\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "df[\"fyear\"] = pd.to_numeric(df[\"fyear\"], errors=\"coerce\")\n",
    "df = df[df[\"fyear\"].notna()].copy()\n",
    "df[\"fyear\"] = df[\"fyear\"].astype(int)\n",
    "\n",
    "# Convert \"likely numeric\" columns (best-effort), but keep gvkey as string\n",
    "for col in df.columns:\n",
    "    if col == \"gvkey\":\n",
    "        continue\n",
    "    # Do not force-convert obvious non-numeric columns; keep best-effort\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"ignore\")\n",
    "\n",
    "df = df.sort_values([\"gvkey\", \"fyear\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]:,} firm-year observations, {df.shape[1]} variables.\")\n",
    "print(f\"Years: {int(df['fyear'].min())}–{int(df['fyear'].max())}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1A. EDA (pre-imputation): Missingness & basic distributions\n",
    "# =============================================================================\n",
    "\n",
    "TRAIN_END_YEAR  = 2020\n",
    "VAL_YEAR        = 2021\n",
    "TEST_START_YEAR = 2022\n",
    "\n",
    "train_mask_for_imputation = df[\"fyear\"] <= TRAIN_END_YEAR\n",
    "\n",
    "RAW_INPUT_CANDIDATES = [\n",
    "    # Magnitudes / size proxies\n",
    "    \"at\", \"mkvalt\",\n",
    "    # Debt/capital structure\n",
    "    \"dlc\", \"dltt\", \"seq\", \"mibt\",\n",
    "    # Operating performance & coverage inputs\n",
    "    \"oibdp\", \"xint\", \"txt\", \"txdc\", \"txach\",\n",
    "    # Cash flow statement\n",
    "    \"oancf\", \"capx\",\n",
    "    # Liquidity & payout policy (for non-proxy event indicators)\n",
    "    \"che\", \"dv\",\n",
    "    # Liquidity ratio inputs (optional)\n",
    "    \"act\", \"lct\",\n",
    "    # Share repurchases (optional; for a broad DCF proxy)\n",
    "    \"prstkc\",\n",
    "]\n",
    "RAW_INPUTS = [c for c in RAW_INPUT_CANDIDATES if c in df.columns]\n",
    "\n",
    "df_raw_pre = df[RAW_INPUTS].copy()\n",
    "\n",
    "pre_miss = pd.DataFrame({\n",
    "    \"col\": RAW_INPUTS,\n",
    "    \"n\": [int(len(df_raw_pre)) for _ in RAW_INPUTS],\n",
    "    \"n_na_pre\": [int(df_raw_pre[c].isna().sum()) for c in RAW_INPUTS],\n",
    "    \"pct_na_pre\": [float(df_raw_pre[c].isna().mean() * 100.0) for c in RAW_INPUTS],\n",
    "    \"train_pct_na_pre\": [\n",
    "        float(df_raw_pre.loc[train_mask_for_imputation, c].isna().mean() * 100.0) for c in RAW_INPUTS\n",
    "    ],\n",
    "}).sort_values(\"pct_na_pre\", ascending=False)\n",
    "\n",
    "print(\"\\n=== Missingness on raw inputs (before imputation) ===\")\n",
    "display(pre_miss)\n",
    "\n",
    "print(\"\\n=== Distribution snapshot (raw inputs; before imputation) ===\")\n",
    "desc = df_raw_pre.apply(pd.to_numeric, errors=\"coerce\").describe(\n",
    "    percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]\n",
    ").T\n",
    "display(desc)\n",
    "\n",
    "# =============================================================================\n",
    "# 1B. Missing-Data Handling (leakage-aware)\n",
    "# =============================================================================\n",
    "# Goal: minimize mechanical label drift and leakage.\n",
    "#   - Create missingness flags (miss_*) to preserve informative missingness.\n",
    "#   - Construct size_decile from TRAIN distribution of log(assets) to respect scale heterogeneity.\n",
    "#   - Impute raw accounting inputs using TRAIN-only information:\n",
    "#         (i) within-firm lag-1 carryforward (economically plausible)\n",
    "#        (ii) peer medians (training years) by size_decile, with year×size_decile when available\n",
    "#       (iii) KNN imputation (TRAIN-fit) as a final fill for selected balance-sheet items\n",
    "\n",
    "# --- Missingness indicators ---\n",
    "for c in RAW_INPUTS:\n",
    "    df[f\"miss_{c}\"] = df[c].isna().astype(\"int8\")\n",
    "\n",
    "# --- Size deciles (TRAIN-only cutpoints) ---\n",
    "if \"at\" in df.columns:\n",
    "    at = pd.to_numeric(df[\"at\"], errors=\"coerce\")\n",
    "    log_at_raw = pd.Series(np.where(at > 0, np.log(at), np.nan), index=df.index)\n",
    "else:\n",
    "    log_at_raw = pd.Series(np.nan, index=df.index)\n",
    "\n",
    "train_log_at = log_at_raw.loc[train_mask_for_imputation].dropna()\n",
    "\n",
    "if len(train_log_at) >= 200:\n",
    "    try:\n",
    "        _, bins = pd.qcut(train_log_at, q=10, retbins=True, duplicates=\"drop\")\n",
    "        bins = np.unique(bins)\n",
    "        # Ensure open-ended bins for stable assignment\n",
    "        bins[0] = -np.inf\n",
    "        bins[-1] = np.inf\n",
    "        size_decile = pd.cut(log_at_raw, bins=bins, labels=False, include_lowest=True)\n",
    "        df[\"size_decile\"] = (size_decile + 1).astype(\"Int64\").fillna(5).astype(int)\n",
    "    except Exception:\n",
    "        df[\"size_decile\"] = 5\n",
    "else:\n",
    "    df[\"size_decile\"] = 5\n",
    "\n",
    "# --- Step (i): within-firm lag-1 carryforward for selected level variables ---\n",
    "lag_fill_candidates = [\n",
    "    \"at\", \"mkvalt\", \"dlc\", \"dltt\", \"seq\", \"mibt\",\n",
    "    \"oibdp\", \"xint\", \"oancf\", \"capx\", \"che\", \"dv\", \"act\", \"lct\", \"prstkc\",\n",
    "]\n",
    "lag_fill_cols = [c for c in lag_fill_candidates if c in df.columns]\n",
    "\n",
    "for c in lag_fill_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df[c] = df.groupby(\"gvkey\")[c].transform(lambda s: s.fillna(s.shift(1)))\n",
    "\n",
    "# --- Step (ii): peer medians from TRAIN only (size_decile, optionally year×size_decile within TRAIN years) ---\n",
    "def peer_median_impute_inplace(df_in: pd.DataFrame, cols: list[str]) -> None:\n",
    "    tr = df_in.loc[train_mask_for_imputation, [\"fyear\", \"size_decile\"] + cols].copy()\n",
    "    for c in cols:\n",
    "        if c not in df_in.columns:\n",
    "            continue\n",
    "        tr_c = pd.to_numeric(tr[c], errors=\"coerce\")\n",
    "        if tr_c.notna().sum() == 0:\n",
    "            continue\n",
    "\n",
    "        med_global = float(tr_c.median())\n",
    "        med_by_dec = tr.groupby(\"size_decile\")[c].median()\n",
    "        med_by_year_dec = tr.groupby([\"fyear\", \"size_decile\"])[c].median()\n",
    "\n",
    "        miss_idx = df_in.index[df_in[c].isna()]\n",
    "        if len(miss_idx) == 0:\n",
    "            continue\n",
    "\n",
    "        tmp = df_in.loc[miss_idx, [\"fyear\", \"size_decile\"]].copy()\n",
    "        year_dec_key = list(zip(tmp[\"fyear\"].astype(int), tmp[\"size_decile\"].astype(int)))\n",
    "        fill_year_dec = pd.Series(year_dec_key, index=tmp.index).map(med_by_year_dec)\n",
    "        fill_dec = tmp[\"size_decile\"].map(med_by_dec)\n",
    "\n",
    "        fill = fill_year_dec.where(fill_year_dec.notna(), fill_dec)\n",
    "        fill = fill.fillna(med_global)\n",
    "\n",
    "        df_in.loc[miss_idx, c] = fill.values\n",
    "\n",
    "peer_median_cols = [c for c in RAW_INPUTS if c in df.columns]\n",
    "peer_median_impute_inplace(df, peer_median_cols)\n",
    "\n",
    "# --- Step (iii): KNN imputation (TRAIN-fit) as final fill for selected balance-sheet items ---\n",
    "knn_candidates = [\"at\", \"dlc\", \"dltt\", \"che\", \"act\", \"lct\", \"seq\"]\n",
    "knn_cols = [c for c in knn_candidates if c in df.columns]\n",
    "\n",
    "def signed_log1p(x: np.ndarray) -> np.ndarray:\n",
    "    return np.sign(x) * np.log1p(np.abs(x))\n",
    "\n",
    "def signed_expm1(z: np.ndarray) -> np.ndarray:\n",
    "    return np.sign(z) * np.expm1(np.abs(z))\n",
    "\n",
    "if len(knn_cols) >= 2:\n",
    "    X_knn = df[knn_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    X_knn_log = signed_log1p(X_knn.to_numpy(dtype=float))\n",
    "\n",
    "    imputer = KNNImputer(n_neighbors=5, weights=\"distance\")\n",
    "    imputer.fit(X_knn_log[train_mask_for_imputation.values, :])\n",
    "\n",
    "    X_imp = imputer.transform(X_knn_log)\n",
    "    X_imp = signed_expm1(X_imp)\n",
    "    X_imp = pd.DataFrame(X_imp, columns=knn_cols, index=df.index)\n",
    "\n",
    "    for c in knn_cols:\n",
    "        m = df[c].isna()\n",
    "        if int(m.sum()) > 0:\n",
    "            df.loc[m, c] = X_imp.loc[m, c]\n",
    "\n",
    "# --- Imputation impact audit ---\n",
    "df_raw_post = df[RAW_INPUTS].copy()\n",
    "post_miss = pd.DataFrame({\n",
    "    \"col\": RAW_INPUTS,\n",
    "    \"n_na_pre\": [int(df_raw_pre[c].isna().sum()) for c in RAW_INPUTS],\n",
    "    \"n_na_post\": [int(df_raw_post[c].isna().sum()) for c in RAW_INPUTS],\n",
    "    \"pct_na_pre\": [float(df_raw_pre[c].isna().mean() * 100.0) for c in RAW_INPUTS],\n",
    "    \"pct_na_post\": [float(df_raw_post[c].isna().mean() * 100.0) for c in RAW_INPUTS],\n",
    "})\n",
    "post_miss[\"n_filled\"] = post_miss[\"n_na_pre\"] - post_miss[\"n_na_post\"]\n",
    "post_miss[\"pct_filled_of_na\"] = np.where(\n",
    "    post_miss[\"n_na_pre\"] > 0,\n",
    "    100.0 * post_miss[\"n_filled\"] / post_miss[\"n_na_pre\"],\n",
    "    np.nan\n",
    ")\n",
    "post_miss = post_miss.sort_values(\"pct_na_pre\", ascending=False)\n",
    "\n",
    "print(\"\\n=== Missingness AFTER imputation (audit) ===\")\n",
    "display(post_miss)\n",
    "\n",
    "# (Optional) quick visual: top-missing variables before vs after\n",
    "top = post_miss.head(12).copy()\n",
    "plt.figure()\n",
    "plt.barh(top[\"col\"], top[\"pct_na_pre\"], label=\"Pre\")\n",
    "plt.barh(top[\"col\"], top[\"pct_na_post\"], label=\"Post\")\n",
    "plt.xlabel(\"% missing\")\n",
    "plt.title(\"Top missing raw inputs: before vs after imputation\")\n",
    "plt.legend()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2585ef2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:15.335282Z",
     "start_time": "2026-01-04T21:21:15.320289Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. Helper Functions\n",
    "# =============================================================================\n",
    "\n",
    "def safe_divide(a, b):\n",
    "    \"\"\"Numerically stable division with correct alignment; returns NaN for non-finite results.\"\"\"\n",
    "    a = pd.to_numeric(a, errors=\"coerce\")\n",
    "    b = pd.to_numeric(b, errors=\"coerce\")\n",
    "\n",
    "    # Correctness: enforce alignment/broadcasting so Series/scalars never mis-shape\n",
    "    if isinstance(a, pd.Series) and isinstance(b, pd.Series):\n",
    "        a, b = a.align(b)\n",
    "    else:\n",
    "        a, b = np.broadcast_arrays(a, b)\n",
    "\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        res = a / b\n",
    "\n",
    "    if isinstance(res, pd.Series):\n",
    "        res = res.replace([np.inf, -np.inf], np.nan)\n",
    "    else:\n",
    "        res = np.where(np.isfinite(res), res, np.nan)\n",
    "    return res\n",
    "\n",
    "def safe_log(x):\n",
    "    \"\"\"log(x) for x>0 else NaN.\"\"\"\n",
    "    x = pd.to_numeric(x, errors=\"coerce\")\n",
    "    out = pd.Series(np.nan, index=x.index, dtype=\"float64\")\n",
    "    m = x > 0\n",
    "    out.loc[m] = np.log(x.loc[m])\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cbb98be7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:15.514169Z",
     "start_time": "2026-01-04T21:21:15.336329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling sample: 63,599 firm-years with defensible current distress and next-year labels.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3. Feature Engineering and Label Construction\n",
    "# =============================================================================\n",
    "# Outcome design:\n",
    "#   - distress_dummy(t) is a constructed proxy (high leverage OR negative equity)\n",
    "#   - target_next_year_distress(t) = distress_dummy(t+1) within the same firm\n",
    "#\n",
    "# Measurement guardrail:\n",
    "#   - comparisons are missingness-aware: we do not silently treat NaNs as \"healthy\"\n",
    "#   - for the PROXY ratios only: non-positive denominators are treated as tail states (set to +inf)\n",
    "\n",
    "firm_col = \"gvkey\"\n",
    "\n",
    "# --- Debt and capital components (missingness-aware aggregation) ---\n",
    "dlc  = pd.to_numeric(df.get(\"dlc\", np.nan), errors=\"coerce\")\n",
    "dltt = pd.to_numeric(df.get(\"dltt\", np.nan), errors=\"coerce\")\n",
    "df[\"total_debt\"] = pd.concat([dlc, dltt], axis=1).sum(axis=1, min_count=1)\n",
    "\n",
    "seq  = pd.to_numeric(df.get(\"seq\", np.nan), errors=\"coerce\")\n",
    "mibt = pd.to_numeric(df.get(\"mibt\", np.nan), errors=\"coerce\") if \"mibt\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "df[\"equity_plus_mi_sp\"] = pd.concat([seq, mibt], axis=1).sum(axis=1, min_count=1)\n",
    "\n",
    "df[\"total_capital_sp\"] = df[\"total_debt\"] + df[\"equity_plus_mi_sp\"]\n",
    "\n",
    "# --- Operating inputs ---\n",
    "oibdp = pd.to_numeric(df.get(\"oibdp\", np.nan), errors=\"coerce\")  # EBITDA proxy\n",
    "xint  = pd.to_numeric(df.get(\"xint\", np.nan), errors=\"coerce\")\n",
    "\n",
    "# --- FFO proxy (tax adjustment only where available; avoids hard-coding zeros) ---\n",
    "txt   = pd.to_numeric(df.get(\"txt\", np.nan), errors=\"coerce\") if \"txt\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "txdc  = pd.to_numeric(df.get(\"txdc\", np.nan), errors=\"coerce\") if \"txdc\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "txach = pd.to_numeric(df.get(\"txach\", np.nan), errors=\"coerce\") if \"txach\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "\n",
    "tax_adj = (txt - txdc - txach)\n",
    "ffo_base = oibdp - xint\n",
    "ffo_adj = ffo_base.copy()\n",
    "ffo_adj.loc[tax_adj.notna()] = (ffo_base - tax_adj).loc[tax_adj.notna()]\n",
    "\n",
    "# --- Cash flow capacity ratios (NON-proxy modeling channels) ---\n",
    "oancf = pd.to_numeric(df.get(\"oancf\", np.nan), errors=\"coerce\")\n",
    "capx  = pd.to_numeric(df.get(\"capx\", np.nan), errors=\"coerce\") if \"capx\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "\n",
    "df[\"focf\"] = oancf - capx\n",
    "\n",
    "dv = pd.to_numeric(df.get(\"dv\", np.nan), errors=\"coerce\") if \"dv\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "prstkc = pd.to_numeric(df.get(\"prstkc\", np.nan), errors=\"coerce\") if \"prstkc\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "\n",
    "# Defensive convention: if payout/repurchase is missing, treat as 0 for DCF proxy,\n",
    "# while retaining miss_dv/miss_prstkc indicators from the imputation block.\n",
    "dv0 = dv.fillna(0.0)\n",
    "prstkc0 = prstkc.fillna(0.0)\n",
    "\n",
    "df[\"dcf\"] = df[\"focf\"] - dv0 - prstkc0\n",
    "\n",
    "td = df[\"total_debt\"]\n",
    "td_pos = td.notna() & (td > 0)\n",
    "\n",
    "df[\"sp_cfo_to_debt\"]  = np.where(td_pos, safe_divide(oancf, td), np.nan)\n",
    "df[\"sp_focf_to_debt\"] = np.where(td_pos, safe_divide(df[\"focf\"], td), np.nan)\n",
    "df[\"sp_dcf_to_debt\"]  = np.where(td_pos, safe_divide(df[\"dcf\"], td), np.nan)\n",
    "\n",
    "# --- Size / market variables (explicit inside notebook for reproducibility) ---\n",
    "if \"at\" in df.columns:\n",
    "    df[\"log_at\"] = safe_log(df[\"at\"])\n",
    "if \"mkvalt\" in df.columns:\n",
    "    df[\"log_mkvalt\"] = safe_log(df[\"mkvalt\"])\n",
    "\n",
    "# =============================================================================\n",
    "# Distress proxy (measurement is the outcome; treat non-positive denominators as tail states)\n",
    "# =============================================================================\n",
    "\n",
    "cap = df[\"total_capital_sp\"]\n",
    "\n",
    "# Proxy ratios (tail-handling)\n",
    "ffo_to_debt_pct = pd.Series(np.nan, index=df.index, dtype=\"float64\")\n",
    "m_ffo = td_pos & ffo_adj.notna()\n",
    "ffo_to_debt_pct.loc[m_ffo] = 100.0 * (ffo_adj.loc[m_ffo] / td.loc[m_ffo])\n",
    "\n",
    "debt_to_capital_pct = pd.Series(np.nan, index=df.index, dtype=\"float64\")\n",
    "m_cap_pos = td_pos & cap.notna() & (cap > 0)\n",
    "debt_to_capital_pct.loc[m_cap_pos] = 100.0 * (td.loc[m_cap_pos] / cap.loc[m_cap_pos])\n",
    "m_cap_nonpos = td_pos & cap.notna() & (cap <= 0)\n",
    "debt_to_capital_pct.loc[m_cap_nonpos] = np.inf\n",
    "\n",
    "debt_to_ebitda = pd.Series(np.nan, index=df.index, dtype=\"float64\")\n",
    "m_eb_pos = td_pos & oibdp.notna() & (oibdp > 0)\n",
    "debt_to_ebitda.loc[m_eb_pos] = td.loc[m_eb_pos] / oibdp.loc[m_eb_pos]\n",
    "m_eb_nonpos = td_pos & oibdp.notna() & (oibdp <= 0)\n",
    "debt_to_ebitda.loc[m_eb_nonpos] = np.inf\n",
    "\n",
    "valid_hl = ffo_to_debt_pct.notna() & debt_to_capital_pct.notna() & debt_to_ebitda.notna()\n",
    "\n",
    "hl_ffo = valid_hl & (ffo_to_debt_pct < 15)\n",
    "hl_cap = valid_hl & (debt_to_capital_pct > 55)\n",
    "hl_deb = valid_hl & (debt_to_ebitda > 4.5)\n",
    "\n",
    "is_highly_leveraged = hl_ffo & hl_cap & hl_deb\n",
    "\n",
    "valid_seq = seq.notna()\n",
    "is_equity_negative = valid_seq & (seq < 0)\n",
    "\n",
    "distress = pd.Series(np.nan, index=df.index, dtype=\"float64\")\n",
    "info_mask = valid_hl | valid_seq\n",
    "distress.loc[info_mask] = (is_highly_leveraged | is_equity_negative).loc[info_mask].astype(\"int8\")\n",
    "\n",
    "df[\"distress_dummy\"] = distress  # keep NaN where label is not defensible\n",
    "\n",
    "# Target: next year's distress (panel-safe due to sorting above)\n",
    "df[\"target_next_year_distress\"] = df.groupby(firm_col)[\"distress_dummy\"].shift(-1)\n",
    "\n",
    "# Modeling sample restriction: require defensible current distress AND next-year label\n",
    "df_model = df[df[\"target_next_year_distress\"].notna() & df[\"distress_dummy\"].notna()].copy()\n",
    "df_model[\"target_next_year_distress\"] = df_model[\"target_next_year_distress\"].astype(\"int8\")\n",
    "df_model[\"distress_dummy\"] = df_model[\"distress_dummy\"].astype(\"int8\")\n",
    "\n",
    "print(f\"Modeling sample: {len(df_model):,} firm-years with defensible current distress and next-year labels.\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3A. EDA (post-label): base rates and attrition diagnostics\n",
    "# =============================================================================\n",
    "\n",
    "rate_by_year = df_model.groupby(\"fyear\")[[\"distress_dummy\", \"target_next_year_distress\"]].mean()\n",
    "print(\"\\n=== Base rates by fiscal year (current vs next-year proxy) ===\")\n",
    "display(rate_by_year)\n",
    "\n",
    "if \"size_decile\" in df_model.columns:\n",
    "    rate_by_size = df_model.groupby(\"size_decile\")[[\"distress_dummy\", \"target_next_year_distress\"]].mean()\n",
    "    print(\"\\n=== Base rates by TRAIN-derived size decile ===\")\n",
    "    display(rate_by_size)\n",
    "\n",
    "# Quick distribution check for modeling channels\n",
    "eda_cols = [c for c in [\"sp_cfo_to_debt\", \"sp_focf_to_debt\", \"sp_dcf_to_debt\", \"log_at\", \"log_mkvalt\"] if c in df_model.columns]\n",
    "if len(eda_cols) > 0:\n",
    "    df_model[eda_cols].hist(bins=40, figsize=(12, 6))\n",
    "    plt.suptitle(\"Distributions of key modeling channels (post-imputation/engineering)\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07d1cfd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:15.612397Z",
     "start_time": "2026-01-04T21:21:15.519678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event indicators included: ['evt_div_suspend', 'evt_div_initiate', 'evt_div_cut', 'evt_cfo_neg', 'evt_cfo_collapse', 'evt_focf_neg', 'evt_focf_collapse', 'evt_cash_drawdown']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 4. Event Indicators — Interpretable Drivers (NON-proxy channels only)\n",
    "# =============================================================================\n",
    "# Constraint: exclude coverage/leverage/EBITDA-proxy events (anything mechanically embedded in distress_proxy).\n",
    "# We therefore focus on:\n",
    "#   - Dividend policy moments (cuts / suspensions / initiations)\n",
    "#   - Cash-flow shocks (CFO / FOCF)\n",
    "#   - Liquidity deterioration (cash drawdowns; current-ratio squeeze if available)\n",
    "#\n",
    "# Thresholds are calibrated on TRAIN ONLY (<= TRAIN_END_YEAR) to avoid leakage.\n",
    "\n",
    "train_mask = df_model[\"fyear\"] <= TRAIN_END_YEAR\n",
    "\n",
    "# --- Dividend moments ---\n",
    "if \"dv\" in df_model.columns:\n",
    "    dv = pd.to_numeric(df_model[\"dv\"], errors=\"coerce\")\n",
    "    df_model[\"dv_l1\"] = df_model.groupby(\"gvkey\")[\"dv\"].shift(1)\n",
    "\n",
    "    # Among observed payers in TRAIN: calibrate \"cut\" threshold on low percentile of YoY ratio\n",
    "    dv_ratio = safe_divide(dv, df_model[\"dv_l1\"])\n",
    "    valid_dv = df_model[\"dv_l1\"] > 0\n",
    "\n",
    "    cut_q = dv_ratio[train_mask & valid_dv].quantile(0.10)\n",
    "    cut_thr = float(np.clip(cut_q, 0.50, 0.95))  # bounded for stability\n",
    "\n",
    "    df_model[\"evt_div_suspend\"] = (valid_dv & (dv == 0)).astype(\"int8\")\n",
    "    df_model[\"evt_div_cut\"]     = (valid_dv & (dv_ratio < cut_thr) & (dv > 0)).astype(\"int8\")\n",
    "    df_model[\"evt_div_init\"]    = ((df_model[\"dv_l1\"].fillna(0) == 0) & (dv > 0)).astype(\"int8\")\n",
    "else:\n",
    "    cut_thr = 0.75\n",
    "    df_model[\"evt_div_suspend\"] = 0\n",
    "    df_model[\"evt_div_cut\"]     = 0\n",
    "    df_model[\"evt_div_init\"]    = 0\n",
    "\n",
    "# --- CFO shocks ---\n",
    "if \"oancf\" in df_model.columns:\n",
    "    cfo = pd.to_numeric(df_model[\"oancf\"], errors=\"coerce\")\n",
    "    df_model[\"oancf_l1\"] = df_model.groupby(\"gvkey\")[\"oancf\"].shift(1)\n",
    "\n",
    "    df_model[\"evt_cfo_neg\"] = (cfo < 0).astype(\"int8\")\n",
    "\n",
    "    cfo_ratio = safe_divide(cfo, df_model[\"oancf_l1\"])\n",
    "    valid_cfo = df_model[\"oancf_l1\"] > 0\n",
    "    cfo_drop_q = cfo_ratio[train_mask & valid_cfo].quantile(0.05)\n",
    "    cfo_drop_thr = float(np.clip(cfo_drop_q, 0.10, 0.90))\n",
    "\n",
    "    df_model[\"evt_cfo_collapse\"] = (valid_cfo & (cfo_ratio < cfo_drop_thr)).astype(\"int8\")\n",
    "else:\n",
    "    cfo_drop_thr = 0.75\n",
    "    df_model[\"evt_cfo_neg\"] = 0\n",
    "    df_model[\"evt_cfo_collapse\"] = 0\n",
    "\n",
    "# --- FOCF shocks (oancf - capx) ---\n",
    "if \"focf\" in df_model.columns:\n",
    "    focf = pd.to_numeric(df_model[\"focf\"], errors=\"coerce\")\n",
    "    df_model[\"focf_l1\"] = df_model.groupby(\"gvkey\")[\"focf\"].shift(1)\n",
    "\n",
    "    df_model[\"evt_focf_neg\"] = (focf < 0).astype(\"int8\")\n",
    "\n",
    "    focf_ratio = safe_divide(focf, df_model[\"focf_l1\"])\n",
    "    valid_focf = df_model[\"focf_l1\"] > 0\n",
    "    focf_drop_q = focf_ratio[train_mask & valid_focf].quantile(0.05)\n",
    "    focf_drop_thr = float(np.clip(focf_drop_q, 0.10, 0.90))\n",
    "\n",
    "    df_model[\"evt_focf_collapse\"] = (valid_focf & (focf_ratio < focf_drop_thr)).astype(\"int8\")\n",
    "else:\n",
    "    focf_drop_thr = 0.75\n",
    "    df_model[\"evt_focf_neg\"] = 0\n",
    "    df_model[\"evt_focf_collapse\"] = 0\n",
    "\n",
    "# --- Liquidity drawdown: cash drop ---\n",
    "if \"che\" in df_model.columns:\n",
    "    che = pd.to_numeric(df_model[\"che\"], errors=\"coerce\")\n",
    "    df_model[\"che_l1\"] = df_model.groupby(\"gvkey\")[\"che\"].shift(1)\n",
    "\n",
    "    che_ratio = safe_divide(che, df_model[\"che_l1\"])\n",
    "    valid_che = df_model[\"che_l1\"] > 0\n",
    "    che_drop_q = che_ratio[train_mask & valid_che].quantile(0.05)\n",
    "    che_drop_thr = float(np.clip(che_drop_q, 0.10, 0.90))\n",
    "\n",
    "    df_model[\"evt_cash_drawdown\"] = (valid_che & (che_ratio < che_drop_thr)).astype(\"int8\")\n",
    "else:\n",
    "    che_drop_thr = 0.75\n",
    "    df_model[\"evt_cash_drawdown\"] = 0\n",
    "\n",
    "# --- Liquidity squeeze: current ratio deterioration (optional; non-proxy) ---\n",
    "if (\"act\" in df_model.columns) and (\"lct\" in df_model.columns):\n",
    "    act = pd.to_numeric(df_model[\"act\"], errors=\"coerce\")\n",
    "    lct = pd.to_numeric(df_model[\"lct\"], errors=\"coerce\")\n",
    "\n",
    "    df_model[\"current_ratio\"] = safe_divide(act, lct)\n",
    "    df_model[\"current_ratio_l1\"] = df_model.groupby(\"gvkey\")[\"current_ratio\"].shift(1)\n",
    "\n",
    "    cr_ratio = safe_divide(df_model[\"current_ratio\"], df_model[\"current_ratio_l1\"])\n",
    "    valid_cr = df_model[\"current_ratio_l1\"] > 0\n",
    "    cr_drop_q = cr_ratio[train_mask & valid_cr].quantile(0.05)\n",
    "    cr_drop_thr = float(np.clip(cr_drop_q, 0.10, 0.90))\n",
    "\n",
    "    df_model[\"evt_liquidity_squeeze\"] = (valid_cr & (cr_ratio < cr_drop_thr)).astype(\"int8\")\n",
    "else:\n",
    "    cr_drop_thr = 0.75\n",
    "    df_model[\"evt_liquidity_squeeze\"] = 0\n",
    "\n",
    "event_feats = [c for c in df_model.columns if c.startswith(\"evt_\")]\n",
    "print(f\"Event indicators included: {event_feats}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36bb9a99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:15.869134Z",
     "start_time": "2026-01-04T21:21:15.615611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: train=44,780 | val=6,415 | test=12,404\n",
      "Features: 15 (continuous=7 + events=8)\n",
      "Continuous feats: ['sp_debt_to_capital', 'sp_debt_to_ebitda', 'sp_ffo_to_debt', 'sp_cfo_to_debt', 'sp_focf_to_debt', 'log_at', 'log_mkvalt']\n",
      "Event feats: ['evt_div_suspend', 'evt_div_initiate', 'evt_div_cut', 'evt_cfo_neg', 'evt_cfo_collapse', 'evt_focf_neg', 'evt_focf_collapse', 'evt_cash_drawdown']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 5. Train / Validation / Test Split and Preprocessing\n",
    "# =============================================================================\n",
    "\n",
    "train = df_model[df_model[\"fyear\"] <= TRAIN_END_YEAR].copy()\n",
    "val   = df_model[df_model[\"fyear\"] == VAL_YEAR].copy()\n",
    "test  = df_model[df_model[\"fyear\"] >= TEST_START_YEAR].copy()\n",
    "\n",
    "TARGET_COL = \"target_next_year_distress\"\n",
    "\n",
    "# Core NON-proxy continuous channels (avoid mechanical overlap with distress proxy definition)\n",
    "continuous_feats = [c for c in [\"sp_cfo_to_debt\", \"sp_focf_to_debt\", \"sp_dcf_to_debt\"] if c in df_model.columns]\n",
    "\n",
    "# Market / size controls are explicitly allowed\n",
    "for opt in [\"log_at\", \"log_mkvalt\"]:\n",
    "    if opt in df_model.columns:\n",
    "        continuous_feats.append(opt)\n",
    "\n",
    "event_feats = [c for c in df_model.columns if c.startswith(\"evt_\")]\n",
    "\n",
    "MODEL_FEATS = continuous_feats + event_feats\n",
    "\n",
    "# --- Stabilize continuous inputs: train-fitted clipping + train-median imputation ---\n",
    "WINSOR_LO_Q = 0.01\n",
    "WINSOR_HI_Q = 0.99\n",
    "\n",
    "clip_bounds = {}\n",
    "train_medians = {}\n",
    "\n",
    "for col in continuous_feats:\n",
    "    s = pd.to_numeric(train[col], errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "    lo = float(s.quantile(WINSOR_LO_Q))\n",
    "    hi = float(s.quantile(WINSOR_HI_Q))\n",
    "    clip_bounds[col] = (lo, hi)\n",
    "    train_medians[col] = float(s.median())\n",
    "\n",
    "def clip_and_impute(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_out = df_in.copy()\n",
    "    for col in continuous_feats:\n",
    "        lo, hi = clip_bounds[col]\n",
    "        x = pd.to_numeric(df_out[col], errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "        x = x.clip(lower=lo, upper=hi)\n",
    "        df_out[col] = x.fillna(train_medians[col])\n",
    "    for col in event_feats:\n",
    "        df_out[col] = pd.to_numeric(df_out[col], errors=\"coerce\").fillna(0).astype(\"int8\")\n",
    "    return df_out\n",
    "\n",
    "train = clip_and_impute(train)\n",
    "val   = clip_and_impute(val)\n",
    "test  = clip_and_impute(test)\n",
    "\n",
    "# Standardize continuous features (train statistics)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train[continuous_feats])\n",
    "\n",
    "train.loc[:, continuous_feats] = scaler.transform(train[continuous_feats])\n",
    "val.loc[:, continuous_feats]   = scaler.transform(val[continuous_feats])\n",
    "test.loc[:, continuous_feats]  = scaler.transform(test[continuous_feats])\n",
    "\n",
    "print(f\"Split sizes: train={len(train):,} | val={len(val):,} | test={len(test):,}\")\n",
    "print(f\"Features: {len(MODEL_FEATS)} (continuous={len(continuous_feats)} + events={len(event_feats)})\")\n",
    "print(\"Continuous feats:\", continuous_feats)\n",
    "print(\"Event feats:\", event_feats)\n",
    "\n",
    "# Quick EDA: event prevalence in TRAIN\n",
    "if len(event_feats) > 0:\n",
    "    evt_prev = train[event_feats].mean().sort_values(ascending=False)\n",
    "    print(\"\\n=== Event prevalence (TRAIN) ===\")\n",
    "    display(evt_prev.to_frame(\"train_prevalence\").head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8fc19d35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:15.986169Z",
     "start_time": "2026-01-04T21:21:15.871150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation performance (Logit):\n",
      " AUC           0.6953\n",
      "AP            0.3729\n",
      "Brier         0.1260\n",
      "LogLoss       0.4123\n",
      "PosRate       0.1668\n",
      "N          6415.0000\n",
      "dtype: float64\n",
      "Test performance (Logit):\n",
      " AUC            0.6885\n",
      "AP             0.3725\n",
      "Brier          0.1365\n",
      "LogLoss        0.4384\n",
      "PosRate        0.1831\n",
      "N          12404.0000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 6. Logit Model (Benchmark)\n",
    "# =============================================================================\n",
    "\n",
    "X_train, y_train = train[MODEL_FEATS], train[TARGET_COL].astype(int)\n",
    "X_val, y_val     = val[MODEL_FEATS],   val[TARGET_COL].astype(int)\n",
    "X_test, y_test   = test[MODEL_FEATS],  test[TARGET_COL].astype(int)\n",
    "\n",
    "def evaluate_split(y_true, p_pred):\n",
    "    return pd.Series({\n",
    "        \"AUC\": roc_auc_score(y_true, p_pred),\n",
    "        \"AP\": average_precision_score(y_true, p_pred),\n",
    "        \"Brier\": brier_score_loss(y_true, p_pred),\n",
    "        \"LogLoss\": log_loss(y_true, p_pred),\n",
    "        \"PosRate\": float(np.mean(y_true)),\n",
    "        \"N\": int(len(y_true)),\n",
    "    })\n",
    "\n",
    "# --- Validation-tuned regularization (out-of-time) ---\n",
    "C_grid = [0.01, 0.1, 1.0, 10.0]\n",
    "tune_rows = []\n",
    "\n",
    "for C in C_grid:\n",
    "    clf = LogisticRegression(C=C, max_iter=2000, solver=\"lbfgs\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    p = clf.predict_proba(X_val)[:, 1]\n",
    "    m = evaluate_split(y_val, p)\n",
    "    tune_rows.append(pd.concat([pd.Series({\"C\": C}), m]))\n",
    "\n",
    "tune_df = pd.DataFrame(tune_rows).sort_values(\"AUC\", ascending=False).reset_index(drop=True)\n",
    "print(\"\\n=== Logit tuning (choose by VAL AUC) ===\")\n",
    "display(tune_df)\n",
    "\n",
    "best_C = float(tune_df.loc[0, \"C\"])\n",
    "logit = LogisticRegression(C=best_C, max_iter=2000, solver=\"lbfgs\")\n",
    "logit.fit(X_train, y_train)\n",
    "\n",
    "p_val  = logit.predict_proba(X_val)[:, 1]\n",
    "p_test = logit.predict_proba(X_test)[:, 1]\n",
    "\n",
    "eval_val  = evaluate_split(y_val, p_val)\n",
    "eval_test = evaluate_split(y_test, p_test)\n",
    "\n",
    "print(f\"\\nChosen C={best_C}\")\n",
    "print(\"\\nValidation performance (Logit):\\n\", eval_val.round(4))\n",
    "print(\"\\nTest performance (Logit):\\n\", eval_test.round(4))\n",
    "\n",
    "# --- Coefficient audit (predictive; not causal) ---\n",
    "coef = pd.Series(logit.coef_.ravel(), index=MODEL_FEATS).sort_values(key=lambda s: s.abs(), ascending=False)\n",
    "print(\"\\nTop |coefficients| (Logit):\")\n",
    "display(coef.head(25).to_frame(\"coef\"))\n",
    "\n",
    "# --- Inference audit via statsmodels Logit with firm-clustered SEs ---\n",
    "# Purpose: descriptive stability check under within-firm dependence (NOT causal inference).\n",
    "try:\n",
    "    X_sm = sm.add_constant(train[MODEL_FEATS], has_constant=\"add\")\n",
    "    res = sm.Logit(y_train, X_sm).fit(disp=False, maxiter=200)\n",
    "    res_cl = res.get_robustcov_results(cov_type=\"cluster\", groups=train[\"gvkey\"])\n",
    "    summ = res_cl.summary2().tables[1].copy()\n",
    "    # Keep a compact view for the notebook\n",
    "    keep_cols = [c for c in [\"Coef.\", \"Std.Err.\", \"z\", \"P>|z|\"] if c in summ.columns]\n",
    "    print(\"\\n=== Statsmodels logit (firm-clustered SE) — coefficient table ===\")\n",
    "    display(summ[keep_cols].sort_values(\"Coef.\", key=lambda s: s.abs(), ascending=False).head(30))\n",
    "except Exception as e:\n",
    "    print(\"Statsmodels inference audit skipped (convergence/collinearity). Error:\", repr(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17e814f",
   "metadata": {},
   "source": [
    "## 6.1 Logit benchmark with current-state (Task A: surveillance)\n",
    "\n",
    "This reports the incremental value of accounting/market features **conditional on the current distress state** (persistence benchmark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e640975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Add current-state as an additional predictor (surveillance use-case)\n",
    "state_train = pd.to_numeric(train.get(\"distress_dummy\"), errors=\"coerce\").fillna(0.0).clip(0, 1).to_numpy(float)\n",
    "state_val   = pd.to_numeric(val.get(\"distress_dummy\"), errors=\"coerce\").fillna(0.0).clip(0, 1).to_numpy(float)\n",
    "state_test  = pd.to_numeric(test.get(\"distress_dummy\"), errors=\"coerce\").fillna(0.0).clip(0, 1).to_numpy(float)\n",
    "\n",
    "X_train_state = np.column_stack([X_train.to_numpy(float), state_train])\n",
    "X_val_state   = np.column_stack([X_val.to_numpy(float),   state_val])\n",
    "X_test_state  = np.column_stack([X_test.to_numpy(float),  state_test])\n",
    "\n",
    "C_grid_state = [0.01, 0.1, 1.0, 10.0]\n",
    "best_state = {\"C\": None, \"auc\": -np.inf, \"model\": None}\n",
    "\n",
    "for C in C_grid_state:\n",
    "    m = LogisticRegression(C=C, solver=\"lbfgs\", max_iter=3000, class_weight=\"balanced\", random_state=42)\n",
    "    m.fit(X_train_state, y_train)\n",
    "    pv = m.predict_proba(X_val_state)[:, 1]\n",
    "    auc = roc_auc_score(y_val, pv) if len(np.unique(y_val)) > 1 else np.nan\n",
    "    if np.isfinite(auc) and auc > best_state[\"auc\"]:\n",
    "        best_state = {\"C\": C, \"auc\": float(auc), \"model\": m}\n",
    "\n",
    "logit_state = best_state[\"model\"]\n",
    "p_val_logit_state  = logit_state.predict_proba(X_val_state)[:, 1]\n",
    "p_test_logit_state = logit_state.predict_proba(X_test_state)[:, 1]\n",
    "\n",
    "print(\"Best Logit(feats+state) C:\", best_state[\"C\"], \"| VAL AUC:\", round(best_state[\"auc\"], 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfc47a7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:17.284760Z",
     "start_time": "2026-01-04T21:21:15.988224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation performance (XGB):\n",
      " AUC           0.9068\n",
      "AP            0.7292\n",
      "Brier         0.0767\n",
      "LogLoss       0.2603\n",
      "PosRate       0.1668\n",
      "N          6415.0000\n",
      "dtype: float64\n",
      "Test performance (XGB):\n",
      " AUC            0.9112\n",
      "AP             0.7540\n",
      "Brier          0.0788\n",
      "LogLoss        0.2671\n",
      "PosRate        0.1831\n",
      "N          12404.0000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 7. Tree-based Model (XGBoost)\n",
    "# =============================================================================\n",
    "\n",
    "xgb_params = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"aucpr\",\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    n_estimators=500,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(**xgb_params)\n",
    "xgb_clf.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "p_val_xgb = xgb_clf.predict_proba(X_val)[:, 1]\n",
    "p_test_xgb = xgb_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "eval_val_xgb = evaluate_split(y_val, p_val_xgb)\n",
    "eval_test_xgb = evaluate_split(y_test, p_test_xgb)\n",
    "\n",
    "print(\"Validation performance (XGB):\\n\", eval_val_xgb.round(4))\n",
    "print(\"Test performance (XGB):\\n\", eval_test_xgb.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "419eee4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:18.282903Z",
     "start_time": "2026-01-04T21:21:17.380987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute SHAP contributions (validation):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sp_debt_to_capital    1.133036\n",
       "sp_ffo_to_debt        0.527658\n",
       "log_at                0.275150\n",
       "sp_debt_to_ebitda     0.186086\n",
       "log_mkvalt            0.145138\n",
       "sp_focf_to_debt       0.142577\n",
       "sp_cfo_to_debt        0.136545\n",
       "evt_cfo_neg           0.071492\n",
       "evt_focf_neg          0.054369\n",
       "evt_cfo_collapse      0.019340\n",
       "evt_cash_drawdown     0.014455\n",
       "evt_div_cut           0.007996\n",
       "evt_focf_collapse     0.005306\n",
       "evt_div_suspend       0.003284\n",
       "evt_div_initiate      0.002192\n",
       "dtype: float32"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 8. Explainability (TreeSHAP via XGBoost pred_contribs)\n",
    "# =============================================================================\n",
    "\n",
    "booster = xgb_clf.get_booster()\n",
    "dval = xgb.DMatrix(X_val, feature_names=MODEL_FEATS)\n",
    "\n",
    "# pred_contribs=True returns SHAP contributions per feature plus a bias term (last column)\n",
    "shap_val = booster.predict(dval, pred_contribs=True)\n",
    "shap_cols = MODEL_FEATS + [\"bias\"]\n",
    "shap_df = pd.DataFrame(shap_val, columns=shap_cols)\n",
    "\n",
    "abs_mean = shap_df[MODEL_FEATS].abs().mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"Mean absolute SHAP contributions (validation):\")\n",
    "display(abs_mean.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6df650c",
   "metadata": {},
   "source": [
    "## 7.1 Explainability extension: event-toggle ΔPD (local business lever effects)\n",
    "\n",
    "For each event binary, compute the **average change in predicted PD** on VAL when toggling the event (0→1 and 1→0), holding other features fixed. This complements SHAP by giving an interpretable *policy lever* effect size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56303cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event toggle ΔPD table (on VAL)\n",
    "# Uses the already-fit xgb_clf and feature matrix X_val.\n",
    "\n",
    "if len(event_feats) == 0:\n",
    "    print(\"No event features available for toggle analysis.\")\n",
    "else:\n",
    "    Xv = X_val.copy()\n",
    "    base_p = xgb_clf.predict_proba(Xv)[:, 1]\n",
    "\n",
    "    rows = []\n",
    "    for e in event_feats:\n",
    "        if e not in Xv.columns:\n",
    "            continue\n",
    "        x0 = Xv.copy()\n",
    "        x1 = Xv.copy()\n",
    "        x0[e] = 0\n",
    "        x1[e] = 1\n",
    "\n",
    "        p0 = xgb_clf.predict_proba(x0)[:, 1]\n",
    "        p1 = xgb_clf.predict_proba(x1)[:, 1]\n",
    "\n",
    "        # Conditional averages for interpretability\n",
    "        m0 = (Xv[e].to_numpy() == 0)\n",
    "        m1 = (Xv[e].to_numpy() == 1)\n",
    "\n",
    "        delta_0_to_1 = float(np.mean((p1 - base_p)[m0])) if np.any(m0) else np.nan\n",
    "        delta_1_to_0 = float(np.mean((p0 - base_p)[m1])) if np.any(m1) else np.nan\n",
    "\n",
    "        rows.append({\n",
    "            \"event\": e,\n",
    "            \"prevalence_val\": float(np.mean(Xv[e])),\n",
    "            \"ΔPD if toggle 0→1 (among zeros)\": delta_0_to_1,\n",
    "            \"ΔPD if toggle 1→0 (among ones)\": delta_1_to_0,\n",
    "        })\n",
    "\n",
    "    toggle_tbl = pd.DataFrame(rows).sort_values(\"prevalence_val\", ascending=False)\n",
    "    display(toggle_tbl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dbd538d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:18.787694Z",
     "start_time": "2026-01-04T21:21:18.386795Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Split</th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC</th>\n",
       "      <th>AP</th>\n",
       "      <th>Brier</th>\n",
       "      <th>LogLoss</th>\n",
       "      <th>PosRate</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VAL</td>\n",
       "      <td>Persistence</td>\n",
       "      <td>0.806236</td>\n",
       "      <td>0.528628</td>\n",
       "      <td>0.100968</td>\n",
       "      <td>0.699751</td>\n",
       "      <td>0.166797</td>\n",
       "      <td>6415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VAL</td>\n",
       "      <td>Logit</td>\n",
       "      <td>0.695316</td>\n",
       "      <td>0.372922</td>\n",
       "      <td>0.126003</td>\n",
       "      <td>0.412337</td>\n",
       "      <td>0.166797</td>\n",
       "      <td>6415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VAL</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.906799</td>\n",
       "      <td>0.729185</td>\n",
       "      <td>0.076717</td>\n",
       "      <td>0.260257</td>\n",
       "      <td>0.166797</td>\n",
       "      <td>6415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST</td>\n",
       "      <td>Persistence</td>\n",
       "      <td>0.816405</td>\n",
       "      <td>0.569372</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.692567</td>\n",
       "      <td>0.183086</td>\n",
       "      <td>12404.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST</td>\n",
       "      <td>Logit</td>\n",
       "      <td>0.688549</td>\n",
       "      <td>0.372482</td>\n",
       "      <td>0.136455</td>\n",
       "      <td>0.438371</td>\n",
       "      <td>0.183086</td>\n",
       "      <td>12404.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TEST</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.911201</td>\n",
       "      <td>0.754015</td>\n",
       "      <td>0.078791</td>\n",
       "      <td>0.267070</td>\n",
       "      <td>0.183086</td>\n",
       "      <td>12404.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Split        Model       AUC        AP     Brier   LogLoss   PosRate  \\\n",
       "0   VAL  Persistence  0.806236  0.528628  0.100968  0.699751  0.166797   \n",
       "1   VAL        Logit  0.695316  0.372922  0.126003  0.412337  0.166797   \n",
       "2   VAL          XGB  0.906799  0.729185  0.076717  0.260257  0.166797   \n",
       "3  TEST  Persistence  0.816405  0.569372  0.099930  0.692567  0.183086   \n",
       "4  TEST        Logit  0.688549  0.372482  0.136455  0.438371  0.183086   \n",
       "5  TEST          XGB  0.911201  0.754015  0.078791  0.267070  0.183086   \n",
       "\n",
       "         N  \n",
       "0   6415.0  \n",
       "1   6415.0  \n",
       "2   6415.0  \n",
       "3  12404.0  \n",
       "4  12404.0  \n",
       "5  12404.0  "
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subset</th>\n",
       "      <th>N</th>\n",
       "      <th>MeanPD</th>\n",
       "      <th>MedianPD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VAL: 0→1 transitions (XGB PD)</td>\n",
       "      <td>356</td>\n",
       "      <td>0.183557</td>\n",
       "      <td>0.139482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST: 0→1 transitions (XGB PD)</td>\n",
       "      <td>716</td>\n",
       "      <td>0.203632</td>\n",
       "      <td>0.162034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Subset    N    MeanPD  MedianPD\n",
       "0   VAL: 0→1 transitions (XGB PD)  356  0.183557  0.139482\n",
       "1  TEST: 0→1 transitions (XGB PD)  716  0.203632  0.162034"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 9. Evaluation: Persistence vs. Early Warning\n",
    "# =============================================================================\n",
    "\n",
    "eps = 1e-3\n",
    "p_persist_val  = np.clip(val[\"distress_dummy\"].astype(float),  eps, 1 - eps)\n",
    "p_persist_test = np.clip(test[\"distress_dummy\"].astype(float), eps, 1 - eps)\n",
    "\n",
    "benchmarks = pd.DataFrame([\n",
    "    [\"VAL\",  \"Persistence\", *evaluate_split(y_val,  p_persist_val)],\n",
    "    [\"VAL\",  \"Logit\",       *eval_val],\n",
    "    [\"VAL\",  \"XGB\",         *eval_val_xgb],\n",
    "    [\"TEST\", \"Persistence\", *evaluate_split(y_test, p_persist_test)],\n",
    "    [\"TEST\", \"Logit\",       *eval_test],\n",
    "    [\"TEST\", \"XGB\",         *eval_test_xgb],\n",
    "], columns=[\"Split\", \"Model\", \"AUC\", \"AP\", \"Brier\", \"LogLoss\", \"PosRate\", \"N\"])\n",
    "\n",
    "display(benchmarks)\n",
    "\n",
    "# Early-warning subset: not distressed at t but distressed at t+1 (0→1 transitions)\n",
    "val_ew  = (val[\"distress_dummy\"] == 0) & (val[TARGET_COL] == 1)\n",
    "test_ew = (test[\"distress_dummy\"] == 0) & (test[TARGET_COL] == 1)\n",
    "\n",
    "def ew_summary(mask, p_pred, label):\n",
    "    mask = mask.fillna(False)\n",
    "    if int(mask.sum()) == 0:\n",
    "        return pd.Series({\"Subset\": label, \"N\": 0, \"MeanPD\": np.nan, \"MedianPD\": np.nan})\n",
    "    return pd.Series({\n",
    "        \"Subset\": label,\n",
    "        \"N\": int(mask.sum()),\n",
    "        \"MeanPD\": float(np.mean(p_pred[mask.values])),\n",
    "        \"MedianPD\": float(np.median(p_pred[mask.values])),\n",
    "    })\n",
    "\n",
    "ew = pd.DataFrame([\n",
    "    ew_summary(val_ew,  p_val,      \"VAL: 0→1 transitions (Logit PD)\"),\n",
    "    ew_summary(val_ew,  p_val_xgb,  \"VAL: 0→1 transitions (XGB PD)\"),\n",
    "    ew_summary(test_ew, p_test,     \"TEST: 0→1 transitions (Logit PD)\"),\n",
    "    ew_summary(test_ew, p_test_xgb, \"TEST: 0→1 transitions (XGB PD)\"),\n",
    "])\n",
    "\n",
    "print(\"\\n=== Early-warning PD levels on 0→1 transitions ===\")\n",
    "display(ew)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c04528",
   "metadata": {},
   "source": [
    "## 8.1 Task-specific benchmarks (restore journal-grade baselines)\n",
    "\n",
    "Task A (Surveillance): compare to persistence baseline.\n",
    "\n",
    "Task B (Early warning): restrict to distress_dummy(t)=0 and compare to Always-0 and constant transition-rate baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87738e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, log_loss\n",
    "\n",
    "def _eval(y_true, p):\n",
    "    return {\n",
    "        \"AUC\": float(roc_auc_score(y_true, p)) if len(np.unique(y_true)) > 1 else np.nan,\n",
    "        \"AP\": float(average_precision_score(y_true, p)) if len(np.unique(y_true)) > 1 else np.nan,\n",
    "        \"Brier\": float(brier_score_loss(y_true, p)),\n",
    "        \"LogLoss\": float(log_loss(y_true, p)),\n",
    "        \"PosRate\": float(np.mean(y_true)),\n",
    "        \"N\": int(len(y_true)),\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Task A: surveillance\n",
    "# -------------------------\n",
    "eps = 1e-3\n",
    "p_val_persist  = np.clip(pd.to_numeric(val[\"distress_dummy\"], errors=\"coerce\").fillna(0).to_numpy(float),  eps, 1-eps)\n",
    "p_test_persist = np.clip(pd.to_numeric(test[\"distress_dummy\"], errors=\"coerce\").fillna(0).to_numpy(float), eps, 1-eps)\n",
    "\n",
    "taskA_rows = [\n",
    "    {\"Task\":\"A (Surveillance)\", \"Split\":\"VAL\",  \"Model\":\"Persistence\", **_eval(y_val, p_val_persist)},\n",
    "    {\"Task\":\"A (Surveillance)\", \"Split\":\"VAL\",  \"Model\":\"Logit (feats-only)\", **_eval(y_val, p_val_logit)},\n",
    "    {\"Task\":\"A (Surveillance)\", \"Split\":\"VAL\",  \"Model\":\"Logit (feats+state)\", **_eval(y_val, p_val_logit_state)},\n",
    "    {\"Task\":\"A (Surveillance)\", \"Split\":\"VAL\",  \"Model\":\"XGBoost\", **_eval(y_val, p_val_xgb)},\n",
    "    {\"Task\":\"A (Surveillance)\", \"Split\":\"TEST\", \"Model\":\"Persistence\", **_eval(y_test, p_test_persist)},\n",
    "    {\"Task\":\"A (Surveillance)\", \"Split\":\"TEST\", \"Model\":\"Logit (feats-only)\", **_eval(y_test, p_test_logit)},\n",
    "    {\"Task\":\"A (Surveillance)\", \"Split\":\"TEST\", \"Model\":\"Logit (feats+state)\", **_eval(y_test, p_test_logit_state)},\n",
    "    {\"Task\":\"A (Surveillance)\", \"Split\":\"TEST\", \"Model\":\"XGBoost\", **_eval(y_test, p_test_xgb)},\n",
    "]\n",
    "taskA_tbl = pd.DataFrame(taskA_rows)\n",
    "display(taskA_tbl)\n",
    "\n",
    "# -------------------------\n",
    "# Task B: early warning (transition into distress)\n",
    "# restrict to distress_dummy(t)=0\n",
    "# -------------------------\n",
    "valB  = val[pd.to_numeric(val[\"distress_dummy\"], errors=\"coerce\").fillna(0).astype(int) == 0].copy()\n",
    "testB = test[pd.to_numeric(test[\"distress_dummy\"], errors=\"coerce\").fillna(0).astype(int) == 0].copy()\n",
    "\n",
    "X_valB  = valB[MODEL_FEATS]\n",
    "y_valB  = valB[TARGET_COL].astype(int).to_numpy()\n",
    "X_testB = testB[MODEL_FEATS]\n",
    "y_testB = testB[TARGET_COL].astype(int).to_numpy()\n",
    "\n",
    "# baselines\n",
    "p_valB_zero  = np.zeros_like(y_valB, dtype=float) + 1e-6\n",
    "p_testB_zero = np.zeros_like(y_testB, dtype=float) + 1e-6\n",
    "\n",
    "# constant transition rate fitted on TRAIN non-distressed\n",
    "trainB = train[pd.to_numeric(train[\"distress_dummy\"], errors=\"coerce\").fillna(0).astype(int) == 0].copy()\n",
    "rateB = float(trainB[TARGET_COL].mean()) if len(trainB) else float(np.mean(y_train))\n",
    "p_valB_rate  = np.full_like(y_valB, rateB, dtype=float)\n",
    "p_testB_rate = np.full_like(y_testB, rateB, dtype=float)\n",
    "\n",
    "# model (feats-only) predictions on B subsets\n",
    "p_valB_logit  = logit.predict_proba(X_valB)[:, 1]\n",
    "p_testB_logit = logit.predict_proba(X_testB)[:, 1]\n",
    "p_valB_xgb    = xgb_clf.predict_proba(X_valB)[:, 1]\n",
    "p_testB_xgb   = xgb_clf.predict_proba(X_testB)[:, 1]\n",
    "\n",
    "taskB_rows = [\n",
    "    {\"Task\":\"B (Early warning)\", \"Split\":\"VAL\",  \"Model\":\"Always-0\", **_eval(y_valB, p_valB_zero)},\n",
    "    {\"Task\":\"B (Early warning)\", \"Split\":\"VAL\",  \"Model\":f\"Const rate={rateB:.4f}\", **_eval(y_valB, p_valB_rate)},\n",
    "    {\"Task\":\"B (Early warning)\", \"Split\":\"VAL\",  \"Model\":\"Logit (feats-only)\", **_eval(y_valB, p_valB_logit)},\n",
    "    {\"Task\":\"B (Early warning)\", \"Split\":\"VAL\",  \"Model\":\"XGBoost\", **_eval(y_valB, p_valB_xgb)},\n",
    "    {\"Task\":\"B (Early warning)\", \"Split\":\"TEST\", \"Model\":\"Always-0\", **_eval(y_testB, p_testB_zero)},\n",
    "    {\"Task\":\"B (Early warning)\", \"Split\":\"TEST\", \"Model\":f\"Const rate={rateB:.4f}\", **_eval(y_testB, p_testB_rate)},\n",
    "    {\"Task\":\"B (Early warning)\", \"Split\":\"TEST\", \"Model\":\"Logit (feats-only)\", **_eval(y_testB, p_testB_logit)},\n",
    "    {\"Task\":\"B (Early warning)\", \"Split\":\"TEST\", \"Model\":\"XGBoost\", **_eval(y_testB, p_testB_xgb)},\n",
    "]\n",
    "taskB_tbl = pd.DataFrame(taskB_rows)\n",
    "display(taskB_tbl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f0e9b5",
   "metadata": {},
   "source": [
    "## 8.2 Temporal stability (year-by-year performance on TEST)\n",
    "\n",
    "Finance-journal expectation: performance should be reported across calendar years to diagnose drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1e9af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_by_year(df_split: pd.DataFrame, p: np.ndarray, y_col: str):\n",
    "    out = []\n",
    "    for y in sorted(df_split[\"fyear\"].unique()):\n",
    "        m = (df_split[\"fyear\"] == y)\n",
    "        yy = df_split.loc[m, y_col].astype(int).to_numpy()\n",
    "        pp = p[m]\n",
    "        if len(yy) == 0:\n",
    "            continue\n",
    "        out.append({\n",
    "            \"fyear\": int(y),\n",
    "            **_eval(yy, pp)\n",
    "        })\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "# Ensure arrays align with 'test' index order (they do if computed from test[MODEL_FEATS])\n",
    "st_persist = year_by_year(test, p_test_persist, TARGET_COL).rename(columns={\"AUC\":\"AUC_persist\",\"AP\":\"AP_persist\",\"Brier\":\"Brier_persist\",\"LogLoss\":\"LL_persist\"})\n",
    "st_logit   = year_by_year(test, p_test_logit, TARGET_COL).rename(columns={\"AUC\":\"AUC_logit\",\"AP\":\"AP_logit\",\"Brier\":\"Brier_logit\",\"LogLoss\":\"LL_logit\"})\n",
    "st_xgb     = year_by_year(test, p_test_xgb, TARGET_COL).rename(columns={\"AUC\":\"AUC_xgb\",\"AP\":\"AP_xgb\",\"Brier\":\"Brier_xgb\",\"LogLoss\":\"LL_xgb\"})\n",
    "\n",
    "st = st_persist[[\"fyear\",\"AUC_persist\",\"AP_persist\",\"Brier_persist\",\"LL_persist\",\"PosRate\",\"N\"]].merge(\n",
    "    st_logit[[\"fyear\",\"AUC_logit\",\"AP_logit\",\"Brier_logit\",\"LL_logit\"]], on=\"fyear\", how=\"left\"\n",
    ").merge(\n",
    "    st_xgb[[\"fyear\",\"AUC_xgb\",\"AP_xgb\",\"Brier_xgb\",\"LL_xgb\"]], on=\"fyear\", how=\"left\"\n",
    ")\n",
    "\n",
    "display(st)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559d062a",
   "metadata": {},
   "source": [
    "## 8.3 Calibration (Platt + Isotonic on VAL)\n",
    "\n",
    "Restore probability calibration to make PDs decision-usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563e1cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Calibrate XGBoost using VAL (prefit)\n",
    "cal_platt = CalibratedClassifierCV(xgb_clf, method=\"sigmoid\", cv=\"prefit\")\n",
    "cal_platt.fit(X_val, y_val)\n",
    "p_test_xgb_platt = cal_platt.predict_proba(X_test)[:, 1]\n",
    "\n",
    "cal_iso = CalibratedClassifierCV(xgb_clf, method=\"isotonic\", cv=\"prefit\")\n",
    "cal_iso.fit(X_val, y_val)\n",
    "p_test_xgb_iso = cal_iso.predict_proba(X_test)[:, 1]\n",
    "\n",
    "cal_tbl = pd.DataFrame([\n",
    "    {\"Split\":\"TEST\",\"Model\":\"XGB (raw)\", **_eval(y_test, p_test_xgb)},\n",
    "    {\"Split\":\"TEST\",\"Model\":\"XGB + Platt\", **_eval(y_test, p_test_xgb_platt)},\n",
    "    {\"Split\":\"TEST\",\"Model\":\"XGB + Isotonic\", **_eval(y_test, p_test_xgb_iso)},\n",
    "])\n",
    "display(cal_tbl)\n",
    "\n",
    "# Reliability plot (TEST)\n",
    "plt.figure(figsize=(6, 5))\n",
    "for name, p in [(\"raw\", p_test_xgb), (\"platt\", p_test_xgb_platt), (\"isotonic\", p_test_xgb_iso)]:\n",
    "    frac_pos, mean_pred = calibration_curve(y_test, p, n_bins=10, strategy=\"quantile\")\n",
    "    plt.plot(mean_pred, frac_pos, marker=\"o\", label=name)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"perfect\")\n",
    "plt.xlabel(\"Mean predicted PD\")\n",
    "plt.ylabel(\"Empirical event rate\")\n",
    "plt.title(\"Calibration curve (TEST)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d653f58",
   "metadata": {},
   "source": [
    "## 8.4 Operating-point policy (threshold selection + confusion matrices)\n",
    "\n",
    "Provide action-oriented operating points (precision/recall tradeoffs) on VAL and report out-of-sample performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e9cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
    "\n",
    "# Choose score source for policy: prefer calibrated XGB if available\n",
    "score_val = p_val_xgb\n",
    "score_test = p_test_xgb\n",
    "score_name = \"XGB (raw)\"\n",
    "\n",
    "if \"p_test_xgb_platt\" in globals():\n",
    "    score_test = p_test_xgb_platt\n",
    "    # for VAL, calibrator is fit on VAL; to avoid circularity, use raw scores for threshold selection OR use nested CV.\n",
    "    # Minimal: select threshold on raw VAL, apply to calibrated TEST for policy demonstration.\n",
    "    score_name = \"XGB (Platt TEST)\"\n",
    "\n",
    "# Threshold sweep on VAL (raw XGB scores)\n",
    "prec, rec, thr = precision_recall_curve(y_val, p_val_xgb)\n",
    "f1 = (2 * prec * rec) / (prec + rec + 1e-12)\n",
    "best_i = int(np.nanargmax(f1))\n",
    "thr_star = float(thr[max(best_i - 1, 0)]) if len(thr) else 0.5\n",
    "\n",
    "print(\"Chosen threshold (max F1 on VAL, raw XGB):\", round(thr_star, 4))\n",
    "\n",
    "def cm_report(y, p, t):\n",
    "    pred = (p >= t).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
    "    return {\n",
    "        \"threshold\": float(t),\n",
    "        \"TP\": int(tp), \"FP\": int(fp), \"TN\": int(tn), \"FN\": int(fn),\n",
    "        \"precision\": float(precision_score(y, pred, zero_division=0)),\n",
    "        \"recall\": float(recall_score(y, pred, zero_division=0)),\n",
    "        \"f1\": float(f1_score(y, pred, zero_division=0)),\n",
    "        \"flag_rate\": float(np.mean(pred)),\n",
    "    }\n",
    "\n",
    "val_rep = cm_report(y_val, p_val_xgb, thr_star)\n",
    "test_rep = cm_report(y_test, score_test, thr_star)\n",
    "\n",
    "display(pd.DataFrame([\n",
    "    {\"Split\":\"VAL\",  \"Model\":\"XGB (raw)\", **val_rep},\n",
    "    {\"Split\":\"TEST\", \"Model\":score_name, **test_rep},\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca849b",
   "metadata": {},
   "source": [
    "## 8.5 Decision curves (net benefit)\n",
    "\n",
    "Restore decision-analytic evaluation for screening/triage policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e42faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_curve(y, p, thresholds):\n",
    "    y = np.asarray(y)\n",
    "    p = np.asarray(p)\n",
    "    n = len(y)\n",
    "    out = []\n",
    "    for pt in thresholds:\n",
    "        pred = (p >= pt).astype(int)\n",
    "        tp = np.sum((pred == 1) & (y == 1))\n",
    "        fp = np.sum((pred == 1) & (y == 0))\n",
    "        nb = (tp / n) - (fp / n) * (pt / (1 - pt))\n",
    "        out.append((pt, nb))\n",
    "    return pd.DataFrame(out, columns=[\"threshold\", \"net_benefit\"])\n",
    "\n",
    "ths = np.linspace(0.01, 0.50, 50)\n",
    "\n",
    "# Use calibrated test scores if available, else raw\n",
    "p_test_policy = p_test_xgb_platt if \"p_test_xgb_platt\" in globals() else p_test_xgb\n",
    "dc_xgb = decision_curve(y_test, p_test_policy, ths)\n",
    "dc_persist = decision_curve(y_test, p_test_persist, ths)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(dc_xgb[\"threshold\"], dc_xgb[\"net_benefit\"], label=\"XGB (policy score)\")\n",
    "plt.plot(dc_persist[\"threshold\"], dc_persist[\"net_benefit\"], label=\"Persistence\")\n",
    "plt.axhline(0.0, linestyle=\"--\", label=\"Treat none\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Net benefit\")\n",
    "plt.title(\"Decision curve (TEST, Task A)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa9c9eda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:19.160416Z",
     "start_time": "2026-01-04T21:21:18.850641Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scenario</th>\n",
       "      <th>PD</th>\n",
       "      <th>ΔPD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Base</td>\n",
       "      <td>0.194363</td>\n",
       "      <td>0.139428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dividend suspension (dv=0)</td>\n",
       "      <td>0.194363</td>\n",
       "      <td>0.139428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CFO shock (-30%)</td>\n",
       "      <td>0.195867</td>\n",
       "      <td>0.140932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Capex surge (+25%)</td>\n",
       "      <td>0.191345</td>\n",
       "      <td>0.136410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cash drawdown (-20%)</td>\n",
       "      <td>0.194363</td>\n",
       "      <td>0.139428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Scenario        PD       ΔPD\n",
       "0                        Base  0.194363  0.139428\n",
       "1  Dividend suspension (dv=0)  0.194363  0.139428\n",
       "2            CFO shock (-30%)  0.195867  0.140932\n",
       "3          Capex surge (+25%)  0.191345  0.136410\n",
       "4        Cash drawdown (-20%)  0.194363  0.139428"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 10. Decision Support and Scenario Analysis (non-proxy shocks)\n",
    "# =============================================================================\n",
    "# Requirements:\n",
    "#   - No deleveraging / leverage-ratio shocks (proxy-mechanical)\n",
    "#   - No coverage shocks\n",
    "#   - Scenarios must propagate through engineered features and event indicators\n",
    "#\n",
    "# We implement \"primitive shocks\" (dv, oancf, capx, che) and RECOMPUTE:\n",
    "#   - continuous ratios (sp_cfo_to_debt, focf, sp_focf_to_debt, dcf, sp_dcf_to_debt)\n",
    "#   - event indicators that depend on current vs lag values (stored in *_l1 columns)\n",
    "\n",
    "EVENT_THRESHOLDS = {\n",
    "    \"div_cut_thr\": float(locals().get(\"cut_thr\", 0.75)),\n",
    "    \"cfo_drop_thr\": float(locals().get(\"cfo_drop_thr\", 0.75)),\n",
    "    \"focf_drop_thr\": float(locals().get(\"focf_drop_thr\", 0.75)),\n",
    "    \"cash_drop_thr\": float(locals().get(\"che_drop_thr\", 0.75)),\n",
    "    \"cr_drop_thr\": float(locals().get(\"cr_drop_thr\", 0.75)),\n",
    "}\n",
    "\n",
    "def recompute_features_for_rows(df_rows: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df_rows.copy()\n",
    "\n",
    "    # Recompute flows\n",
    "    out[\"oancf\"] = pd.to_numeric(out.get(\"oancf\"), errors=\"coerce\")\n",
    "    out[\"capx\"]  = pd.to_numeric(out.get(\"capx\"), errors=\"coerce\") if \"capx\" in out.columns else np.nan\n",
    "    out[\"focf\"]  = out[\"oancf\"] - out[\"capx\"]\n",
    "\n",
    "    dv = pd.to_numeric(out.get(\"dv\"), errors=\"coerce\") if \"dv\" in out.columns else pd.Series(np.nan, index=out.index)\n",
    "    prstkc = pd.to_numeric(out.get(\"prstkc\"), errors=\"coerce\") if \"prstkc\" in out.columns else pd.Series(np.nan, index=out.index)\n",
    "    dv0 = dv.fillna(0.0)\n",
    "    prstkc0 = prstkc.fillna(0.0)\n",
    "    out[\"dcf\"] = out[\"focf\"] - dv0 - prstkc0\n",
    "\n",
    "    # Ratios (NON-proxy channels)\n",
    "    td = pd.to_numeric(out.get(\"total_debt\"), errors=\"coerce\")\n",
    "    td_pos = td.notna() & (td > 0)\n",
    "    out[\"sp_cfo_to_debt\"]  = np.where(td_pos, safe_divide(out[\"oancf\"], td), np.nan)\n",
    "    out[\"sp_focf_to_debt\"] = np.where(td_pos, safe_divide(out[\"focf\"], td), np.nan)\n",
    "    out[\"sp_dcf_to_debt\"]  = np.where(td_pos, safe_divide(out[\"dcf\"], td), np.nan)\n",
    "\n",
    "    # Dividend events (need dv_l1)\n",
    "    if \"dv\" in out.columns and \"dv_l1\" in out.columns:\n",
    "        dv_l1 = pd.to_numeric(out[\"dv_l1\"], errors=\"coerce\")\n",
    "        dv_ratio = safe_divide(dv, dv_l1)\n",
    "        valid_dv = dv_l1 > 0\n",
    "\n",
    "        out[\"evt_div_suspend\"] = (valid_dv & (dv == 0)).astype(\"int8\")\n",
    "        out[\"evt_div_cut\"]     = (valid_dv & (dv_ratio < EVENT_THRESHOLDS[\"div_cut_thr\"]) & (dv > 0)).astype(\"int8\")\n",
    "        out[\"evt_div_init\"]    = ((dv_l1.fillna(0) == 0) & (dv > 0)).astype(\"int8\")\n",
    "\n",
    "    # CFO events (need oancf_l1)\n",
    "    if \"oancf_l1\" in out.columns:\n",
    "        cfo_l1 = pd.to_numeric(out[\"oancf_l1\"], errors=\"coerce\")\n",
    "        cfo_ratio = safe_divide(out[\"oancf\"], cfo_l1)\n",
    "        valid_cfo = cfo_l1 > 0\n",
    "\n",
    "        out[\"evt_cfo_neg\"] = (out[\"oancf\"] < 0).astype(\"int8\")\n",
    "        out[\"evt_cfo_collapse\"] = (valid_cfo & (cfo_ratio < EVENT_THRESHOLDS[\"cfo_drop_thr\"])).astype(\"int8\")\n",
    "\n",
    "    # FOCF events (need focf_l1)\n",
    "    if \"focf_l1\" in out.columns:\n",
    "        focf_l1 = pd.to_numeric(out[\"focf_l1\"], errors=\"coerce\")\n",
    "        focf_ratio = safe_divide(out[\"focf\"], focf_l1)\n",
    "        valid_focf = focf_l1 > 0\n",
    "\n",
    "        out[\"evt_focf_neg\"] = (out[\"focf\"] < 0).astype(\"int8\")\n",
    "        out[\"evt_focf_collapse\"] = (valid_focf & (focf_ratio < EVENT_THRESHOLDS[\"focf_drop_thr\"])).astype(\"int8\")\n",
    "\n",
    "    # Cash drawdown (need che_l1)\n",
    "    if \"che\" in out.columns and \"che_l1\" in out.columns:\n",
    "        che = pd.to_numeric(out[\"che\"], errors=\"coerce\")\n",
    "        che_l1 = pd.to_numeric(out[\"che_l1\"], errors=\"coerce\")\n",
    "        che_ratio = safe_divide(che, che_l1)\n",
    "        valid_che = che_l1 > 0\n",
    "        out[\"evt_cash_drawdown\"] = (valid_che & (che_ratio < EVENT_THRESHOLDS[\"cash_drop_thr\"])).astype(\"int8\")\n",
    "\n",
    "    # Liquidity squeeze (current ratio) if present\n",
    "    if (\"act\" in out.columns) and (\"lct\" in out.columns) and (\"current_ratio_l1\" in out.columns):\n",
    "        act = pd.to_numeric(out[\"act\"], errors=\"coerce\")\n",
    "        lct = pd.to_numeric(out[\"lct\"], errors=\"coerce\")\n",
    "        out[\"current_ratio\"] = safe_divide(act, lct)\n",
    "        cr_ratio = safe_divide(out[\"current_ratio\"], pd.to_numeric(out[\"current_ratio_l1\"], errors=\"coerce\"))\n",
    "        valid_cr = pd.to_numeric(out[\"current_ratio_l1\"], errors=\"coerce\") > 0\n",
    "        out[\"evt_liquidity_squeeze\"] = (valid_cr & (cr_ratio < EVENT_THRESHOLDS[\"cr_drop_thr\"])).astype(\"int8\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# Choose a representative case: highest XGB PD in TEST\n",
    "test_pd = pd.Series(p_test_xgb, index=test.index, name=\"pd_xgb\")\n",
    "base_idx = test_pd.idxmax()\n",
    "base_row = test.loc[base_idx].copy()\n",
    "base_pd = float(test_pd.loc[base_idx])\n",
    "\n",
    "print(f\"Base case (TEST) index={base_idx} | PD={base_pd:.4f}\")\n",
    "\n",
    "scenarios = {\n",
    "    \"Base\": {},\n",
    "    \"Dividend suspension (dv=0)\": {\"dv\": 0.0} if \"dv\" in test.columns else {},\n",
    "    \"CFO shock (-30%)\": {\"oancf\": float(base_row.get(\"oancf\", np.nan)) * 0.70} if \"oancf\" in test.columns else {},\n",
    "    \"Capex surge (+25%)\": {\"capx\": float(base_row.get(\"capx\", np.nan)) * 1.25} if \"capx\" in test.columns else {},\n",
    "    \"Cash drawdown (-20%)\": {\"che\": float(base_row.get(\"che\", np.nan)) * 0.80} if \"che\" in test.columns else {},\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, adj in scenarios.items():\n",
    "    row_s = base_row.copy()\n",
    "    for k, v in adj.items():\n",
    "        row_s[k] = v\n",
    "\n",
    "    tmp = pd.DataFrame([row_s])\n",
    "    tmp = recompute_features_for_rows(tmp)\n",
    "\n",
    "    # Apply the same preprocessing pipeline as training\n",
    "    tmp = clip_and_impute(tmp)\n",
    "    tmp.loc[:, continuous_feats] = scaler.transform(tmp[continuous_feats])\n",
    "\n",
    "    x_in = tmp[MODEL_FEATS]\n",
    "    pd_s = float(xgb_clf.predict_proba(x_in)[:, 1][0])\n",
    "    results.append((name, pd_s, pd_s - base_pd))\n",
    "\n",
    "pd_results = pd.DataFrame(results, columns=[\"Scenario\", \"PD\", \"ΔPD\"]).sort_values(\"PD\", ascending=False)\n",
    "display(pd_results)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
