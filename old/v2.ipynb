{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =============================================================================\n",
    "# 0. CONFIGURATION\n",
    "# =============================================================================\n",
    "FILE_NAME = \"data.csv\"\n",
    "\n",
    "# IMPORTANT: because target is t+1, we split by label_year = fyear + 1\n",
    "TRAIN_CUTOFF_LABEL_YEAR = 2022     # Train/Val labels up to and incl. 2022, Test labels after 2022\n",
    "VAL_YEARS = 1                      # Hold out the last 1 label-year from the training pool as validation\n",
    "\n",
    "# Rolling year-based CV folds (within train_pool)\n",
    "N_SPLITS_TIME_CV = 5\n",
    "\n",
    "WINSOR_LOWER_Q = 0.01\n",
    "WINSOR_UPPER_Q = 0.99\n",
    "EPS = 1e-8\n",
    "\n",
    "NUMERIC_COLS = [\n",
    "    'prcc_c', 'prcc_f', 'gvkey', 'fyear', 'ismod',\n",
    "    'ib', 'at', 'dltt', 'dlc', 'che', 're', 'seq',\n",
    "    'xrd', 'dv', 'ni', 'act', 'lct', 'oancf', 'ivncf', 'fincf',\n",
    "    'oibdp', 'xint', 'mkvalt', 'capx'\n",
    "]\n",
    "\n",
    "REQUIRED_KEYS = ['gvkey', 'fyear']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 1. HELPERS (No Imputation)\n",
    "# =============================================================================\n",
    "def _ensure_required_columns(df, required):\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required column(s): {missing}. Check your CSV schema.\")\n",
    "    return True\n",
    "\n",
    "def _safe_div(numer, denom, eps=EPS):\n",
    "    numer = pd.to_numeric(numer, errors='coerce')\n",
    "    denom = pd.to_numeric(denom, errors='coerce')\n",
    "    return numer / (denom + eps)\n",
    "\n",
    "def _add_log_features(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            s = pd.to_numeric(df[c], errors='coerce')\n",
    "            m = s >= 0\n",
    "            out = pd.Series(np.nan, index=df.index, dtype='float64')\n",
    "            out.loc[m] = np.log1p(s.loc[m])\n",
    "            df[f'log_{c}'] = out\n",
    "    return df\n",
    "\n",
    "def _fit_winsor_bounds(train_df, cols, q_lo=WINSOR_LOWER_Q, q_hi=WINSOR_UPPER_Q):\n",
    "    bounds = {}\n",
    "    for c in cols:\n",
    "        s = pd.to_numeric(train_df[c], errors='coerce')\n",
    "        if s.notna().sum() == 0:\n",
    "            bounds[c] = (np.nan, np.nan)\n",
    "            continue\n",
    "        bounds[c] = (s.quantile(q_lo), s.quantile(q_hi))\n",
    "    return bounds\n",
    "\n",
    "def _apply_winsor(df, bounds):\n",
    "    for c, (lo, hi) in bounds.items():\n",
    "        if c in df.columns and np.isfinite(lo) and np.isfinite(hi):\n",
    "            s = pd.to_numeric(df[c], errors='coerce')\n",
    "            df[c] = s.clip(lower=lo, upper=hi)\n",
    "    return df\n",
    "\n",
    "def _fit_scaler(train_df, cols):\n",
    "    stats = {}\n",
    "    for c in cols:\n",
    "        s = pd.to_numeric(train_df[c], errors='coerce')\n",
    "        mu = s.mean()\n",
    "        sd = s.std(ddof=0)\n",
    "        stats[c] = (mu, sd if np.isfinite(sd) and sd > 0 else np.nan)\n",
    "    return stats\n",
    "\n",
    "def _apply_zscore(df, stats, prefix=\"z_\"):\n",
    "    for c, (mu, sd) in stats.items():\n",
    "        if c in df.columns:\n",
    "            s = pd.to_numeric(df[c], errors='coerce')\n",
    "            if np.isfinite(sd) and sd > 0:\n",
    "                df[f\"{prefix}{c}\"] = (s - mu) / (sd + EPS)\n",
    "            else:\n",
    "                df[f\"{prefix}{c}\"] = np.nan\n",
    "    return df"
   ],
   "id": "cd3fe8cf95f0b18c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. LOAD + BASIC DATA CLEANING (Formats, Duplicates)\n",
    "# =============================================================================\n",
    "df = pd.read_csv(FILE_NAME, low_memory=False)\n",
    "\n",
    "_ensure_required_columns(df, REQUIRED_KEYS)\n",
    "\n",
    "if 'datadate' in df.columns:\n",
    "    df['datadate'] = pd.to_datetime(df['datadate'], errors='coerce')\n",
    "\n",
    "for col in NUMERIC_COLS:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df['firm_id'] = df['gvkey']\n",
    "df = df.sort_values(['firm_id', 'fyear']).reset_index(drop=True)\n",
    "df = df.drop_duplicates(subset=['firm_id', 'fyear'], keep='last').reset_index(drop=True)"
   ],
   "id": "61c323add487aa7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T16:32:34.981207Z",
     "start_time": "2025-12-19T16:32:34.761405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 3. TRAIN / VALIDATION / TEST SPLIT (Temporal, based on label_year)\n",
    "# =============================================================================\n",
    "train_pool = df[df['label_year'] <= TRAIN_CUTOFF_LABEL_YEAR].copy()\n",
    "test = df[df['label_year'] > TRAIN_CUTOFF_LABEL_YEAR].copy()\n",
    "\n",
    "if train_pool.empty:\n",
    "    raise ValueError(\"Training pool is empty after applying label_year cutoff. Check TRAIN_CUTOFF_LABEL_YEAR.\")\n",
    "\n",
    "unique_label_years = np.sort(train_pool['label_year'].dropna().unique())\n",
    "val_years = unique_label_years[-VAL_YEARS:] if len(unique_label_years) >= VAL_YEARS else unique_label_years\n",
    "\n",
    "val = train_pool[train_pool['label_year'].isin(val_years)].copy()\n",
    "train = train_pool[~train_pool['label_year'].isin(val_years)].copy()\n",
    "\n",
    "print(\"----- Split Summary (based on label_year = fyear+1) -----\")\n",
    "print(f\"Train label_year max: {train['label_year'].max()} | n={len(train)}\")\n",
    "print(f\"Val   label_years: {list(val_years)} | n={len(val)}\")\n",
    "print(f\"Test  label_year min: {test['label_year'].min()} | n={len(test)}\")\n"
   ],
   "id": "5cc3180f3d630611",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Split Summary (based on label_year = fyear+1) -----\n",
      "Train label_year max: 2021 | n=44783\n",
      "Val   label_years: [np.int64(2022)] | n=6415\n",
      "Test  label_year min: 2023 | n=12404\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 4. EDA (NO IMPUTATION) + VISUALIZATIONS\n",
    "# =============================================================================\n",
    "miss_rate = train.isna().mean().sort_values(ascending=False)\n",
    "top_miss = miss_rate.head(25)\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.bar(range(len(top_miss)), top_miss.values)\n",
    "plt.xticks(range(len(top_miss)), top_miss.index, rotation=75, ha='right')\n",
    "plt.ylabel(\"Missing share (train)\")\n",
    "plt.title(\"Top missingness rates (train, no imputation)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "def plot_distress_rate(split_df, name):\n",
    "    if split_df.empty:\n",
    "        return\n",
    "    g = split_df.groupby('label_year')['target_next_year_distress'].mean()\n",
    "    plt.figure(figsize=(7, 3))\n",
    "    plt.plot(g.index, g.values, marker='o')\n",
    "    plt.title(f\"Distress rate by label_year â€” {name}\")\n",
    "    plt.xlabel(\"label_year\")\n",
    "    plt.ylabel(\"mean(target_next_year_distress)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_distress_rate(train, \"Train\")\n",
    "plot_distress_rate(val, \"Validation\")\n",
    "plot_distress_rate(test, \"Test\")\n",
    "\n",
    "base_features = [\n",
    "    'roa', 'cf_roa', 'leverage', 'debt_to_equity', 'current_ratio',\n",
    "    'cash_ratio', 're_to_assets', 'mkt_to_book', 'capex_ratio', 'interest_coverage'\n",
    "]\n",
    "base_features = [c for c in base_features if c in train.columns]\n",
    "\n",
    "for c in base_features:\n",
    "    s = pd.to_numeric(train[c], errors='coerce').dropna()\n",
    "    if s.empty:\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.hist(s.values, bins=40)\n",
    "    plt.title(f\"Histogram (Train, raw): {c}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 1.8))\n",
    "    plt.boxplot(s.values, vert=False)\n",
    "    plt.title(f\"Boxplot (Train, raw): {c}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "9a0f0ceeca39e7af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 6. TRANSFORMATIONS (fit on TRAIN only; apply to VAL/TEST)\n",
    "# =============================================================================\n",
    "for split in [train, val, test]:\n",
    "    _add_log_features(split, cols=['at', 'mkvalt', 'sale'])\n",
    "\n",
    "log_features = [c for c in ['log_at', 'log_mkvalt', 'log_sale'] if c in train.columns]\n",
    "\n",
    "winsor_cols = base_features + log_features\n",
    "winsor_cols = [c for c in winsor_cols if c in train.columns and c in val.columns and c in test.columns]\n",
    "\n",
    "# Fit bounds on TRAIN only; apply to VAL/TEST (leakage-safe)\n",
    "winsor_bounds = _fit_winsor_bounds(train, winsor_cols, q_lo=WINSOR_LOWER_Q, q_hi=WINSOR_UPPER_Q)\n",
    "train = _apply_winsor(train, winsor_bounds)\n",
    "val   = _apply_winsor(val, winsor_bounds)\n",
    "test  = _apply_winsor(test, winsor_bounds)\n",
    "print(\"Winsorization done (bounds fit on TRAIN only).\")\n",
    "\n",
    "# Fit scaling on TRAIN only; apply to VAL/TEST (leakage-safe)\n",
    "scaler_stats = _fit_scaler(train, winsor_cols)\n",
    "train = _apply_zscore(train, scaler_stats, prefix=\"z_\")\n",
    "val   = _apply_zscore(val, scaler_stats, prefix=\"z_\")\n",
    "test  = _apply_zscore(test, scaler_stats, prefix=\"z_\")\n",
    "print(\"Standardization done (stats fit on TRAIN only).\")\n"
   ],
   "id": "b0b59bf7770c93e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. TARGET CONSTRUCTION (t+1) + RAW FEATURE ENGINEERING (Ratios)\n",
    "# =============================================================================\n",
    "df['interest_coverage'] = np.nan\n",
    "if all(c in df.columns for c in ['oibdp', 'xint']):\n",
    "    df['interest_coverage'] = _safe_div(df['oibdp'], df['xint'])\n",
    "\n",
    "cond_coverage = df['interest_coverage'] < 1.0\n",
    "cond_insolvency = (df['seq'] < 0) if 'seq' in df.columns else pd.Series(False, index=df.index)\n",
    "df['distress_dummy'] = (cond_coverage.fillna(False) | cond_insolvency.fillna(False)).astype(int)\n",
    "\n",
    "df['roa'] = np.nan\n",
    "df['cf_roa'] = np.nan\n",
    "if all(c in df.columns for c in ['ib', 'at']):\n",
    "    df['roa'] = _safe_div(df['ib'], df['at'])\n",
    "if all(c in df.columns for c in ['oancf', 'at']):\n",
    "    df['cf_roa'] = _safe_div(df['oancf'], df['at'])\n",
    "\n",
    "if 'dltt' in df.columns or 'dlc' in df.columns:\n",
    "    debt_parts = []\n",
    "    if 'dltt' in df.columns: debt_parts.append(pd.to_numeric(df['dltt'], errors='coerce'))\n",
    "    if 'dlc' in df.columns:  debt_parts.append(pd.to_numeric(df['dlc'], errors='coerce'))\n",
    "    df['total_debt'] = pd.concat(debt_parts, axis=1).sum(axis=1, min_count=1)\n",
    "else:\n",
    "    df['total_debt'] = np.nan\n",
    "\n",
    "df['leverage'] = np.nan\n",
    "df['debt_to_equity'] = np.nan\n",
    "if 'total_debt' in df.columns and 'at' in df.columns:\n",
    "    df['leverage'] = _safe_div(df['total_debt'], df['at'])\n",
    "if 'total_debt' in df.columns and 'seq' in df.columns:\n",
    "    df['debt_to_equity'] = _safe_div(df['total_debt'], df['seq'])\n",
    "\n",
    "df['current_ratio'] = np.nan\n",
    "df['cash_ratio'] = np.nan\n",
    "if all(c in df.columns for c in ['act', 'lct']):\n",
    "    df['current_ratio'] = _safe_div(df['act'], df['lct'])\n",
    "if all(c in df.columns for c in ['che', 'lct']):\n",
    "    df['cash_ratio'] = _safe_div(df['che'], df['lct'])\n",
    "\n",
    "df['re_to_assets'] = np.nan\n",
    "df['mkt_to_book'] = np.nan\n",
    "if all(c in df.columns for c in ['re', 'at']):\n",
    "    df['re_to_assets'] = _safe_div(df['re'], df['at'])\n",
    "if all(c in df.columns for c in ['mkvalt', 'seq']):\n",
    "    df['mkt_to_book'] = _safe_div(df['mkvalt'], df['seq'])\n",
    "\n",
    "df['capex_ratio'] = np.nan\n",
    "if all(c in df.columns for c in ['capx', 'at']):\n",
    "    df['capex_ratio'] = _safe_div(df['capx'], df['at'])\n",
    "\n",
    "df['target_next_year_distress'] = df.groupby('firm_id')['distress_dummy'].shift(-1)\n",
    "df['label_year'] = df['fyear'] + 1\n",
    "df = df.dropna(subset=['target_next_year_distress']).reset_index(drop=True)\n"
   ],
   "id": "8012f4d5cbb10bab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 7. FEATURE SELECTION DIAGNOSTICS (TRAIN ONLY)\n",
    "# =============================================================================\n",
    "target_col = 'target_next_year_distress'\n",
    "\n",
    "print(\"\\n----- Correlation with Target (TRAIN, winsorized raw features) -----\")\n",
    "corr_df = train[[target_col] + winsor_cols].corr()\n",
    "for c in winsor_cols:\n",
    "    r = corr_df.loc[target_col, c]\n",
    "    print(f\"{c:<20} r = {r: .4f}\")\n",
    "\n",
    "print(\"\\n----- Variability Checks (TRAIN, winsorized raw features) -----\")\n",
    "for c in winsor_cols:\n",
    "    s = pd.to_numeric(train[c], errors='coerce').dropna()\n",
    "    if s.empty:\n",
    "        print(f\"{c:<20} IQR: NA, Std. Dev.: NA\")\n",
    "        continue\n",
    "    iqr = s.quantile(0.75) - s.quantile(0.25)\n",
    "    std = s.std(ddof=1)\n",
    "    print(f\"{c:<20} IQR: {iqr:.6f}, Std. Dev.: {std:.6f}\")\n"
   ],
   "id": "586edb5962c2ab7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 8. ROLLING / FORWARD CV BY YEAR (NO MODEL TRAINING; JUST SPLITS)\n",
    "# =============================================================================\n",
    "def rolling_year_folds(df_in, year_col='label_year', n_splits=5, min_train_years=3):\n",
    "    years = np.sort(df_in[year_col].dropna().unique())\n",
    "    if len(years) < (min_train_years + n_splits):\n",
    "        n_splits = max(1, len(years) - min_train_years)\n",
    "    folds = []\n",
    "    for k in range(n_splits):\n",
    "        train_years = years[:min_train_years + k]\n",
    "        val_year = years[min_train_years + k]\n",
    "        tr_idx = df_in.index[df_in[year_col].isin(train_years)].to_numpy()\n",
    "        va_idx = df_in.index[df_in[year_col] == val_year].to_numpy()\n",
    "        folds.append((tr_idx, va_idx, train_years, val_year))\n",
    "    return folds\n",
    "\n",
    "print(\"\\n----- Rolling Year CV folds (within TRAIN_POOL) -----\")\n",
    "train_pool_for_cv = train_pool.copy()  # includes both train+val (but excludes test)\n",
    "year_folds = rolling_year_folds(train_pool_for_cv, year_col='label_year',\n",
    "                                n_splits=N_SPLITS_TIME_CV, min_train_years=3)\n",
    "\n",
    "for i, (tr_idx, va_idx, tr_years, va_year) in enumerate(year_folds, 1):\n",
    "    print(f\"Fold {i}: train_years={tr_years[0]}..{tr_years[-1]} (n={len(tr_idx)}), \"\n",
    "          f\"val_year={va_year} (n={len(va_idx)})\")\n",
    "\n",
    "# NOTE (for later, when you train models):\n",
    "# In each CV fold, winsor/scaling must be FIT on fold-train and APPLIED to fold-val\n",
    "# to avoid leakage. This follows general leakage-safe preprocessing practice. :contentReference[oaicite:2]{index=2}\n"
   ],
   "id": "29c72a7bdd111ba9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.describe()",
   "id": "87050376f597274c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
