{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e170ebd6",
   "metadata": {},
   "source": [
    "# Financial Distress Prediction Pipeline (Adjusted)\n",
    "\n",
    "This notebook follows the same structure as your current streamlined pipeline while fixing the material methodological and technical issues:\n",
    "\n",
    "- Panel-safe lag/lead construction (sorting enforced)\n",
    "- Missingness-aware distress proxy (avoids NaN → False “healthy” bias)\n",
    "- Leakage-free event threshold calibration (train-only)\n",
    "- Event indicators restricted to non-proxy channels (no coverage/leverage/EBITDA-proxy events)\n",
    "- Stable preprocessing (train-fitted clipping + median imputation + scaling)\n",
    "- Correct TreeSHAP extraction for XGBoost\n",
    "- Scenario analysis that propagates through engineered features (no deleveraging/coverage scenarios; no proxy-related shocks)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "245e0eb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:13.422995Z",
     "start_time": "2026-01-04T21:21:13.391983Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# 0. Project Overview — Financial Distress Prediction Pipeline\n",
    "# =============================================================================\n",
    "# This notebook follows the standard Data Science Lifecycle:\n",
    "#   (1) Data Cleaning and Quality Diagnostics\n",
    "#   (2) Feature Engineering and Label Construction\n",
    "#   (3) Event Indicators (interpretable drivers)\n",
    "#   (4) Train / Validation / Test Split and Preprocessing\n",
    "#   (5) Logit Models (supervised benchmark)\n",
    "#   (6) Tree-based Model (XGBoost with native TreeSHAP explainability)\n",
    "#   (7) Evaluation and Benchmarks (Persistence vs. Early Warning)\n",
    "#   (8) Decision Support and Scenario Analysis\n",
    "#\n",
    "# Adjustments (relative to the simplified script you shared):\n",
    "#   - Enforce panel sorting BEFORE any groupby shift\n",
    "#   - Missingness-aware distress label construction (avoid NaN -> False)\n",
    "#   - Train-only quantile thresholds for event shocks (avoid leakage)\n",
    "#   - Events exclude coverage/leverage/EBITDA-proxy channels\n",
    "#   - Preprocessing: train-fitted clipping + median imputation + scaling\n",
    "#   - Correct SHAP contributions extraction via Booster.predict(pred_contribs=True)\n",
    "#   - Scenario analysis recomputes engineered features and events; no leverage/coverage scenarios\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, log_loss\n",
    "\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display\n"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "1f77ea01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:15.226223Z",
     "start_time": "2026-01-04T21:21:13.485594Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# 1. Data Import and Cleaning\n",
    "# =============================================================================\n",
    "\n",
    "DATA_PATH = \"data.csv\"  # file must be in the same folder\n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "\n",
    "# Basic formatting\n",
    "df.columns = df.columns.str.lower().str.strip()\n",
    "\n",
    "# Drop duplicates if any\n",
    "df = df.drop_duplicates(subset=[\"gvkey\", \"fyear\"], keep=\"last\")\n",
    "\n",
    "# Convert numeric columns (best-effort)\n",
    "num_cols = df.select_dtypes(include=[\"float64\", \"int64\", \"int32\", \"float32\"]).columns\n",
    "df[num_cols] = df[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Panel safety: enforce stable ordering BEFORE any lag/lead\n",
    "df = df.sort_values([\"gvkey\", \"fyear\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]:,} firm-year observations, {df.shape[1]} variables.\")\n",
    "print(f\"Years: {int(df['fyear'].min())}–{int(df['fyear'].max())}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 75,005 firm-year observations, 89 variables.\n",
      "Years: 2014–2024\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "2585ef2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:15.335282Z",
     "start_time": "2026-01-04T21:21:15.320289Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# 2. Helper Functions\n",
    "# =============================================================================\n",
    "\n",
    "def safe_divide(a, b):\n",
    "    # Numerically stable division; returns NaN for non-finite results.\n",
    "    a = pd.to_numeric(a, errors=\"coerce\")\n",
    "    b = pd.to_numeric(b, errors=\"coerce\")\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        res = a / b\n",
    "    res = res.replace([np.inf, -np.inf], np.nan)\n",
    "    return res\n",
    "\n",
    "def safe_log(x):\n",
    "    x = pd.to_numeric(x, errors=\"coerce\")\n",
    "    out = pd.Series(np.nan, index=x.index, dtype=\"float64\")\n",
    "    m = x > 0\n",
    "    out.loc[m] = np.log(x.loc[m])\n",
    "    return out\n"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "cbb98be7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:15.514169Z",
     "start_time": "2026-01-04T21:21:15.336329Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# 3. Feature Engineering and Label Construction\n",
    "# =============================================================================\n",
    "\n",
    "# --- Debt and capital components (missingness-aware) ---\n",
    "dlc = pd.to_numeric(df.get(\"dlc\"), errors=\"coerce\")\n",
    "dltt = pd.to_numeric(df.get(\"dltt\"), errors=\"coerce\")\n",
    "\n",
    "total_debt = dlc.fillna(0) + dltt.fillna(0)\n",
    "# If both components missing, keep total_debt as missing (do NOT coerce to 0)\n",
    "both_debt_missing = dlc.isna() & dltt.isna()\n",
    "total_debt.loc[both_debt_missing] = np.nan\n",
    "df[\"total_debt\"] = total_debt\n",
    "\n",
    "seq = pd.to_numeric(df.get(\"seq\"), errors=\"coerce\")\n",
    "mibt = pd.to_numeric(df.get(\"mibt\"), errors=\"coerce\") if \"mibt\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "\n",
    "equity_plus_mi = seq.fillna(0) + mibt.fillna(0)\n",
    "both_eq_missing = seq.isna() & mibt.isna()\n",
    "equity_plus_mi.loc[both_eq_missing] = np.nan\n",
    "df[\"equity_plus_mi_sp\"] = equity_plus_mi\n",
    "\n",
    "df[\"total_capital_sp\"] = df[\"total_debt\"] + df[\"equity_plus_mi_sp\"]\n",
    "\n",
    "# --- Core ratios (continuous features) ---\n",
    "oibdp = pd.to_numeric(df.get(\"oibdp\"), errors=\"coerce\")\n",
    "xint  = pd.to_numeric(df.get(\"xint\"), errors=\"coerce\")\n",
    "\n",
    "# FFO proxy: keep taxes adjustment only when available (avoid hard-coding zeros)\n",
    "txt  = pd.to_numeric(df.get(\"txt\"), errors=\"coerce\") if \"txt\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "txdc = pd.to_numeric(df.get(\"txdc\"), errors=\"coerce\") if \"txdc\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "txach= pd.to_numeric(df.get(\"txach\"), errors=\"coerce\") if \"txach\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "\n",
    "tax_adj = (txt - txdc - txach)\n",
    "ffo_base = oibdp - xint\n",
    "ffo_adj = ffo_base.copy()\n",
    "ffo_adj.loc[tax_adj.notna()] = (ffo_base - tax_adj).loc[tax_adj.notna()]\n",
    "\n",
    "df[\"sp_debt_to_capital\"] = safe_divide(df[\"total_debt\"], df[\"total_capital_sp\"])\n",
    "df[\"sp_debt_to_ebitda\"]  = safe_divide(df[\"total_debt\"], oibdp)\n",
    "\n",
    "df[\"sp_ffo_to_debt\"]     = safe_divide(ffo_adj, df[\"total_debt\"])\n",
    "\n",
    "oancf = pd.to_numeric(df.get(\"oancf\"), errors=\"coerce\")\n",
    "capx  = pd.to_numeric(df.get(\"capx\"), errors=\"coerce\")\n",
    "df[\"sp_cfo_to_debt\"]     = safe_divide(oancf, df[\"total_debt\"])\n",
    "\n",
    "df[\"focf\"] = oancf - capx\n",
    "df[\"sp_focf_to_debt\"]    = safe_divide(df[\"focf\"], df[\"total_debt\"])\n",
    "\n",
    "# Optional size / market variables (allowed)\n",
    "if \"at\" in df.columns:\n",
    "    df[\"log_at\"] = safe_log(df[\"at\"])\n",
    "if \"mkvalt\" in df.columns:\n",
    "    df[\"log_mkvalt\"] = safe_log(df[\"mkvalt\"])\n",
    "\n",
    "# --- Distress proxy (missingness-aware: do NOT let NaNs become \"healthy\") ---\n",
    "td = df[\"total_debt\"]\n",
    "cap = df[\"total_capital_sp\"]\n",
    "eb = oibdp\n",
    "\n",
    "ffo_to_debt_pct     = 100.0 * safe_divide(ffo_adj, td)\n",
    "debt_to_capital_pct = 100.0 * safe_divide(td, cap)\n",
    "debt_to_ebitda      = safe_divide(td, eb)\n",
    "\n",
    "valid_hl = ffo_to_debt_pct.notna() & debt_to_capital_pct.notna() & debt_to_ebitda.notna()\n",
    "hl_ffo = valid_hl & (ffo_to_debt_pct < 15)\n",
    "hl_cap = valid_hl & (debt_to_capital_pct > 55)\n",
    "hl_deb = valid_hl & (debt_to_ebitda > 4.5)\n",
    "is_highly_leveraged = hl_ffo & hl_cap & hl_deb\n",
    "\n",
    "valid_seq = seq.notna()\n",
    "is_equity_negative = valid_seq & (seq < 0)\n",
    "\n",
    "distress = pd.Series(np.nan, index=df.index, dtype=\"float64\")\n",
    "info_mask = valid_hl | valid_seq\n",
    "distress.loc[info_mask] = (is_highly_leveraged | is_equity_negative).loc[info_mask].astype(\"int8\")\n",
    "\n",
    "df[\"distress_dummy\"] = distress.astype(\"float64\")  # keep NaN where label is not defensible\n",
    "\n",
    "# Target: next year's distress (panel-safe due to sorting above)\n",
    "df[\"target_next_year_distress\"] = df.groupby(\"gvkey\")[\"distress_dummy\"].shift(-1)\n",
    "\n",
    "# Modeling sample restriction: require (i) next-year label and (ii) current-year distress for persistence baseline\n",
    "df_model = df[df[\"target_next_year_distress\"].notna() & df[\"distress_dummy\"].notna()].copy()\n",
    "df_model[\"target_next_year_distress\"] = df_model[\"target_next_year_distress\"].astype(\"int8\")\n",
    "df_model[\"distress_dummy\"] = df_model[\"distress_dummy\"].astype(\"int8\")\n",
    "\n",
    "print(f\"Modeling sample: {len(df_model):,} firm-years with defensible current distress and next-year labels.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling sample: 63,599 firm-years with defensible current distress and next-year labels.\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "07d1cfd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:15.612397Z",
     "start_time": "2026-01-04T21:21:15.519678Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# 4. Event Indicators — Interpretable Drivers (NON-proxy channels only)\n",
    "# =============================================================================\n",
    "# Constraint: exclude coverage/leverage/EBITDA-proxy events (anything mechanically embedded in distress_proxy).\n",
    "# We therefore focus on:\n",
    "#   - Dividend policy moments (cuts / suspensions / initiations)\n",
    "#   - Cash-flow shocks (CFO / FOCF)\n",
    "#   - Liquidity drawdowns (cash)\n",
    "# Thresholds are calibrated on TRAIN ONLY (<= 2020) to avoid leakage.\n",
    "\n",
    "# Define splits first for leakage-free calibration\n",
    "train_mask = df_model[\"fyear\"] <= 2020\n",
    "\n",
    "# --- Dividend moments ---\n",
    "if \"dv\" in df_model.columns:\n",
    "    dv = pd.to_numeric(df_model[\"dv\"], errors=\"coerce\")\n",
    "    df_model[\"dv_abs\"] = dv.abs()\n",
    "    df_model[\"dv_abs_l1\"] = df_model.groupby(\"gvkey\")[\"dv_abs\"].shift(1)\n",
    "\n",
    "    payer = (df_model[\"dv_abs_l1\"] > 0.01) & df_model[\"dv_abs_l1\"].notna()\n",
    "\n",
    "    df_model[\"evt_div_suspend\"]  = (payer & (df_model[\"dv_abs\"] <= 0.01)).astype(\"int8\")\n",
    "    df_model[\"evt_div_initiate\"] = ((~payer) & (df_model[\"dv_abs\"] > 0.01) & df_model[\"dv_abs_l1\"].notna()).astype(\"int8\")\n",
    "\n",
    "    div_chg = safe_divide(df_model[\"dv_abs\"], df_model[\"dv_abs_l1\"])\n",
    "    cut_q = div_chg[train_mask & payer].quantile(0.10)\n",
    "    cut_thr = float(np.clip(cut_q, 0.25, 0.90))\n",
    "    df_model[\"evt_div_cut\"] = (payer & (div_chg < cut_thr)).astype(\"int8\")\n",
    "else:\n",
    "    df_model[\"evt_div_suspend\"]  = 0\n",
    "    df_model[\"evt_div_initiate\"] = 0\n",
    "    df_model[\"evt_div_cut\"]      = 0\n",
    "\n",
    "# --- CFO shocks ---\n",
    "oancf = pd.to_numeric(df_model.get(\"oancf\"), errors=\"coerce\")\n",
    "df_model[\"oancf_l1\"] = df_model.groupby(\"gvkey\")[\"oancf\"].shift(1)\n",
    "\n",
    "df_model[\"evt_cfo_neg\"] = (oancf < 0).astype(\"int8\")\n",
    "\n",
    "cfo_ratio = safe_divide(oancf, df_model[\"oancf_l1\"])\n",
    "valid_cfo = df_model[\"oancf_l1\"] > 0\n",
    "cfo_drop_q = cfo_ratio[train_mask & valid_cfo].quantile(0.05)\n",
    "cfo_drop_thr = float(np.clip(cfo_drop_q, 0.10, 0.90))\n",
    "df_model[\"evt_cfo_collapse\"] = (valid_cfo & (cfo_ratio < cfo_drop_thr)).astype(\"int8\")\n",
    "\n",
    "# --- FOCF shocks (oancf - capx) ---\n",
    "if \"capx\" in df_model.columns:\n",
    "    capx = pd.to_numeric(df_model[\"capx\"], errors=\"coerce\")\n",
    "    focf = pd.to_numeric(df_model[\"focf\"], errors=\"coerce\")\n",
    "    df_model[\"focf_l1\"] = df_model.groupby(\"gvkey\")[\"focf\"].shift(1)\n",
    "\n",
    "    df_model[\"evt_focf_neg\"] = (focf < 0).astype(\"int8\")\n",
    "\n",
    "    focf_ratio = safe_divide(focf, df_model[\"focf_l1\"])\n",
    "    valid_focf = df_model[\"focf_l1\"] > 0\n",
    "    focf_drop_q = focf_ratio[train_mask & valid_focf].quantile(0.05)\n",
    "    focf_drop_thr = float(np.clip(focf_drop_q, 0.10, 0.90))\n",
    "    df_model[\"evt_focf_collapse\"] = (valid_focf & (focf_ratio < focf_drop_thr)).astype(\"int8\")\n",
    "else:\n",
    "    df_model[\"evt_focf_neg\"] = 0\n",
    "    df_model[\"evt_focf_collapse\"] = 0\n",
    "\n",
    "# --- Liquidity drawdown: cash drop ---\n",
    "if \"che\" in df_model.columns:\n",
    "    che = pd.to_numeric(df_model[\"che\"], errors=\"coerce\")\n",
    "    df_model[\"che_l1\"] = df_model.groupby(\"gvkey\")[\"che\"].shift(1)\n",
    "    che_ratio = safe_divide(che, df_model[\"che_l1\"])\n",
    "    valid_che = df_model[\"che_l1\"] > 0\n",
    "    che_drop_q = che_ratio[train_mask & valid_che].quantile(0.05)\n",
    "    che_drop_thr = float(np.clip(che_drop_q, 0.10, 0.90))\n",
    "    df_model[\"evt_cash_drawdown\"] = (valid_che & (che_ratio < che_drop_thr)).astype(\"int8\")\n",
    "else:\n",
    "    df_model[\"evt_cash_drawdown\"] = 0\n",
    "\n",
    "\n",
    "event_feats = [c for c in df_model.columns if c.startswith(\"evt_\")]\n",
    "print(f\"Event indicators included: {event_feats}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event indicators included: ['evt_div_suspend', 'evt_div_initiate', 'evt_div_cut', 'evt_cfo_neg', 'evt_cfo_collapse', 'evt_focf_neg', 'evt_focf_collapse', 'evt_cash_drawdown']\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "36bb9a99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:15.869134Z",
     "start_time": "2026-01-04T21:21:15.615611Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# 5. Train / Validation / Test Split and Preprocessing\n",
    "# =============================================================================\n",
    "\n",
    "train = df_model[df_model[\"fyear\"] <= 2020].copy()\n",
    "val   = df_model[df_model[\"fyear\"] == 2021].copy()\n",
    "test  = df_model[df_model[\"fyear\"] >= 2022].copy()\n",
    "\n",
    "TARGET_COL = \"target_next_year_distress\"\n",
    "\n",
    "continuous_feats = [\n",
    "    \"sp_debt_to_capital\", \"sp_debt_to_ebitda\",\n",
    "    \"sp_ffo_to_debt\", \"sp_cfo_to_debt\", \"sp_focf_to_debt\"\n",
    "]\n",
    "for opt in [\"log_at\", \"log_mkvalt\"]:\n",
    "    if opt in df_model.columns:\n",
    "        continuous_feats.append(opt)\n",
    "\n",
    "event_feats = [c for c in df_model.columns if c.startswith(\"evt_\")]\n",
    "MODEL_FEATS = continuous_feats + event_feats\n",
    "\n",
    "# --- Stabilize ratios (train-fitted clipping) + train-median imputation ---\n",
    "clip_bounds = {}\n",
    "train_medians = {}\n",
    "\n",
    "for col in continuous_feats:\n",
    "    s = pd.to_numeric(train[col], errors=\"coerce\")\n",
    "    lo = s.quantile(0.01)\n",
    "    hi = s.quantile(0.99)\n",
    "    clip_bounds[col] = (float(lo), float(hi))\n",
    "    train_medians[col] = float(s.median())\n",
    "\n",
    "def clip_and_impute(df_in):\n",
    "    df_out = df_in.copy()\n",
    "    for col in continuous_feats:\n",
    "        lo, hi = clip_bounds[col]\n",
    "        df_out[col] = pd.to_numeric(df_out[col], errors=\"coerce\").clip(lower=lo, upper=hi)\n",
    "        df_out[col] = df_out[col].fillna(train_medians[col])\n",
    "    for col in event_feats:\n",
    "        df_out[col] = pd.to_numeric(df_out[col], errors=\"coerce\").fillna(0).astype(\"int8\")\n",
    "    return df_out\n",
    "\n",
    "train = clip_and_impute(train)\n",
    "val   = clip_and_impute(val)\n",
    "test  = clip_and_impute(test)\n",
    "\n",
    "# Standardize continuous features (train statistics)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train[continuous_feats])\n",
    "\n",
    "train.loc[:, continuous_feats] = scaler.transform(train[continuous_feats])\n",
    "val.loc[:, continuous_feats]   = scaler.transform(val[continuous_feats])\n",
    "test.loc[:, continuous_feats]  = scaler.transform(test[continuous_feats])\n",
    "\n",
    "print(f\"Split sizes: train={len(train):,} | val={len(val):,} | test={len(test):,}\")\n",
    "print(f\"Features: {len(MODEL_FEATS)} (continuous={len(continuous_feats)} + events={len(event_feats)})\")\n",
    "print(\"Continuous feats:\", continuous_feats)\n",
    "print(\"Event feats:\", event_feats)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: train=44,780 | val=6,415 | test=12,404\n",
      "Features: 15 (continuous=7 + events=8)\n",
      "Continuous feats: ['sp_debt_to_capital', 'sp_debt_to_ebitda', 'sp_ffo_to_debt', 'sp_cfo_to_debt', 'sp_focf_to_debt', 'log_at', 'log_mkvalt']\n",
      "Event feats: ['evt_div_suspend', 'evt_div_initiate', 'evt_div_cut', 'evt_cfo_neg', 'evt_cfo_collapse', 'evt_focf_neg', 'evt_focf_collapse', 'evt_cash_drawdown']\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "8fc19d35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:15.986169Z",
     "start_time": "2026-01-04T21:21:15.871150Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# 6. Logit Model (Benchmark)\n",
    "# =============================================================================\n",
    "\n",
    "X_train, y_train = train[MODEL_FEATS], train[TARGET_COL].astype(int)\n",
    "X_val, y_val     = val[MODEL_FEATS],   val[TARGET_COL].astype(int)\n",
    "X_test, y_test   = test[MODEL_FEATS],  test[TARGET_COL].astype(int)\n",
    "\n",
    "best_C = 0.1\n",
    "logit = LogisticRegression(C=best_C, max_iter=1000, solver=\"lbfgs\")\n",
    "logit.fit(X_train, y_train)\n",
    "\n",
    "p_val = logit.predict_proba(X_val)[:, 1]\n",
    "p_test = logit.predict_proba(X_test)[:, 1]\n",
    "\n",
    "def evaluate_split(y_true, p_pred):\n",
    "    return pd.Series({\n",
    "        \"AUC\": roc_auc_score(y_true, p_pred),\n",
    "        \"AP\": average_precision_score(y_true, p_pred),\n",
    "        \"Brier\": brier_score_loss(y_true, p_pred),\n",
    "        \"LogLoss\": log_loss(y_true, p_pred),\n",
    "        \"PosRate\": float(np.mean(y_true)),\n",
    "        \"N\": int(len(y_true)),\n",
    "    })\n",
    "\n",
    "eval_val = evaluate_split(y_val, p_val)\n",
    "eval_test = evaluate_split(y_test, p_test)\n",
    "\n",
    "print(\"Validation performance (Logit):\\n\", eval_val.round(4))\n",
    "print(\"Test performance (Logit):\\n\", eval_test.round(4))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation performance (Logit):\n",
      " AUC           0.6953\n",
      "AP            0.3729\n",
      "Brier         0.1260\n",
      "LogLoss       0.4123\n",
      "PosRate       0.1668\n",
      "N          6415.0000\n",
      "dtype: float64\n",
      "Test performance (Logit):\n",
      " AUC            0.6885\n",
      "AP             0.3725\n",
      "Brier          0.1365\n",
      "LogLoss        0.4384\n",
      "PosRate        0.1831\n",
      "N          12404.0000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "cfc47a7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:17.284760Z",
     "start_time": "2026-01-04T21:21:15.988224Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# 7. Tree-based Model (XGBoost)\n",
    "# =============================================================================\n",
    "\n",
    "xgb_params = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"aucpr\",\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    n_estimators=500,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(**xgb_params)\n",
    "xgb_clf.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "p_val_xgb = xgb_clf.predict_proba(X_val)[:, 1]\n",
    "p_test_xgb = xgb_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "eval_val_xgb = evaluate_split(y_val, p_val_xgb)\n",
    "eval_test_xgb = evaluate_split(y_test, p_test_xgb)\n",
    "\n",
    "print(\"Validation performance (XGB):\\n\", eval_val_xgb.round(4))\n",
    "print(\"Test performance (XGB):\\n\", eval_test_xgb.round(4))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation performance (XGB):\n",
      " AUC           0.9068\n",
      "AP            0.7292\n",
      "Brier         0.0767\n",
      "LogLoss       0.2603\n",
      "PosRate       0.1668\n",
      "N          6415.0000\n",
      "dtype: float64\n",
      "Test performance (XGB):\n",
      " AUC            0.9112\n",
      "AP             0.7540\n",
      "Brier          0.0788\n",
      "LogLoss        0.2671\n",
      "PosRate        0.1831\n",
      "N          12404.0000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "419eee4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:18.282903Z",
     "start_time": "2026-01-04T21:21:17.380987Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# 8. Explainability (TreeSHAP via XGBoost pred_contribs)\n",
    "# =============================================================================\n",
    "\n",
    "booster = xgb_clf.get_booster()\n",
    "dval = xgb.DMatrix(X_val, feature_names=MODEL_FEATS)\n",
    "\n",
    "# pred_contribs=True returns SHAP contributions per feature plus a bias term (last column)\n",
    "shap_val = booster.predict(dval, pred_contribs=True)\n",
    "shap_cols = MODEL_FEATS + [\"bias\"]\n",
    "shap_df = pd.DataFrame(shap_val, columns=shap_cols)\n",
    "\n",
    "abs_mean = shap_df[MODEL_FEATS].abs().mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"Mean absolute SHAP contributions (validation):\")\n",
    "display(abs_mean.head(20))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute SHAP contributions (validation):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sp_debt_to_capital    1.133036\n",
       "sp_ffo_to_debt        0.527658\n",
       "log_at                0.275150\n",
       "sp_debt_to_ebitda     0.186086\n",
       "log_mkvalt            0.145138\n",
       "sp_focf_to_debt       0.142577\n",
       "sp_cfo_to_debt        0.136545\n",
       "evt_cfo_neg           0.071492\n",
       "evt_focf_neg          0.054369\n",
       "evt_cfo_collapse      0.019340\n",
       "evt_cash_drawdown     0.014455\n",
       "evt_div_cut           0.007996\n",
       "evt_focf_collapse     0.005306\n",
       "evt_div_suspend       0.003284\n",
       "evt_div_initiate      0.002192\n",
       "dtype: float32"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "dbd538d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:18.787694Z",
     "start_time": "2026-01-04T21:21:18.386795Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# 9. Evaluation: Persistence vs. Early Warning\n",
    "# =============================================================================\n",
    "\n",
    "eps = 1e-3\n",
    "p_persist_val  = np.clip(val[\"distress_dummy\"].astype(float),  eps, 1 - eps)\n",
    "p_persist_test = np.clip(test[\"distress_dummy\"].astype(float), eps, 1 - eps)\n",
    "\n",
    "benchmarks = pd.DataFrame([\n",
    "    [\"VAL\",  \"Persistence\", *evaluate_split(y_val,  p_persist_val)],\n",
    "    [\"VAL\",  \"Logit\",       *eval_val],\n",
    "    [\"VAL\",  \"XGB\",         *eval_val_xgb],\n",
    "    [\"TEST\", \"Persistence\", *evaluate_split(y_test, p_persist_test)],\n",
    "    [\"TEST\", \"Logit\",       *eval_test],\n",
    "    [\"TEST\", \"XGB\",         *eval_test_xgb],\n",
    "], columns=[\"Split\", \"Model\", \"AUC\", \"AP\", \"Brier\", \"LogLoss\", \"PosRate\", \"N\"])\n",
    "\n",
    "display(benchmarks)\n",
    "\n",
    "# Early-warning subset: cases not distressed at t but distressed at t+1\n",
    "val_ew  = (val[\"distress_dummy\"] == 0) & (val[TARGET_COL] == 1)\n",
    "test_ew = (test[\"distress_dummy\"] == 0) & (test[TARGET_COL] == 1)\n",
    "\n",
    "def ew_summary(mask, p_pred, label):\n",
    "    if int(mask.sum()) == 0:\n",
    "        return pd.Series({\"Subset\": label, \"N\": 0, \"MeanPD\": np.nan, \"MedianPD\": np.nan})\n",
    "    return pd.Series({\n",
    "        \"Subset\": label,\n",
    "        \"N\": int(mask.sum()),\n",
    "        \"MeanPD\": float(np.mean(p_pred[mask.values])),\n",
    "        \"MedianPD\": float(np.median(p_pred[mask.values])),\n",
    "    })\n",
    "\n",
    "ew = pd.DataFrame([\n",
    "    ew_summary(val_ew,  p_val_xgb,  \"VAL: 0→1 transitions (XGB PD)\"),\n",
    "    ew_summary(test_ew, p_test_xgb, \"TEST: 0→1 transitions (XGB PD)\"),\n",
    "])\n",
    "\n",
    "display(ew)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Split        Model       AUC        AP     Brier   LogLoss   PosRate  \\\n",
       "0   VAL  Persistence  0.806236  0.528628  0.100968  0.699751  0.166797   \n",
       "1   VAL        Logit  0.695316  0.372922  0.126003  0.412337  0.166797   \n",
       "2   VAL          XGB  0.906799  0.729185  0.076717  0.260257  0.166797   \n",
       "3  TEST  Persistence  0.816405  0.569372  0.099930  0.692567  0.183086   \n",
       "4  TEST        Logit  0.688549  0.372482  0.136455  0.438371  0.183086   \n",
       "5  TEST          XGB  0.911201  0.754015  0.078791  0.267070  0.183086   \n",
       "\n",
       "         N  \n",
       "0   6415.0  \n",
       "1   6415.0  \n",
       "2   6415.0  \n",
       "3  12404.0  \n",
       "4  12404.0  \n",
       "5  12404.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Split</th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC</th>\n",
       "      <th>AP</th>\n",
       "      <th>Brier</th>\n",
       "      <th>LogLoss</th>\n",
       "      <th>PosRate</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VAL</td>\n",
       "      <td>Persistence</td>\n",
       "      <td>0.806236</td>\n",
       "      <td>0.528628</td>\n",
       "      <td>0.100968</td>\n",
       "      <td>0.699751</td>\n",
       "      <td>0.166797</td>\n",
       "      <td>6415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VAL</td>\n",
       "      <td>Logit</td>\n",
       "      <td>0.695316</td>\n",
       "      <td>0.372922</td>\n",
       "      <td>0.126003</td>\n",
       "      <td>0.412337</td>\n",
       "      <td>0.166797</td>\n",
       "      <td>6415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VAL</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.906799</td>\n",
       "      <td>0.729185</td>\n",
       "      <td>0.076717</td>\n",
       "      <td>0.260257</td>\n",
       "      <td>0.166797</td>\n",
       "      <td>6415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST</td>\n",
       "      <td>Persistence</td>\n",
       "      <td>0.816405</td>\n",
       "      <td>0.569372</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.692567</td>\n",
       "      <td>0.183086</td>\n",
       "      <td>12404.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST</td>\n",
       "      <td>Logit</td>\n",
       "      <td>0.688549</td>\n",
       "      <td>0.372482</td>\n",
       "      <td>0.136455</td>\n",
       "      <td>0.438371</td>\n",
       "      <td>0.183086</td>\n",
       "      <td>12404.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TEST</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.911201</td>\n",
       "      <td>0.754015</td>\n",
       "      <td>0.078791</td>\n",
       "      <td>0.267070</td>\n",
       "      <td>0.183086</td>\n",
       "      <td>12404.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "                           Subset    N    MeanPD  MedianPD\n",
       "0   VAL: 0→1 transitions (XGB PD)  356  0.183557  0.139482\n",
       "1  TEST: 0→1 transitions (XGB PD)  716  0.203632  0.162034"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subset</th>\n",
       "      <th>N</th>\n",
       "      <th>MeanPD</th>\n",
       "      <th>MedianPD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VAL: 0→1 transitions (XGB PD)</td>\n",
       "      <td>356</td>\n",
       "      <td>0.183557</td>\n",
       "      <td>0.139482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST: 0→1 transitions (XGB PD)</td>\n",
       "      <td>716</td>\n",
       "      <td>0.203632</td>\n",
       "      <td>0.162034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "fa9c9eda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:21:19.160416Z",
     "start_time": "2026-01-04T21:21:18.850641Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# 10. Decision Support and Scenario Analysis (non-proxy shocks)\n",
    "# =============================================================================\n",
    "# Requirements:\n",
    "#   - No deleveraging / leverage-ratio shocks (proxy-mechanical)\n",
    "#   - No coverage shocks\n",
    "#   - Scenarios must propagate through engineered features and event indicators\n",
    "#\n",
    "# We implement \"primitive shocks\" (dv, oancf, capx, che) and RECOMPUTE:\n",
    "#   - continuous ratios (sp_cfo_to_debt, focf, sp_focf_to_debt, etc.)\n",
    "#   - event indicators that depend on current vs lag values (stored in *_l1 columns)\n",
    "\n",
    "EVENT_THRESHOLDS = {\n",
    "    \"div_cut_thr\": float(locals().get(\"cut_thr\", 0.75)),\n",
    "    \"cfo_drop_thr\": float(locals().get(\"cfo_drop_thr\", 0.75)),\n",
    "    \"focf_drop_thr\": float(locals().get(\"focf_drop_thr\", 0.75)),\n",
    "    \"cash_drop_thr\": float(locals().get(\"che_drop_thr\", 0.75)),\n",
    "}\n",
    "\n",
    "def recompute_features_for_rows(df_rows: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df_rows.copy()\n",
    "\n",
    "    out[\"sp_debt_to_capital\"] = safe_divide(out[\"total_debt\"], out[\"total_capital_sp\"])\n",
    "    out[\"sp_debt_to_ebitda\"]  = safe_divide(out[\"total_debt\"], out[\"oibdp\"])\n",
    "\n",
    "    xint = pd.to_numeric(out.get(\"xint\"), errors=\"coerce\")\n",
    "    oibdp = pd.to_numeric(out.get(\"oibdp\"), errors=\"coerce\")\n",
    "\n",
    "    txt  = pd.to_numeric(out.get(\"txt\"), errors=\"coerce\") if \"txt\" in out.columns else pd.Series(np.nan, index=out.index)\n",
    "    txdc = pd.to_numeric(out.get(\"txdc\"), errors=\"coerce\") if \"txdc\" in out.columns else pd.Series(np.nan, index=out.index)\n",
    "    txach= pd.to_numeric(out.get(\"txach\"), errors=\"coerce\") if \"txach\" in out.columns else pd.Series(np.nan, index=out.index)\n",
    "\n",
    "    tax_adj = (txt - txdc - txach)\n",
    "    ffo_base = oibdp - xint\n",
    "    ffo_adj = ffo_base.copy()\n",
    "    ffo_adj.loc[tax_adj.notna()] = (ffo_base - tax_adj).loc[tax_adj.notna()]\n",
    "\n",
    "    out[\"sp_ffo_to_debt\"] = safe_divide(ffo_adj, out[\"total_debt\"])\n",
    "\n",
    "    oancf = pd.to_numeric(out.get(\"oancf\"), errors=\"coerce\")\n",
    "    capx  = pd.to_numeric(out.get(\"capx\"), errors=\"coerce\")\n",
    "\n",
    "    out[\"sp_cfo_to_debt\"]  = safe_divide(oancf, out[\"total_debt\"])\n",
    "    out[\"focf\"]            = oancf - capx\n",
    "    out[\"sp_focf_to_debt\"] = safe_divide(out[\"focf\"], out[\"total_debt\"])\n",
    "\n",
    "    # --- Events ---\n",
    "    if \"dv\" in out.columns and \"dv_abs_l1\" in out.columns:\n",
    "        dv = pd.to_numeric(out[\"dv\"], errors=\"coerce\")\n",
    "        out[\"dv_abs\"] = dv.abs()\n",
    "        payer = (out[\"dv_abs_l1\"] > 0.01) & out[\"dv_abs_l1\"].notna()\n",
    "\n",
    "        out[\"evt_div_suspend\"]  = (payer & (out[\"dv_abs\"] <= 0.01)).astype(\"int8\")\n",
    "        out[\"evt_div_initiate\"] = ((~payer) & (out[\"dv_abs\"] > 0.01) & out[\"dv_abs_l1\"].notna()).astype(\"int8\")\n",
    "\n",
    "        div_chg = safe_divide(out[\"dv_abs\"], out[\"dv_abs_l1\"])\n",
    "        out[\"evt_div_cut\"] = (payer & (div_chg < EVENT_THRESHOLDS[\"div_cut_thr\"])).astype(\"int8\")\n",
    "\n",
    "    if \"oancf_l1\" in out.columns:\n",
    "        cfo_ratio = safe_divide(oancf, out[\"oancf_l1\"])\n",
    "        valid_cfo = out[\"oancf_l1\"] > 0\n",
    "        out[\"evt_cfo_neg\"] = (oancf < 0).astype(\"int8\")\n",
    "        out[\"evt_cfo_collapse\"] = (valid_cfo & (cfo_ratio < EVENT_THRESHOLDS[\"cfo_drop_thr\"])).astype(\"int8\")\n",
    "\n",
    "    if \"focf_l1\" in out.columns:\n",
    "        focf_ratio = safe_divide(out[\"focf\"], out[\"focf_l1\"])\n",
    "        valid_focf = out[\"focf_l1\"] > 0\n",
    "        out[\"evt_focf_neg\"] = (out[\"focf\"] < 0).astype(\"int8\")\n",
    "        out[\"evt_focf_collapse\"] = (valid_focf & (focf_ratio < EVENT_THRESHOLDS[\"focf_drop_thr\"])).astype(\"int8\")\n",
    "\n",
    "    if \"che\" in out.columns and \"che_l1\" in out.columns:\n",
    "        che = pd.to_numeric(out[\"che\"], errors=\"coerce\")\n",
    "        che_ratio = safe_divide(che, out[\"che_l1\"])\n",
    "        valid_che = out[\"che_l1\"] > 0\n",
    "        out[\"evt_cash_drawdown\"] = (valid_che & (che_ratio < EVENT_THRESHOLDS[\"cash_drop_thr\"])).astype(\"int8\")\n",
    "\n",
    "    return out\n",
    "\n",
    "base_row = test.iloc[0].copy()\n",
    "base_pd = float(p_test_xgb[0])\n",
    "\n",
    "scenarios = {\n",
    "    \"Base\": {},\n",
    "    \"Dividend suspension (dv=0)\": {\"dv\": 0.0} if \"dv\" in test.columns else {},\n",
    "    \"CFO shock (-30%)\": {\"oancf\": float(base_row.get(\"oancf\", np.nan)) * 0.70} if \"oancf\" in test.columns else {},\n",
    "    \"Capex surge (+25%)\": {\"capx\": float(base_row.get(\"capx\", np.nan)) * 1.25} if \"capx\" in test.columns else {},\n",
    "    \"Cash drawdown (-20%)\": {\"che\": float(base_row.get(\"che\", np.nan)) * 0.80} if \"che\" in test.columns else {},\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, adj in scenarios.items():\n",
    "    row_s = base_row.copy()\n",
    "    for k, v in adj.items():\n",
    "        row_s[k] = v\n",
    "\n",
    "    tmp = pd.DataFrame([row_s])\n",
    "    tmp = recompute_features_for_rows(tmp)\n",
    "\n",
    "    tmp = clip_and_impute(tmp)\n",
    "    tmp.loc[:, continuous_feats] = scaler.transform(tmp[continuous_feats])\n",
    "\n",
    "    x_in = tmp[MODEL_FEATS]\n",
    "    pd_s = float(xgb_clf.predict_proba(x_in)[:, 1][0])\n",
    "    results.append((name, pd_s, pd_s - base_pd))\n",
    "\n",
    "pd_results = pd.DataFrame(results, columns=[\"Scenario\", \"PD\", \"ΔPD\"])\n",
    "display(pd_results)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                     Scenario        PD       ΔPD\n",
       "0                        Base  0.194363  0.139428\n",
       "1  Dividend suspension (dv=0)  0.194363  0.139428\n",
       "2            CFO shock (-30%)  0.195867  0.140932\n",
       "3          Capex surge (+25%)  0.191345  0.136410\n",
       "4        Cash drawdown (-20%)  0.194363  0.139428"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scenario</th>\n",
       "      <th>PD</th>\n",
       "      <th>ΔPD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Base</td>\n",
       "      <td>0.194363</td>\n",
       "      <td>0.139428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dividend suspension (dv=0)</td>\n",
       "      <td>0.194363</td>\n",
       "      <td>0.139428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CFO shock (-30%)</td>\n",
       "      <td>0.195867</td>\n",
       "      <td>0.140932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Capex surge (+25%)</td>\n",
       "      <td>0.191345</td>\n",
       "      <td>0.136410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cash drawdown (-20%)</td>\n",
       "      <td>0.194363</td>\n",
       "      <td>0.139428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 35
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
