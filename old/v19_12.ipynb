{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# =============================================================================\n",
    "# Goal of this cell (beginner-friendly):\n",
    "# 1) Load panel data (firm-year).\n",
    "# 2) Clean types + de-duplicate firm-year rows.\n",
    "# 3) Impute missing raw inputs using training-only information.\n",
    "# 4) Build financial ratios (\"features\") + next-year distress target.\n",
    "# 5) Create Train / Validation / Test splits by year.\n",
    "# 6) Winsorize + standardize features, then print basic diagnostics.\n",
    "# =============================================================================\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration / Hyperparams\n",
    "# ----------------------------\n",
    "FILE_NAME = \"../data.csv\"\n",
    "\n",
    "TRAIN_CUTOFF_LABEL_YEAR = 2022   # label_year <= cutoff → train/val pool; after cutoff → test\n",
    "VAL_YEARS = 1                    # last N years within the pool are validation\n",
    "N_SPLITS_TIME_CV = 5             # rolling time-based folds for sanity checks\n",
    "\n",
    "WINSOR_LOWER_Q = 0.01            # winsorization lower quantile (train-only)\n",
    "WINSOR_UPPER_Q = 0.99            # winsorization upper quantile (train-only)\n",
    "\n",
    "REQUIRED_KEYS = [\"gvkey\", \"fyear\"]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Small helper utilities\n",
    "# ----------------------------\n",
    "\n",
    "def to_float_numpy(x) -> np.ndarray:\n",
    "    \"\"\"Convert series/array-like to float numpy array, coercing non-numeric to NaN.\"\"\"\n",
    "    s = pd.to_numeric(x, errors=\"coerce\")\n",
    "    return s.to_numpy(dtype=float) if hasattr(s, \"to_numpy\") else np.asarray(s, dtype=float)\n",
    "def safe_divide(a, b) -> np.ndarray:\n",
    "    \"\"\"Elementwise divide a/b with NaN when division is invalid (0 or non-finite).\"\"\"\n",
    "    a = to_float_numpy(a)\n",
    "    b = to_float_numpy(b)\n",
    "    out = np.full_like(a, np.nan, dtype=float)\n",
    "    np.divide(a, b, out=out, where=(b != 0) & np.isfinite(a) & np.isfinite(b))\n",
    "    return out\n",
    "\n",
    "def rolling_year_folds(\n",
    "    df_in: pd.DataFrame, year_col: str = \"label_year\", n_splits: int = 5, min_train_years: int = 3\n",
    ") -> list[tuple[np.ndarray, np.ndarray, np.ndarray, int]]:\n",
    "    \"\"\"\n",
    "    Create expanding-window time folds:\n",
    "      train years: first (min_train_years + k) years\n",
    "      val year:    next year\n",
    "    Returns: list of (train_idx, val_idx, train_years, val_year)\n",
    "    \"\"\"\n",
    "    years_sorted = np.sort(df_in[year_col].dropna().unique())\n",
    "    if len(years_sorted) <= min_train_years:\n",
    "        return []\n",
    "    n_splits = min(n_splits, len(years_sorted) - min_train_years)\n",
    "\n",
    "    folds_out = []\n",
    "    for k in range(n_splits):\n",
    "        train_years = years_sorted[: min_train_years + k]\n",
    "        val_year = int(years_sorted[min_train_years + k])\n",
    "\n",
    "        train_idx = df_in.index[df_in[year_col].isin(train_years)].to_numpy()\n",
    "        val_idx = df_in.index[df_in[year_col] == val_year].to_numpy()\n",
    "        folds_out.append((train_idx, val_idx, train_years, val_year))\n",
    "\n",
    "    return folds_out\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) Load data + basic cleaning\n",
    "# =============================================================================\n",
    "df = pd.read_csv(FILE_NAME, low_memory=False)\n",
    "\n",
    "# Convert datadate if present\n",
    "if \"datadate\" in df.columns:\n",
    "    df[\"datadate\"] = pd.to_datetime(df[\"datadate\"], errors=\"coerce\")\n",
    "\n",
    "# Create stable firm id + de-duplicate firm-year (keep last record)\n",
    "df[\"firm_id\"] = df[\"gvkey\"]\n",
    "df = (\n",
    "    df.sort_values([\"firm_id\", \"fyear\"])\n",
    "      .drop_duplicates(subset=[\"firm_id\", \"fyear\"], keep=\"last\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Label year: predict distress in the next fiscal year\n",
    "df[\"label_year\"] = df[\"fyear\"] + 1\n",
    "\n",
    "# =============================================================================\n",
    "# 2) Define train/val pool years (based on label_year)\n",
    "# =============================================================================\n",
    "pool_mask = df[\"label_year\"] <= TRAIN_CUTOFF_LABEL_YEAR\n",
    "pool_years = np.sort(df.loc[pool_mask, \"label_year\"].dropna().unique())\n",
    "val_years = pool_years[-VAL_YEARS:] if len(pool_years) else np.array([], dtype=int)\n",
    "\n",
    "# This mask is ONLY used for imputations (train-only information)\n",
    "train_mask_for_imputation = pool_mask & (~df[\"label_year\"].isin(val_years))\n"
   ],
   "id": "7e0ff41063681d9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# 3) Missingness flags + imputation setup (+ EDA before/after imputation)\n",
    "#    Imputation policy implemented:\n",
    "#    - Do NOT impute: gvkey, datadate, fyear, conm, datafmt, indfmt, consol (drop rows if missing)\n",
    "#    - ismod: mode (binary flag)\n",
    "#    - Construct first: mkvalt from prcc_f * csho; dlcch/recch/invch/chech from level diffs; apalch proxy with Δap\n",
    "#    - Stocks: within-firm history (ffill) -> industry-year ratio median (fallback: year-only if no industry col)\n",
    "#    - Flows: ratio median (and optionally regression for xint etc.)\n",
    "# =============================================================================\n",
    "\n",
    "RAW_INPUTS_FOR_FE = [\n",
    "    \"aco\",\"act\",\"ao\",\"aoloch\",\"ap\",\"apalch\",\"aqc\",\"at\",\"caps\",\"capx\",\"ceq\",\"che\",\"chech\",\"csho\",\"cstk\",\"cstke\",\n",
    "    \"datadate\",\"dlc\",\"dlcch\",\"dltis\",\"dltr\",\"dltt\",\"do\",\"dp\",\"dpc\",\"dv\",\"dvc\",\"dvp\",\"dvt\",\"esubc\",\"exre\",\n",
    "    \"fiao\",\"fincf\",\"fopo\",\"fyear\",\"gvkey\",\"ib\",\"ibadj\",\"ibc\",\"intan\",\"invch\",\"invt\",\"ismod\",\"ivaco\",\"ivaeq\",\n",
    "    \"ivao\",\"ivch\",\"ivncf\",\"ivstch\",\"lco\",\"lct\",\"lt\",\"mibt\",\"mkvalt\",\"niadj\",\"nopi\",\"oancf\",\"oibdp\",\"ppent\",\n",
    "    \"prcc_c\",\"prcc_f\",\"prstkc\",\"pstk\",\"pstkn\",\"pstkr\",\"re\",\"recch\",\"rect\",\"seq\",\"siv\",\"spi\",\"sppe\",\"sppiv\",\n",
    "    \"sstk\",\"tstk\",\"txach\",\"txbcof\",\"txdc\",\"txditc\",\"txp\",\"txt\",\"xi\",\"xido\",\"xidoc\",\"xint\",\n",
    "    # optional identifiers present in many extracts:\n",
    "    \"conm\",\"consol\",\"datafmt\",\"indfmt\",\n",
    "]\n",
    "raw = [c for c in RAW_INPUTS_FOR_FE if c in df.columns]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.0 Ensure keys exist + types\n",
    "# ---------------------------------------------------------------------------\n",
    "# firm_id used for panel operations\n",
    "if \"firm_id\" not in df.columns:\n",
    "    if \"gvkey\" in df.columns:\n",
    "        df[\"firm_id\"] = df[\"gvkey\"]\n",
    "    else:\n",
    "        raise ValueError(\"Need either firm_id or gvkey in df to run panel imputations.\")\n",
    "\n",
    "# fyear numeric\n",
    "if \"fyear\" in df.columns:\n",
    "    df[\"fyear\"] = pd.to_numeric(df[\"fyear\"], errors=\"coerce\")\n",
    "\n",
    "# datadate datetime (if present)\n",
    "if \"datadate\" in df.columns:\n",
    "    df[\"datadate\"] = pd.to_datetime(df[\"datadate\"], errors=\"coerce\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.1 Drop rows with missing critical identifiers (do not impute these)\n",
    "# ---------------------------------------------------------------------------\n",
    "NON_IMPUTE_DROP = [c for c in [\"gvkey\", \"datadate\", \"fyear\", \"conm\", \"datafmt\", \"indfmt\", \"consol\"] if c in df.columns]\n",
    "if NON_IMPUTE_DROP:\n",
    "    before_n = df.shape[0]\n",
    "    df = df.dropna(subset=NON_IMPUTE_DROP).copy()\n",
    "    after_n = df.shape[0]\n",
    "    if after_n < before_n:\n",
    "        print(f\"[INFO] Dropped {before_n - after_n:,} rows due to missing non-imputable ID/meta fields: {NON_IMPUTE_DROP}\")\n",
    "\n",
    "# Rebuild raw after potential drop\n",
    "raw = [c for c in RAW_INPUTS_FOR_FE if c in df.columns]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.2 EDA BEFORE (snapshot)\n",
    "# ---------------------------------------------------------------------------\n",
    "df_raw_pre = df[raw].copy(deep=True)\n",
    "\n",
    "pre_miss = pd.DataFrame(\n",
    "    {\n",
    "        \"col\": raw,\n",
    "        \"n\": [int(df_raw_pre[c].shape[0]) for c in raw],\n",
    "        \"n_na_pre\": [int(df_raw_pre[c].isna().sum()) for c in raw],\n",
    "        \"pct_na_pre\": [float(df_raw_pre[c].isna().mean() * 100.0) for c in raw],\n",
    "        \"train_n\": [int(train_mask_for_imputation.sum()) for _ in raw],\n",
    "        \"train_pct_na_pre\": [\n",
    "            float(df_raw_pre.loc[train_mask_for_imputation, c].isna().mean() * 100.0) for c in raw\n",
    "        ],\n",
    "    }\n",
    ").sort_values(\"pct_na_pre\", ascending=False)\n",
    "\n",
    "print(\"\\n=== EDA (BEFORE imputation): Missingness on raw inputs ===\")\n",
    "print(pre_miss.round(4).head(50))\n",
    "\n",
    "# Numeric distribution summary (exclude obvious non-numeric)\n",
    "if raw:\n",
    "    x_pre = df_raw_pre[raw].apply(pd.to_numeric, errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "    q_pre = x_pre.quantile([0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99]).T\n",
    "    pre_dist = pd.DataFrame(\n",
    "        {\n",
    "            \"n_nonmiss_pre\": x_pre.notna().sum(),\n",
    "            \"mean_pre\": x_pre.mean(),\n",
    "            \"std_pre\": x_pre.std(ddof=0),\n",
    "            \"min_pre\": x_pre.min(),\n",
    "            \"p01_pre\": q_pre[0.01],\n",
    "            \"p05_pre\": q_pre[0.05],\n",
    "            \"p25_pre\": q_pre[0.25],\n",
    "            \"p50_pre\": q_pre[0.50],\n",
    "            \"p75_pre\": q_pre[0.75],\n",
    "            \"p95_pre\": q_pre[0.95],\n",
    "            \"p99_pre\": q_pre[0.99],\n",
    "            \"max_pre\": x_pre.max(),\n",
    "        }\n",
    "    )\n",
    "    print(\"\\n=== EDA (BEFORE imputation): Distribution summary (raw inputs) ===\")\n",
    "    print(pre_dist.round(4).sort_values(\"n_nonmiss_pre\", ascending=True).head(50))\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.3 Missingness flags (ALWAYS create before imputations)\n",
    "# ---------------------------------------------------------------------------\n",
    "for c in raw:\n",
    "    df[f\"miss_{c}\"] = df[c].isna().astype(\"int8\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.4 Helper: pick an \"industry\" column if available, else fallback to year-only\n",
    "# ---------------------------------------------------------------------------\n",
    "INDUSTRY_CANDIDATES = [\"sic\", \"naics\", \"gsector\", \"gind\", \"gsubind\", \"industry\", \"ff49\", \"ff12\"]\n",
    "industry_col = next((c for c in INDUSTRY_CANDIDATES if c in df.columns), None)\n",
    "group_cols = [\"fyear\"] + ([industry_col] if industry_col is not None else [])\n",
    "if industry_col is None:\n",
    "    print(\"[WARN] No industry column found (sic/naics/gics/etc.). Using year-only medians for ratio imputations.\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.5 Step 1: Construct / reconcile FIRST (no leakage: uses contemporaneous or lag only)\n",
    "# ---------------------------------------------------------------------------\n",
    "df = df.sort_values([\"firm_id\", \"fyear\"]).copy()\n",
    "\n",
    "# 3.5.1 mkvalt construction from prcc_f * csho (if mkvalt missing)\n",
    "if all(c in df.columns for c in [\"mkvalt\", \"prcc_f\", \"csho\"]):\n",
    "    mkvalt_miss = df[\"mkvalt\"].isna()\n",
    "    mkvalt_calc = pd.to_numeric(df[\"prcc_f\"], errors=\"coerce\") * pd.to_numeric(df[\"csho\"], errors=\"coerce\")\n",
    "    df.loc[mkvalt_miss & mkvalt_calc.notna(), \"mkvalt\"] = mkvalt_calc.loc[mkvalt_miss & mkvalt_calc.notna()]\n",
    "\n",
    "# 3.5.2 Reconstruct change variables from level differences (fill only if change var is missing)\n",
    "def _fill_change_from_levels(change_col, level_col):\n",
    "    if change_col in df.columns and level_col in df.columns:\n",
    "        miss = df[change_col].isna()\n",
    "        lvl = pd.to_numeric(df[level_col], errors=\"coerce\")\n",
    "        lag_lvl = df.groupby(\"firm_id\")[level_col].shift(1)\n",
    "        lag_lvl = pd.to_numeric(lag_lvl, errors=\"coerce\")\n",
    "        recon = lvl - lag_lvl\n",
    "        df.loc[miss & recon.notna(), change_col] = recon.loc[miss & recon.notna()]\n",
    "\n",
    "_fill_change_from_levels(\"dlcch\", \"dlc\")\n",
    "_fill_change_from_levels(\"recch\", \"rect\")\n",
    "_fill_change_from_levels(\"invch\", \"invt\")\n",
    "_fill_change_from_levels(\"chech\", \"che\")\n",
    "\n",
    "# 3.5.3 apalch proxy with Δap if apalch missing\n",
    "if \"apalch\" in df.columns and \"ap\" in df.columns:\n",
    "    miss = df[\"apalch\"].isna()\n",
    "    ap = pd.to_numeric(df[\"ap\"], errors=\"coerce\")\n",
    "    lag_ap = pd.to_numeric(df.groupby(\"firm_id\")[\"ap\"].shift(1), errors=\"coerce\")\n",
    "    recon = ap - lag_ap\n",
    "    df.loc[miss & recon.notna(), \"apalch\"] = recon.loc[miss & recon.notna()]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.6 Step 2: ismod mode imputation (binary flag)\n",
    "# ---------------------------------------------------------------------------\n",
    "if \"ismod\" in df.columns:\n",
    "    tr_obs = df.loc[train_mask_for_imputation, \"ismod\"]\n",
    "    tr_obs_num = pd.to_numeric(tr_obs, errors=\"coerce\")\n",
    "    # global mode on training\n",
    "    if tr_obs_num.notna().any():\n",
    "        mode_val = float(tr_obs_num.mode(dropna=True).iloc[0])\n",
    "    else:\n",
    "        mode_val = 0.0\n",
    "    df.loc[df[\"ismod\"].isna(), \"ismod\"] = mode_val\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.7 Step 3: Stocks — firm-history (ffill) -> group median of ratio (x/at)\n",
    "# ---------------------------------------------------------------------------\n",
    "STOCKS = [c for c in [\n",
    "    \"aco\",\"act\",\"ao\",\"ap\",\"at\",\"caps\",\"ceq\",\"che\",\"csho\",\"cstk\",\"dlc\",\"dltt\",\"intan\",\"invt\",\"lco\",\"lct\",\"lt\",\n",
    "    \"mibt\",\"ppent\",\"pstk\",\"pstkn\",\"pstkr\",\"re\",\"rect\",\"seq\",\"tstk\",\"ivaeq\",\"mkvalt\"\n",
    "] if c in df.columns]\n",
    "\n",
    "# firm-history forward fill within each firm (uses only past observations)\n",
    "if STOCKS:\n",
    "    df[STOCKS] = df.groupby(\"firm_id\")[STOCKS].ffill()\n",
    "\n",
    "# ratio median impute remaining missing stocks (fit on training only)\n",
    "NONNEG_STOCKS = set([c for c in STOCKS if c in {\"aco\",\"act\",\"ao\",\"ap\",\"at\",\"caps\",\"ceq\",\"che\",\"csho\",\"cstk\",\"dlc\",\"dltt\",\n",
    "                                               \"intan\",\"invt\",\"lco\",\"lct\",\"lt\",\"mibt\",\"mkvalt\",\"ppent\",\"pstk\",\"pstkn\",\"pstkr\",\n",
    "                                               \"rect\",\"seq\",\"tstk\",\"ivaeq\"}])\n",
    "\n",
    "def _fit_ratio_medians(train_df, col, size_col=\"at\"):\n",
    "    # returns (overall_ratio_median, series indexed by group tuple)\n",
    "    s = pd.to_numeric(train_df[col], errors=\"coerce\")\n",
    "    size = pd.to_numeric(train_df[size_col], errors=\"coerce\") if size_col in train_df.columns else None\n",
    "    if size is None:\n",
    "        # fallback: level median by groups\n",
    "        grp_med = train_df.groupby(group_cols)[col].median()\n",
    "        overall = float(s.median()) if s.notna().any() else 0.0\n",
    "        return (\"level\", overall, grp_med)\n",
    "\n",
    "    valid = s.notna() & size.notna() & (size > 0)\n",
    "    if valid.sum() == 0:\n",
    "        grp_med = train_df.groupby(group_cols)[col].median()\n",
    "        overall = float(s.median()) if s.notna().any() else 0.0\n",
    "        return (\"level\", overall, grp_med)\n",
    "\n",
    "    ratio = (s[valid] / size[valid]).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    overall = float(ratio.median()) if ratio.notna().any() else 0.0\n",
    "    tmp = train_df.loc[valid, group_cols].copy()\n",
    "    tmp[\"_ratio_\"] = ratio.values\n",
    "    grp_med = tmp.groupby(group_cols)[\"_ratio_\"].median()\n",
    "    return (\"ratio\", overall, grp_med)\n",
    "\n",
    "def _apply_ratio_medians(df_all, col, fit_obj, size_col=\"at\", nonneg=False):\n",
    "    kind, overall, grp_med = fit_obj\n",
    "    miss = df_all[col].isna()\n",
    "    if not miss.any():\n",
    "        return\n",
    "\n",
    "    if kind == \"ratio\" and size_col in df_all.columns:\n",
    "        size = pd.to_numeric(df_all.loc[miss, size_col], errors=\"coerce\")\n",
    "        # map group median ratio\n",
    "        g = df_all.loc[miss, group_cols]\n",
    "        mapped = pd.Series([np.nan] * miss.sum(), index=df_all.index[miss], dtype=\"float64\")\n",
    "        # build tuple key if multi-col grouping\n",
    "        if len(group_cols) == 1:\n",
    "            mapped = g[group_cols[0]].map(grp_med)\n",
    "        else:\n",
    "            keys = list(map(tuple, g[group_cols].to_numpy()))\n",
    "            mapped = pd.Series(keys, index=g.index).map(grp_med)\n",
    "\n",
    "        r = pd.to_numeric(mapped, errors=\"coerce\").fillna(overall)\n",
    "        fill = r * size\n",
    "        # if size missing/<=0, fallback to level median within groups\n",
    "        fill = fill.where(size.notna() & (size > 0), np.nan)\n",
    "        df_all.loc[miss & fill.notna(), col] = fill.loc[miss & fill.notna()].to_numpy()\n",
    "\n",
    "    # fallback: level median by group (still fit on training)\n",
    "    # (use training group median of levels if ratio path didn't fill everything)\n",
    "    miss2 = df_all[col].isna()\n",
    "    if miss2.any():\n",
    "        tr = df_all.loc[train_mask_for_imputation, [*group_cols, col]].copy()\n",
    "        tr[col] = pd.to_numeric(tr[col], errors=\"coerce\")\n",
    "        lvl_overall = float(tr[col].median()) if tr[col].notna().any() else 0.0\n",
    "        lvl_grp = tr.groupby(group_cols)[col].median()\n",
    "\n",
    "        g2 = df_all.loc[miss2, group_cols]\n",
    "        if len(group_cols) == 1:\n",
    "            mapped2 = g2[group_cols[0]].map(lvl_grp)\n",
    "        else:\n",
    "            keys2 = list(map(tuple, g2[group_cols].to_numpy()))\n",
    "            mapped2 = pd.Series(keys2, index=g2.index).map(lvl_grp)\n",
    "\n",
    "        fill2 = pd.to_numeric(mapped2, errors=\"coerce\").fillna(lvl_overall)\n",
    "        if nonneg:\n",
    "            fill2 = fill2.clip(lower=0.0)\n",
    "        df_all.loc[miss2, col] = fill2.to_numpy()\n",
    "\n",
    "# Fit on training\n",
    "tr_all = df.loc[train_mask_for_imputation].copy()\n",
    "ratio_fits = {}\n",
    "for c in STOCKS:\n",
    "    ratio_fits[c] = _fit_ratio_medians(tr_all, c, size_col=\"at\")\n",
    "\n",
    "# Apply to full df\n",
    "for c in STOCKS:\n",
    "    _apply_ratio_medians(df, c, ratio_fits[c], size_col=\"at\", nonneg=(c in NONNEG_STOCKS))\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.8 Step 4: Flows / Income variables — ratio median (x/at), with debt-aware rules for xint\n",
    "# ---------------------------------------------------------------------------\n",
    "FLOWS = [c for c in [\n",
    "    \"ib\",\"ibadj\",\"ibc\",\"niadj\",\"nopi\",\"oibdp\",\"dp\",\"txt\",\"oancf\",\"fincf\",\"ivncf\",\"xint\",\"esubc\"\n",
    "] if c in df.columns]\n",
    "\n",
    "# Debt-aware rule: if total debt == 0, xint can be set to 0 (keep miss flag already created)\n",
    "if \"xint\" in df.columns and all(c in df.columns for c in [\"dlc\", \"dltt\"]):\n",
    "    total_debt = pd.to_numeric(df[\"dlc\"], errors=\"coerce\").fillna(0.0) + pd.to_numeric(df[\"dltt\"], errors=\"coerce\").fillna(0.0)\n",
    "    xint_miss = df[\"xint\"].isna()\n",
    "    df.loc[xint_miss & (total_debt <= 0), \"xint\"] = 0.0\n",
    "\n",
    "# Fit ratio medians on training and apply\n",
    "flow_fits = {}\n",
    "for c in FLOWS:\n",
    "    flow_fits[c] = _fit_ratio_medians(tr_all, c, size_col=\"at\")\n",
    "\n",
    "for c in FLOWS:\n",
    "    # flows can be negative; do not clip\n",
    "    _apply_ratio_medians(df, c, flow_fits[c], size_col=\"at\", nonneg=False)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.9 EDA AFTER + change analysis\n",
    "# ---------------------------------------------------------------------------\n",
    "df_raw_post = df[raw].copy(deep=True)\n",
    "\n",
    "post_miss = pd.DataFrame(\n",
    "    {\n",
    "        \"col\": raw,\n",
    "        \"n_na_post\": [int(df_raw_post[c].isna().sum()) for c in raw],\n",
    "        \"pct_na_post\": [float(df_raw_post[c].isna().mean() * 100.0) for c in raw],\n",
    "        \"train_pct_na_post\": [\n",
    "            float(df_raw_post.loc[train_mask_for_imputation, c].isna().mean() * 100.0) for c in raw\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "changes = pre_miss.merge(post_miss, on=\"col\", how=\"left\")\n",
    "changes[\"n_imputed\"] = changes[\"n_na_pre\"] - changes[\"n_na_post\"]\n",
    "changes[\"pct_points_na_reduction\"] = changes[\"pct_na_pre\"] - changes[\"pct_na_post\"]\n",
    "\n",
    "x_post = df_raw_post[raw].apply(pd.to_numeric, errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "q_post = x_post.quantile([0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99]).T\n",
    "post_dist = pd.DataFrame(\n",
    "    {\n",
    "        \"n_nonmiss_post\": x_post.notna().sum(),\n",
    "        \"mean_post\": x_post.mean(),\n",
    "        \"std_post\": x_post.std(ddof=0),\n",
    "        \"min_post\": x_post.min(),\n",
    "        \"p01_post\": q_post[0.01],\n",
    "        \"p05_post\": q_post[0.05],\n",
    "        \"p25_post\": q_post[0.25],\n",
    "        \"p50_post\": q_post[0.50],\n",
    "        \"p75_post\": q_post[0.75],\n",
    "        \"p95_post\": q_post[0.95],\n",
    "        \"p99_post\": q_post[0.99],\n",
    "        \"max_post\": x_post.max(),\n",
    "    }\n",
    ")\n",
    "\n",
    "pre_dist_key = pre_dist[[\"n_nonmiss_pre\",\"mean_pre\",\"std_pre\",\"p01_pre\",\"p50_pre\",\"p99_pre\"]].copy() if raw else pd.DataFrame()\n",
    "post_dist_key = post_dist[[\"n_nonmiss_post\",\"mean_post\",\"std_post\",\"p01_post\",\"p50_post\",\"p99_post\"]].copy() if raw else pd.DataFrame()\n",
    "\n",
    "dist_delta = pre_dist_key.join(post_dist_key, how=\"outer\")\n",
    "dist_delta[\"delta_mean\"] = dist_delta[\"mean_post\"] - dist_delta[\"mean_pre\"]\n",
    "dist_delta[\"delta_std\"]  = dist_delta[\"std_post\"]  - dist_delta[\"std_pre\"]\n",
    "dist_delta[\"delta_p50\"]  = dist_delta[\"p50_post\"]  - dist_delta[\"p50_pre\"]\n",
    "\n",
    "# Imputed-only diagnostics\n",
    "rows = []\n",
    "for c in raw:\n",
    "    pre_na_mask = df_raw_pre[c].isna()\n",
    "    n_imp = int(pre_na_mask.sum())\n",
    "    if n_imp == 0:\n",
    "        rows.append((c, 0, np.nan, np.nan, np.nan, np.nan, np.nan))\n",
    "        continue\n",
    "\n",
    "    imp_vals = pd.to_numeric(df_raw_post.loc[pre_na_mask, c], errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "    obs_vals = pd.to_numeric(df_raw_pre.loc[~pre_na_mask, c], errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    rows.append(\n",
    "        (\n",
    "            c,\n",
    "            n_imp,\n",
    "            float(imp_vals.mean()) if imp_vals.notna().any() else np.nan,\n",
    "            float(imp_vals.median()) if imp_vals.notna().any() else np.nan,\n",
    "            float(imp_vals.std(ddof=0)) if imp_vals.notna().any() else np.nan,\n",
    "            float(obs_vals.mean()) if obs_vals.notna().any() else np.nan,\n",
    "            float(obs_vals.median()) if obs_vals.notna().any() else np.nan,\n",
    "        )\n",
    "    )\n",
    "\n",
    "imputed_only = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\"col\",\"n_imputed\",\"imputed_mean\",\"imputed_median\",\"imputed_std\",\"observed_mean_pre\",\"observed_median_pre\"],\n",
    ").set_index(\"col\")\n",
    "\n",
    "print(\"\\n=== EDA (AFTER imputation): Missingness on raw inputs + change ===\")\n",
    "cols_show = [\n",
    "    \"col\", \"n\", \"n_na_pre\", \"pct_na_pre\", \"n_na_post\", \"pct_na_post\",\n",
    "    \"n_imputed\", \"pct_points_na_reduction\", \"train_pct_na_pre\", \"train_pct_na_post\",\n",
    "]\n",
    "print(\n",
    "    changes[cols_show]\n",
    "    .sort_values([\"n_imputed\",\"pct_points_na_reduction\"], ascending=[False, False])\n",
    "    .round(4)\n",
    "    .head(50)\n",
    ")\n",
    "\n",
    "print(\"\\n=== Change analysis: Distribution deltas (post - pre) on raw inputs ===\")\n",
    "print(\n",
    "    dist_delta[[\"n_nonmiss_pre\",\"n_nonmiss_post\",\"delta_mean\",\"delta_std\",\"delta_p50\"]]\n",
    "    .sort_values(\"delta_mean\", key=lambda s: s.abs(), ascending=False)\n",
    "    .round(6)\n",
    "    .head(50)\n",
    ")\n",
    "\n",
    "print(\"\\n=== Change analysis: Imputed-only vs observed (pre) summary ===\")\n",
    "print(\n",
    "    imputed_only.assign(\n",
    "        mean_gap_imputed_minus_observed=lambda d: d[\"imputed_mean\"] - d[\"observed_mean_pre\"],\n",
    "        median_gap_imputed_minus_observed=lambda d: d[\"imputed_median\"] - d[\"observed_median_pre\"],\n",
    "    )\n",
    "    .sort_values(\"n_imputed\", ascending=False)\n",
    "    .round(6)\n",
    "    .head(50)\n",
    ")\n"
   ],
   "id": "b33657b33f838671",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 4) Feature engineering (ratios) + distress definition + next-year target\n",
    "# =============================================================================\n",
    "dlc = pd.to_numeric(df.get(\"dlc\", np.nan), errors=\"coerce\")\n",
    "dltt = pd.to_numeric(df.get(\"dltt\", np.nan), errors=\"coerce\")\n",
    "df[\"total_debt\"] = pd.concat([dlc, dltt], axis=1).sum(axis=1, min_count=1)\n",
    "\n",
    "seq = pd.to_numeric(df.get(\"seq\", np.nan), errors=\"coerce\")\n",
    "mibt = pd.to_numeric(df.get(\"mibt\", 0.0), errors=\"coerce\")\n",
    "df[\"equity_plus_mi_sp\"] = seq + mibt\n",
    "df[\"total_capital_sp\"] = df[\"total_debt\"] + df[\"equity_plus_mi_sp\"]\n",
    "df[\"sp_debt_to_capital\"] = safe_divide(df[\"total_debt\"], df[\"total_capital_sp\"])\n",
    "\n",
    "oibdp = pd.to_numeric(df.get(\"oibdp\", np.nan), errors=\"coerce\")\n",
    "xint = pd.to_numeric(df.get(\"xint\", np.nan), errors=\"coerce\")\n",
    "df[\"sp_debt_to_ebitda\"] = safe_divide(df[\"total_debt\"], oibdp)\n",
    "\n",
    "txt = pd.to_numeric(df.get(\"txt\", np.nan), errors=\"coerce\")\n",
    "txdc = pd.to_numeric(df.get(\"txdc\", 0.0), errors=\"coerce\")\n",
    "txach = pd.to_numeric(df.get(\"txach\", 0.0), errors=\"coerce\")\n",
    "df[\"cash_tax_paid_proxy\"] = txt - txdc - txach\n",
    "\n",
    "df[\"ffo_proxy\"] = oibdp - xint - pd.to_numeric(df[\"cash_tax_paid_proxy\"], errors=\"coerce\")\n",
    "df[\"sp_ffo_to_debt\"] = safe_divide(df[\"ffo_proxy\"], df[\"total_debt\"])\n",
    "\n",
    "oancf = pd.to_numeric(df.get(\"oancf\", np.nan), errors=\"coerce\")\n",
    "capx = pd.to_numeric(df.get(\"capx\", np.nan), errors=\"coerce\")\n",
    "df[\"sp_cfo_to_debt\"] = safe_divide(oancf, df[\"total_debt\"])\n",
    "df[\"focf\"] = oancf - capx\n",
    "df[\"sp_focf_to_debt\"] = safe_divide(df[\"focf\"], df[\"total_debt\"])\n",
    "\n",
    "dv = pd.to_numeric(df.get(\"dv\", 0.0), errors=\"coerce\")\n",
    "prstkc = pd.to_numeric(df.get(\"prstkc\", 0.0), errors=\"coerce\")\n",
    "df[\"dcf\"] = df[\"focf\"] - dv - prstkc\n",
    "df[\"sp_dcf_to_debt\"] = safe_divide(df[\"dcf\"], df[\"total_debt\"])\n",
    "\n",
    "# Log transforms (log1p handles 0 nicely). Negative → NaN.\n",
    "for c in [\"at\", \"mkvalt\"]:\n",
    "    if c in df.columns:\n",
    "        s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        df[f\"log_{c}\"] = np.where(s >= 0, np.log1p(s), np.nan)\n",
    "\n",
    "# Interest coverage: EBITDA / |interest expense|\n",
    "df[\"sp_interest_coverage\"] = safe_divide(oibdp, xint.abs())\n",
    "\n",
    "# --- Distress label based on S&P-style \"highly leveraged\"  ---\n",
    "td = pd.to_numeric(df[\"total_debt\"], errors=\"coerce\").to_numpy(dtype=float)\n",
    "cap = pd.to_numeric(df[\"total_capital_sp\"], errors=\"coerce\").to_numpy(dtype=float)\n",
    "eb = pd.to_numeric(oibdp, errors=\"coerce\").to_numpy(dtype=float)\n",
    "ffo = pd.to_numeric(df[\"ffo_proxy\"], errors=\"coerce\").to_numpy(dtype=float)\n",
    "\n",
    "ffo_to_debt_pct = 100.0 * safe_divide(ffo, td)\n",
    "debt_to_capital_pct = 100.0 * safe_divide(td, cap)\n",
    "debt_to_ebitda = safe_divide(td, eb)\n",
    "\n",
    "# Three “highly leveraged” conditions (S&P table)\n",
    "hl_ffo = (td > 0) & (ffo_to_debt_pct < 15.0)          # FFO/total debt < 15%\n",
    "hl_cap = (cap > 0) & (debt_to_capital_pct > 55.0)     # Total debt/total capital > 55%\n",
    "hl_deb = (td > 0) & ( (debt_to_ebitda > 4.5))  # TD/EBITDA > 4.5\n",
    "\n",
    "is_highly_leveraged = hl_ffo & hl_cap & hl_deb\n",
    "\n",
    "# Equity strictly negative and not missing\n",
    "is_equity_negative = seq.notna() & (seq < -1)\n",
    "\n",
    "# Final distress rule: highly leveraged\n",
    "df[\"distress_dummy\"] = ( is_highly_leveraged).astype(\"int8\")\n",
    "\n",
    "# Target: next year's distress (within firm)\n",
    "df[\"target_next_year_distress\"] = (\n",
    "    df.groupby(\"firm_id\")[\"distress_dummy\"].shift(-1)\n",
    ")\n",
    "df = df.dropna(subset=[\"target_next_year_distress\"]).reset_index(drop=True)\n",
    "df[\"target_next_year_distress\"] = df[\"target_next_year_distress\"].astype(\"int8\")"
   ],
   "id": "81bf3e0761a7f117",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 5) Final Train / Val / Test split (by label_year)\n",
    "# =============================================================================\n",
    "train_pool = df[df[\"label_year\"] <= TRAIN_CUTOFF_LABEL_YEAR].copy()\n",
    "test = df[df[\"label_year\"] > TRAIN_CUTOFF_LABEL_YEAR].copy()\n",
    "\n",
    "years = np.sort(train_pool[\"label_year\"].dropna().unique())\n",
    "val_years = years[-VAL_YEARS:] if len(years) else np.array([], dtype=int)\n",
    "\n",
    "val = train_pool[train_pool[\"label_year\"].isin(val_years)].copy()\n",
    "train = train_pool[~train_pool[\"label_year\"].isin(val_years)].copy()\n",
    "\n",
    "print(\n",
    "    \"Split:\",\n",
    "    f\"train={len(train):,}\",\n",
    "    f\"val={len(val):,}\",\n",
    "    f\"test={len(test):,}\",\n",
    "    \"| val_years:\",\n",
    "    list(val_years),\n",
    ")"
   ],
   "id": "5829c3bf2d60daf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 6) Feature selection + cleaning (inf/NaN) + winsorize + standardize\n",
    "# =============================================================================\n",
    "base_feats = [\n",
    "    \"sp_debt_to_capital\",\n",
    "    \"sp_ffo_to_debt\",\n",
    "    \"sp_cfo_to_debt\",\n",
    "    \"sp_focf_to_debt\",\n",
    "    \"sp_dcf_to_debt\",\n",
    "    \"sp_debt_to_ebitda\",\n",
    "    \"sp_interest_coverage\",\n",
    "    \"log_at\",\n",
    "    \"log_mkvalt\",\n",
    "]\n",
    "feats = [c for c in base_feats if c in train.columns and c in val.columns and c in test.columns]\n",
    "\n",
    "# Replace +/-inf with NaN\n",
    "for d in (train, val, test):\n",
    "    d[feats] = d[feats].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Median fill (fit on train only)\n",
    "fill = train[feats].median(numeric_only=True)\n",
    "for d in (train, val, test):\n",
    "    d[feats] = d[feats].fillna(fill)\n",
    "\n",
    "# Winsorize each feature using train-only quantiles\n",
    "bounds = {}\n",
    "for c in feats:\n",
    "    s = pd.to_numeric(train[c], errors=\"coerce\")\n",
    "    bounds[c] = (s.quantile(WINSOR_LOWER_Q), s.quantile(WINSOR_UPPER_Q))\n",
    "\n",
    "for d in (train, val, test):\n",
    "    for c, (lo, hi) in bounds.items():\n",
    "        s = pd.to_numeric(d[c], errors=\"coerce\")\n",
    "        d[c] = s.clip(lo, hi)\n",
    "\n",
    "# Standardize (z-scores), fit on train only\n",
    "x_train = train[feats].to_numpy(dtype=float)\n",
    "x_val = val[feats].to_numpy(dtype=float)\n",
    "x_test = test[feats].to_numpy(dtype=float)\n",
    "\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "x_train_z = scaler.transform(x_train)\n",
    "x_val_z = scaler.transform(x_val)\n",
    "x_test_z = scaler.transform(x_test)\n",
    "\n",
    "z_cols = [f\"z_{c}\" for c in feats]\n",
    "train[z_cols] = x_train_z\n",
    "val[z_cols] = x_val_z\n",
    "test[z_cols] = x_test_z"
   ],
   "id": "234ae95e1f693074",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 7) Diagnostics (correlations, folds, distributions, drift, collinearity)\n",
    "# =============================================================================\n",
    "t = \"target_next_year_distress\"\n",
    "\n",
    "corr = (\n",
    "    train[[t] + feats]\n",
    "    .corr(numeric_only=True)[t]\n",
    "    .drop(t)\n",
    "    .sort_values(key=np.abs, ascending=False)\n",
    ")\n",
    "print(corr)\n",
    "\n",
    "folds = rolling_year_folds(train_pool, n_splits=N_SPLITS_TIME_CV, min_train_years=3)\n",
    "for i, (tr_idx, va_idx, tr_years, va_year) in enumerate(folds, 1):\n",
    "    print(\n",
    "        f\"Fold {i}: train_years={tr_years[0]}..{tr_years[-1]} (n={len(tr_idx)}), \"\n",
    "        f\"val_year={va_year} (n={len(va_idx)})\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _overview(d: pd.DataFrame, name: str) -> None:\n",
    "    n_rows = len(d)\n",
    "    n_firms = d[\"firm_id\"].nunique() if \"firm_id\" in d.columns else np.nan\n",
    "    n_years = d[\"fyear\"].nunique() if \"fyear\" in d.columns else np.nan\n",
    "    target_rate = float(d[t].mean()) if t in d.columns else np.nan\n",
    "\n",
    "    print(f\"\\n=== {name} === rows={n_rows:,} | firms={n_firms:,} | years={n_years} | target_rate={target_rate:.4f}\")\n",
    "\n",
    "    if \"label_year\" in d.columns:\n",
    "        by_year = d.groupby(\"label_year\")[t].agg([\"mean\", \"count\"])\n",
    "        print(\"\\nTarget by label_year (tail):\")\n",
    "        print(by_year.tail(12))\n",
    "\n",
    "\n",
    "_overview(train, \"TRAIN\")\n",
    "_overview(val, \"VAL\")\n",
    "_overview(test, \"TEST\")\n",
    "\n",
    "post_miss = pd.DataFrame(\n",
    "    {\n",
    "        \"col\": raw,\n",
    "        \"train_pct_na\": [train[c].isna().mean() * 100 for c in raw if c in train.columns],\n",
    "        \"val_pct_na\": [val[c].isna().mean() * 100 for c in raw if c in val.columns],\n",
    "        \"test_pct_na\": [test[c].isna().mean() * 100 for c in raw if c in test.columns],\n",
    "    }\n",
    ")\n",
    "if not post_miss.empty:\n",
    "    post_miss = post_miss.sort_values(\"train_pct_na\", ascending=False)\n",
    "    print(\"\\nPost-imputation missingness on raw inputs (pct):\")\n",
    "    print(post_miss.head(50).round(4))\n",
    "\n",
    "\n",
    "def _dist(d: pd.DataFrame, cols: list[str], name: str) -> pd.DataFrame:\n",
    "    x = d[cols].replace([np.inf, -np.inf], np.nan)\n",
    "    q = x.quantile([0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99]).T\n",
    "    out = pd.DataFrame(\n",
    "        {\n",
    "            \"n\": x.notna().sum(),\n",
    "            \"mean\": x.mean(),\n",
    "            \"std\": x.std(ddof=0),\n",
    "            \"min\": x.min(),\n",
    "            \"p01\": q[0.01],\n",
    "            \"p05\": q[0.05],\n",
    "            \"p25\": q[0.25],\n",
    "            \"p50\": q[0.50],\n",
    "            \"p75\": q[0.75],\n",
    "            \"p95\": q[0.95],\n",
    "            \"p99\": q[0.99],\n",
    "            \"max\": x.max(),\n",
    "            \"skew\": x.skew(numeric_only=True),\n",
    "            \"kurt\": x.kurtosis(numeric_only=True),\n",
    "        }\n",
    "    )\n",
    "    print(f\"\\nDistribution summary ({name})\")\n",
    "    print(out.round(4).sort_values(\"skew\", key=lambda s: s.abs(), ascending=False))\n",
    "    return out\n",
    "\n",
    "\n",
    "_ = _dist(train, feats, \"TRAIN | winsorized raw feats\")\n",
    "_ = _dist(train, z_cols, \"TRAIN | standardized feats\")\n",
    "\n",
    "\n",
    "def _hi_corr(d: pd.DataFrame, cols: list[str], thr: float = 0.80) -> list[tuple[str, str, float]]:\n",
    "    cm = d[cols].corr(numeric_only=True)\n",
    "    pairs = []\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            r = cm.iloc[i, j]\n",
    "            if np.isfinite(r) and abs(r) >= thr:\n",
    "                pairs.append((cols[i], cols[j], float(r)))\n",
    "    return sorted(pairs, key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "\n",
    "pairs = _hi_corr(train, feats, thr=0.80)\n",
    "print(\"\\nHigh collinearity pairs among feats (|corr|>=0.80) [top 25]:\")\n",
    "for a, b, r in pairs[:25]:\n",
    "    print(f\"{a} vs {b}: r={r:.3f}\")\n",
    "\n",
    "\n",
    "def _drift_smd(a_df: pd.DataFrame, b_df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for c in cols:\n",
    "        a = pd.to_numeric(a_df[c], errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "        b = pd.to_numeric(b_df[c], errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "        ma, mb = float(a.mean()), float(b.mean())\n",
    "        sa, sb = float(a.std(ddof=0)), float(b.std(ddof=0))\n",
    "        sp = np.sqrt(0.5 * (sa ** 2 + sb ** 2))\n",
    "        smd = (mb - ma) / sp if sp > 0 else np.nan\n",
    "\n",
    "        rows.append((c, ma, mb, smd, abs(smd) if np.isfinite(smd) else np.nan))\n",
    "\n",
    "        out = pd.DataFrame(rows, columns=[\"feature\", \"mean_train\", \"mean_test\", \"smd\", \"abs_smd\"])\n",
    "    return out.sort_values(\"abs_smd\", ascending=False)\n",
    "\n",
    "\n",
    "drift = _drift_smd(train, test, feats)\n",
    "print(\"\\nTrain→Test drift (SMD) [top 15]:\")\n",
    "print(drift.head(15).round(4))\n",
    "\n",
    "\n",
    "def _group_diff(d: pd.DataFrame, cols: list[str]) -> pd.Series:\n",
    "    g = d.groupby(t)[cols].mean(numeric_only=True)\n",
    "    if 0 in g.index and 1 in g.index:\n",
    "        return (g.loc[1] - g.loc[0]).sort_values(key=np.abs, ascending=False)\n",
    "    return pd.Series(dtype=\"float64\")\n",
    "\n",
    "\n",
    "diff = _group_diff(train, feats)\n",
    "if not diff.empty:\n",
    "    print(\"\\nMean difference (target=1 minus target=0) on TRAIN feats [top 15]:\")\n",
    "    print(diff.head(15).round(4))\n"
   ],
   "id": "822793c7947cae74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Task: visualize how feature distributions differ for firms flagged as distressed vs not distressed.\n",
    "\n",
    "plot_df = train_pool.copy() if \"train_pool\" in globals() else df.copy()\n",
    "\n",
    "flag_col = \"distress_dummy\" if \"distress_dummy\" in plot_df.columns else (\n",
    "    \"target_next_year_distress\" if \"target_next_year_distress\" in plot_df.columns else None\n",
    ")\n",
    "if flag_col is None:\n",
    "    raise KeyError(\"No distress flag found. Expected 'distress_dummy' or 'target_next_year_distress' in the data.\")\n",
    "\n",
    "plot_feats = [c for c in (feats if \"feats\" in globals() else []) if c in plot_df.columns]\n",
    "if not plot_feats:\n",
    "    plot_feats = [c for c in (z_cols if \"z_cols\" in globals() else []) if c in plot_df.columns]\n",
    "if not plot_feats:\n",
    "    raise KeyError(\n",
    "        \"No feature columns found to plot. Expected 'feats' or 'z_cols' to exist and be present in the data.\")\n",
    "\n",
    "tmp = plot_df[[flag_col] + plot_feats].copy()\n",
    "tmp[flag_col] = pd.to_numeric(tmp[flag_col], errors=\"coerce\").astype(\"Int64\")\n",
    "tmp = tmp[tmp[flag_col].isin([0, 1])].copy()\n",
    "\n",
    "long = tmp.melt(id_vars=[flag_col], value_vars=plot_feats, var_name=\"feature\", value_name=\"value\")\n",
    "long[\"value\"] = pd.to_numeric(long[\"value\"], errors=\"coerce\")\n",
    "long = long.dropna(subset=[\"value\"])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, max(4.5, 0.45 * len(plot_feats))))\n",
    "sns.boxplot(\n",
    "    data=long,\n",
    "    x=\"value\",\n",
    "    y=\"feature\",\n",
    "    hue=flag_col,\n",
    "    orient=\"h\",\n",
    "    showfliers=False,\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title(f\"Feature distributions by {flag_col} (0=No distress, 1=Distress)\")\n",
    "ax.set_xlabel(\"Feature value\")\n",
    "ax.set_ylabel(\"\")\n",
    "ax.legend(title=flag_col, loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "2ec169034d267c32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(pd.Series({\n",
    "    \"hl_ffo\": hl_ffo.mean(),\n",
    "    \"hl_cap\": hl_cap.mean(),\n",
    "    \"hl_deb\": hl_deb.mean(),\n",
    "    \"hl_all\": is_highly_leveraged.mean()\n",
    "}))\n",
    "df[\"size_decile\"] = pd.qcut(df[\"log_at\"], 10, duplicates=\"drop\")\n",
    "print(df.groupby(\"size_decile\")[\"distress_dummy\"].mean())\n"
   ],
   "id": "ec93009d92a34d93",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
