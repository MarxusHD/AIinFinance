{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise for Chapter 12: Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This chapter introduces transformers. These were initially targeted at natural language processing (NLP) problems, where the network input is a series of high-dimensional\n",
        "embeddings representing words or word fragments. Language datasets share some of the\n",
        "characteristics of image data. The number of input variables can be very large, and the\n",
        "statistics are similar at every position; itâ€™s not sensible to re-learn the meaning of the\n",
        "word dog at every possible position in a body of text. However, language datasets have\n",
        "the complication that text sequences vary in length, and unlike images, there is no easy\n",
        "way to resize them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9vk9Elugvmi"
      },
      "source": [
        "# Coding Exercise: Self Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "\\mathbf{v}_m = \\boldsymbol{\\beta}_v + \\boldsymbol{\\Omega}_v \\mathbf{x}_m ,\n",
        "\\tag{12.2}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "\\text{sa}_n[\\mathbf{x}_1, \\ldots, \\mathbf{x}_N] \n",
        "= \\sum_{m=1}^{N} a[\\mathbf{x}_m, \\mathbf{x}_n] \\, \\mathbf{v}_m .\n",
        "\\tag{12.3}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "\\mathbf{q}_n = \\boldsymbol{\\beta}_q + \\boldsymbol{\\Omega}_q \\mathbf{x}_n\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{k}_m = \\boldsymbol{\\beta}_k + \\boldsymbol{\\Omega}_k \\mathbf{x}_m ,\n",
        "\\tag{12.4}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "a[\\mathbf{x}_m, \\mathbf{x}_n] \n",
        "= \\text{softmax}_m \\big[\\mathbf{k}_m^T \\mathbf{q}_n \\big] \n",
        "= \\frac{\\exp\\big[\\mathbf{k}_m^T \\mathbf{q}_n\\big]}\n",
        "       {\\sum_{m'=1}^N \\exp\\big[\\mathbf{k}_{m'}^T \\mathbf{q}_n\\big]} ,\n",
        "\\tag{12.5}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "\\mathbf{V}[\\mathbf{X}] = \\boldsymbol{\\beta}_v \\mathbf{1}^T + \\boldsymbol{\\Omega}_v \\mathbf{X}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{Q}[\\mathbf{X}] = \\boldsymbol{\\beta}_q \\mathbf{1}^T + \\boldsymbol{\\Omega}_q \\mathbf{X}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{K}[\\mathbf{X}] = \\boldsymbol{\\beta}_k \\mathbf{1}^T + \\boldsymbol{\\Omega}_k \\mathbf{X},\n",
        "\\tag{12.6}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "\\text{Sa}[\\mathbf{X}] \n",
        "= \\mathbf{V}[\\mathbf{X}] \\cdot \\text{Softmax}\\!\\Big[\\mathbf{K}[\\mathbf{X}]^T \\mathbf{Q}[\\mathbf{X}]\\Big],\n",
        "\\tag{12.7}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "\\text{Sa}[\\mathbf{X}] \n",
        "= \\mathbf{V} \\cdot \\text{Softmax}\\!\\big[\\mathbf{K}^T \\mathbf{Q}\\big] .\n",
        "\\tag{12.8}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "\\text{Sa}[\\mathbf{X}] \n",
        "= \\mathbf{V} \\cdot \\text{Softmax}\\!\\left[\\frac{\\mathbf{K}^T \\mathbf{Q}}{\\sqrt{D_q}}\\right] .\n",
        "\\tag{12.9}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLComQyvCIJ7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OJkkoNqCVK2"
      },
      "source": [
        "The self-attention mechanism maps $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ and returns $N$ outputs $\\mathbf{x}'_{n}\\in \\mathbb{R}^{D}$.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAygJwLiCSri"
      },
      "outputs": [],
      "source": [
        "# Set seed so we get the same random numbers\n",
        "np.random.seed(3)\n",
        "# Number of inputs\n",
        "N = 3\n",
        "# Number of dimensions of each input\n",
        "D = 4\n",
        "# Create an empty list\n",
        "all_x = []\n",
        "# Create elements x_n and append to list\n",
        "for n in range(N):\n",
        "  all_x.append(np.random.normal(size=(D,1)))\n",
        "# Print out the list\n",
        "print(all_x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2iHFbtKMaDp"
      },
      "source": [
        "We'll also need the weights and biases for the keys, queries, and values (equations 12.2 and 12.4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79TSK7oLMobe"
      },
      "outputs": [],
      "source": [
        "# Set seed so we get the same random numbers\n",
        "np.random.seed(0)\n",
        "\n",
        "# Choose random values for the parameters\n",
        "omega_q = np.random.normal(size=(D,D))\n",
        "omega_k = np.random.normal(size=(D,D))\n",
        "omega_v = np.random.normal(size=(D,D))\n",
        "beta_q = np.random.normal(size=(D,1))\n",
        "beta_k = np.random.normal(size=(D,1))\n",
        "beta_v = np.random.normal(size=(D,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxaKQtP3Ng6R"
      },
      "source": [
        "Now let's compute the queries, keys, and values for each input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwDK2tfdNmw9"
      },
      "outputs": [],
      "source": [
        "# Make three lists to store queries, keys, and values\n",
        "all_queries = []\n",
        "all_keys = []\n",
        "all_values = []\n",
        "# For every input\n",
        "for x in all_x:\n",
        "  # inside: for x in all_x:\n",
        "  query = omega_q @ x + beta_q\n",
        "  key   = omega_k @ x + beta_k\n",
        "  value = omega_v @ x + beta_v\n",
        "\n",
        "\n",
        "  all_queries.append(query)\n",
        "  all_keys.append(key)\n",
        "  all_values.append(value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se7DK6PGPSUk"
      },
      "source": [
        "We'll need a softmax function (equation 12.5) -- here, it will take a list of arbitrary numbers and return a list where the elements are non-negative and sum to one\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u93LIcE5PoiM"
      },
      "outputs": [],
      "source": [
        "def softmax(items_in):\n",
        "  shifted = items_in - np.max(items_in)\n",
        "  exp_x = np.exp(shifted)\n",
        "  items_out = exp_x / np.sum(exp_x)\n",
        "  return items_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aJVhbKDW7lm"
      },
      "source": [
        "Now compute the self attention values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yimz-5nCW6vQ"
      },
      "outputs": [],
      "source": [
        "# Create empty list for output\n",
        "all_x_prime = []\n",
        "\n",
        "# For each output\n",
        "for n in range(N):\n",
        "  # Create list for dot products of query N with all keys\n",
        "  all_km_qn = []\n",
        "  # Compute the dot products\n",
        "  for key in all_keys:\n",
        "    # TODO -- compute the appropriate dot product\n",
        "    # Replace this line\n",
        "    dot_product = 1\n",
        "\n",
        "    # Store dot product\n",
        "    all_km_qn.append(dot_product)\n",
        "\n",
        "  # Compute dot product\n",
        "  attention = softmax(all_km_qn)\n",
        "  # Print result (should be positive sum to one)\n",
        "  print(\"Attentions for output \", n)\n",
        "  print(attention)\n",
        "\n",
        "  # TODO: Compute a weighted sum of all of the values according to the attention\n",
        "  # (equation 12.3)\n",
        "  # Replace this line\n",
        "  x_prime = np.zeros((D,1))\n",
        "\n",
        "  all_x_prime.append(x_prime)\n",
        "\n",
        "\n",
        "# Print out true values to check you have it correct\n",
        "print(\"x_prime_0_calculated:\", all_x_prime[0].transpose())\n",
        "print(\"x_prime_0_true: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\")\n",
        "print(\"x_prime_1_calculated:\", all_x_prime[1].transpose())\n",
        "print(\"x_prime_1_true: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\")\n",
        "print(\"x_prime_2_calculated:\", all_x_prime[2].transpose())\n",
        "print(\"x_prime_2_true: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ2vCQ_7C38K"
      },
      "source": [
        "Now let's compute the same thing, but using matrix calculations.  We'll store the $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ in the columns of a $D\\times N$ matrix, using equations 12.6 and 12.7 and 12.8.\n",
        "\n",
        "Note:  The book uses column vectors (for compatibility with the rest of the text), but in the wider literature it is more normal to store the inputs in the rows of a matrix;  in this case, the computation is the same, but all the matrices are transposed and the operations proceed in the reverse order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obaQBdUAMXXv"
      },
      "outputs": [],
      "source": [
        "# Define softmax operation that works independently on each column\n",
        "def softmax_cols(data_in):\n",
        "  # Exponentiate all of the values\n",
        "  exp_values = np.exp(data_in) ;\n",
        "  # Sum over columns\n",
        "  denom = np.sum(exp_values, axis = 0);\n",
        "  # Replicate denominator to N rows\n",
        "  denom = np.matmul(np.ones((data_in.shape[0],1)), denom[np.newaxis,:])\n",
        "  # Compute softmax\n",
        "  softmax = exp_values / denom\n",
        "  # return the answer\n",
        "  return softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gb2WvQ3SiH8r"
      },
      "outputs": [],
      "source": [
        " # Now let's compute self attention in matrix form\n",
        "def self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n",
        "\n",
        "  # TODO -- Write this function\n",
        "  # 1. Compute queries, keys, and values\n",
        "  # 2. Compute dot products\n",
        "  # 3. Apply softmax to calculate attentions\n",
        "  # 4. Weight values by attentions\n",
        "  # Replace this line\n",
        "  X_prime = np.zeros_like(X);\n",
        "\n",
        "\n",
        "  return X_prime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUOJbgJskUpl"
      },
      "outputs": [],
      "source": [
        "# Copy data into matrix\n",
        "X = np.zeros((D, N))\n",
        "X[:,0] = np.squeeze(all_x[0])\n",
        "X[:,1] = np.squeeze(all_x[1])\n",
        "X[:,2] = np.squeeze(all_x[2])\n",
        "\n",
        "# Run the self attention mechanism\n",
        "X_prime = self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
        "\n",
        "# Print out the results\n",
        "print(X_prime)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as_lRKQFpvz0"
      },
      "source": [
        "If you did this correctly, the values should be the same as above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Coding Exercise: Multihead Self-Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The multihead self-attention mechanism maps $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ and returns $N$ outputs $\\mathbf{x}'_{n}\\in \\mathbb{R}^{D}$.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set seed so we get the same random numbers\n",
        "np.random.seed(3)\n",
        "# Number of inputs\n",
        "N = 6\n",
        "# Number of dimensions of each input\n",
        "D = 8\n",
        "# Create an empty list\n",
        "X = np.random.normal(size=(D,N))\n",
        "# Print X\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll use two heads.  We'll need the weights and biases for the keys, queries, and values (equations 12.2 and 12.4).  We'll use two heads, and we'll make the queries keys and values of size D/H"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Number of heads\n",
        "H = 2\n",
        "# QDV dimension\n",
        "H_D = int(D/H)\n",
        "\n",
        "# Set seed so we get the same random numbers\n",
        "np.random.seed(0)\n",
        "\n",
        "# Choose random values for the parameters for the first head\n",
        "omega_q1 = np.random.normal(size=(H_D,D))\n",
        "omega_k1 = np.random.normal(size=(H_D,D))\n",
        "omega_v1 = np.random.normal(size=(H_D,D))\n",
        "beta_q1 = np.random.normal(size=(H_D,1))\n",
        "beta_k1 = np.random.normal(size=(H_D,1))\n",
        "beta_v1 = np.random.normal(size=(H_D,1))\n",
        "\n",
        "# Choose random values for the parameters for the second head\n",
        "omega_q2 = np.random.normal(size=(H_D,D))\n",
        "omega_k2 = np.random.normal(size=(H_D,D))\n",
        "omega_v2 = np.random.normal(size=(H_D,D))\n",
        "beta_q2 = np.random.normal(size=(H_D,1))\n",
        "beta_k2 = np.random.normal(size=(H_D,1))\n",
        "beta_v2 = np.random.normal(size=(H_D,1))\n",
        "\n",
        "# Choose random values for the parameters\n",
        "omega_c = np.random.normal(size=(D,D))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's compute the multiscale self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define softmax operation that works independently on each column\n",
        "def softmax_cols(data_in):\n",
        "  # Exponentiate all of the values\n",
        "  exp_values = np.exp(data_in) ;\n",
        "  # Sum over columns\n",
        "  denom = np.sum(exp_values, axis = 0);\n",
        "  # Compute softmax (numpy broadcasts denominator to all rows automatically)\n",
        "  softmax = exp_values / denom\n",
        "  # return the answer\n",
        "  return softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        " # Now let's compute self attention in matrix form\n",
        "def multihead_scaled_self_attention(X,omega_v1, omega_q1, omega_k1, beta_v1, beta_q1, beta_k1, omega_v2, omega_q2, omega_k2, beta_v2, beta_q2, beta_k2, omega_c):\n",
        "\n",
        "  # TODO Write the multihead scaled self-attention mechanism.\n",
        "  # Replace this line\n",
        "  X_prime = np.zeros_like(X) ;\n",
        "\n",
        "\n",
        "  return X_prime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the self attention mechanism\n",
        "X_prime = multihead_scaled_self_attention(X,omega_v1, omega_q1, omega_k1, beta_v1, beta_q1, beta_k1, omega_v2, omega_q2, omega_k2, beta_v2, beta_q2, beta_k2, omega_c)\n",
        "\n",
        "# Print out the results\n",
        "np.set_printoptions(precision=3)\n",
        "print(\"Your answer:\")\n",
        "print(X_prime)\n",
        "\n",
        "print(\"True values:\")\n",
        "print(\"[[-21.207  -5.373 -20.933  -9.179 -11.319 -17.812]\")\n",
        "print(\" [ -1.995   7.906 -10.516   3.452   9.863  -7.24 ]\")\n",
        "print(\" [  5.479   1.115   9.244   0.453   5.656   7.089]\")\n",
        "print(\" [ -7.413  -7.416   0.363  -5.573  -6.736  -0.848]\")\n",
        "print(\" [-11.261  -9.937  -4.848  -8.915 -13.378  -5.761]\")\n",
        "print(\" [  3.548  10.036  -2.244   1.604  12.113  -2.557]\")\n",
        "print(\" [  4.888  -5.814   2.407   3.228  -4.232   3.71 ]\")\n",
        "print(\" [  1.248  18.894  -6.409   3.224  19.717  -5.629]]\")\n",
        "\n",
        "# If your answers don't match, then make sure that you are doing the scaling, and make sure the scaling value is correct"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
